{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#MIDS---w261-Machine-Learning-At-Scale\" data-toc-modified-id=\"MIDS---w261-Machine-Learning-At-Scale-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>MIDS - w261 Machine Learning At Scale</a></div><div class=\"lev2 toc-item\"><a href=\"#Assignment---HW5\" data-toc-modified-id=\"Assignment---HW5-11\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Assignment - HW5</a></div><div class=\"lev1 toc-item\"><a href=\"#Table-of-Contents-\" data-toc-modified-id=\"Table-of-Contents--2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Table of Contents </a></div><div class=\"lev1 toc-item\"><a href=\"#1-Instructions\" data-toc-modified-id=\"1-Instructions-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>1 Instructions</a></div><div class=\"lev3 toc-item\"><a href=\"#IMPORTANT\" data-toc-modified-id=\"IMPORTANT-301\"><span class=\"toc-item-num\">3.0.1&nbsp;&nbsp;</span>IMPORTANT</a></div><div class=\"lev3 toc-item\"><a href=\"#===-INSTRUCTIONS-for-SUBMISSIONS-===\" data-toc-modified-id=\"===-INSTRUCTIONS-for-SUBMISSIONS-===-302\"><span class=\"toc-item-num\">3.0.2&nbsp;&nbsp;</span>=== INSTRUCTIONS for SUBMISSIONS ===</a></div><div class=\"lev1 toc-item\"><a href=\"#2-Useful-References\" data-toc-modified-id=\"2-Useful-References-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span></a></div><div class=\"lev1 toc-item\"><a href=\"#HW-Problems\" data-toc-modified-id=\"HW-Problems-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span></a></div><div class=\"lev2 toc-item\"><a href=\"#3.--HW5.0--data-warehouse;-star-schema\" data-toc-modified-id=\"3.--HW5.0--data-warehouse;-star-schema-51\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>3.  HW5.0  data warehouse; star schema</a></div><div class=\"lev2 toc-item\"><a href=\"#3.--HW5.1-Databases:-3NF;-denormalized-\" data-toc-modified-id=\"3.--HW5.1-Databases:-3NF;-denormalized--52\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>3.  HW5.1 Databases: 3NF; denormalized </a></div><div class=\"lev2 toc-item\"><a href=\"#3.--HW5.2--Memory-backed-map-side\" data-toc-modified-id=\"3.--HW5.2--Memory-backed-map-side-53\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>3.  HW5.2  Memory-backed map-side</a></div><div class=\"lev2 toc-item\"><a href=\"#3.--HW5.2.1-(OPTIONAL)-Almost-stateless-reducer-side-join--\" data-toc-modified-id=\"3.--HW5.2.1-(OPTIONAL)-Almost-stateless-reducer-side-join---54\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>3.  HW5.2.1 (OPTIONAL) Almost stateless reducer-side join  </a></div><div class=\"lev1 toc-item\"><a href=\"#5.3-Pairwise-similarity----PHASE-1-\" data-toc-modified-id=\"5.3-Pairwise-similarity----PHASE-1--6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>5.3 Pairwise similarity  - PHASE 1 </a></div><div class=\"lev4 toc-item\"><a href=\"#(1)-Using-the-systems-tests-data-sets,-write-mrjob-code-to-build-the-stripes\" data-toc-modified-id=\"(1)-Using-the-systems-tests-data-sets,-write-mrjob-code-to-build-the-stripes-6001\"><span class=\"toc-item-num\">6.0.0.1&nbsp;&nbsp;</span>(1) Using the systems tests data sets, write mrjob code to build the stripes</a></div><div class=\"lev4 toc-item\"><a href=\"#(2)-Write-mrjob-code-to-build-an-inverted-index-from-the-stripes\" data-toc-modified-id=\"(2)-Write-mrjob-code-to-build-an-inverted-index-from-the-stripes-6002\"><span class=\"toc-item-num\">6.0.0.2&nbsp;&nbsp;</span>(2) Write mrjob code to build an inverted index from the stripes</a></div><div class=\"lev4 toc-item\"><a href=\"#(3)-Using-two-(symmetric)-comparison-methods-of-your-choice-(e.g.,-correlations,-distances,-similarities),-pairwise-compare-all-stripes-(vectors),-and-output-to-a-file.\" data-toc-modified-id=\"(3)-Using-two-(symmetric)-comparison-methods-of-your-choice-(e.g.,-correlations,-distances,-similarities),-pairwise-compare-all-stripes-(vectors),-and-output-to-a-file.-6003\"><span class=\"toc-item-num\">6.0.0.3&nbsp;&nbsp;</span>(3) Using two (symmetric) comparison methods of your choice (e.g., correlations, distances, similarities), pairwise compare all stripes (vectors), and output to a file.</a></div><div class=\"lev2 toc-item\"><a href=\"#HW5.3.1---Run-Systems-tests-locally-on-small-datasets-(PHASE1)-\" data-toc-modified-id=\"HW5.3.1---Run-Systems-tests-locally-on-small-datasets-(PHASE1)--61\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>HW5.3.1   Run Systems tests locally on small datasets (PHASE1) </a></div><div class=\"lev4 toc-item\"><a href=\"#1:-unit/systems-first-10-lines\" data-toc-modified-id=\"1:-unit/systems-first-10-lines-6101\"><span class=\"toc-item-num\">6.1.0.1&nbsp;&nbsp;</span>1: unit/systems first-10-lines</a></div><div class=\"lev4 toc-item\"><a href=\"#2:-unit/systems-atlas-boon\" data-toc-modified-id=\"2:-unit/systems-atlas-boon-6102\"><span class=\"toc-item-num\">6.1.0.2&nbsp;&nbsp;</span>2: unit/systems atlas-boon</a></div><div class=\"lev4 toc-item\"><a href=\"#3:-unit/systems-stripe-docs-test\" data-toc-modified-id=\"3:-unit/systems-stripe-docs-test-6103\"><span class=\"toc-item-num\">6.1.0.3&nbsp;&nbsp;</span>3: unit/systems stripe-docs-test</a></div><div class=\"lev3 toc-item\"><a href=\"#(1)-build-stripes-for-all-the-test-data-sets---run-the-commands-and-insure-that-your-output-matches-the-output-below\" data-toc-modified-id=\"(1)-build-stripes-for-all-the-test-data-sets---run-the-commands-and-insure-that-your-output-matches-the-output-below-611\"><span class=\"toc-item-num\">6.1.1&nbsp;&nbsp;</span>(1) build stripes for all the test data sets - run the commands and insure that your output matches the output below</a></div><div class=\"lev3 toc-item\"><a href=\"#(2)-Build-Inverted-Index---run-the-commands-and-insure-that-your-output-matches-the-output-below\" data-toc-modified-id=\"(2)-Build-Inverted-Index---run-the-commands-and-insure-that-your-output-matches-the-output-below-612\"><span class=\"toc-item-num\">6.1.2&nbsp;&nbsp;</span>(2) Build Inverted Index - run the commands and insure that your output matches the output below</a></div><div class=\"lev3 toc-item\"><a href=\"#Inverted-Index\" data-toc-modified-id=\"Inverted-Index-613\"><span class=\"toc-item-num\">6.1.3&nbsp;&nbsp;</span>Inverted Index</a></div><div class=\"lev3 toc-item\"><a href=\"#(3)-Calculate-similarities---run-the-commands-and-insure-that-your-output-matches-the-output-below\" data-toc-modified-id=\"(3)-Calculate-similarities---run-the-commands-and-insure-that-your-output-matches-the-output-below-614\"><span class=\"toc-item-num\">6.1.4&nbsp;&nbsp;</span>(3) Calculate similarities - run the commands and insure that your output matches the output below</a></div><div class=\"lev4 toc-item\"><a href=\"#NOTE:-you-must-run-in-hadoop-mode-to-generate-sorted-similarities\" data-toc-modified-id=\"NOTE:-you-must-run-in-hadoop-mode-to-generate-sorted-similarities-6141\"><span class=\"toc-item-num\">6.1.4.1&nbsp;&nbsp;</span>NOTE: you must run in hadoop mode to generate sorted similarities</a></div><div class=\"lev3 toc-item\"><a href=\"#Pairwise-Similairity\" data-toc-modified-id=\"Pairwise-Similairity-615\"><span class=\"toc-item-num\">6.1.5&nbsp;&nbsp;</span>Pairwise Similairity</a></div><div class=\"lev1 toc-item\"><a href=\"#===-END-OF-PHASE-1-===\" data-toc-modified-id=\"===-END-OF-PHASE-1-===-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>=== END OF PHASE 1 ===</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIDS - w261 Machine Learning At Scale\n",
    "__Course Lead:__ Dr James G. Shanahan (__email__ Jimi via  James.Shanahan _AT_ gmail.com)\n",
    "\n",
    "## Assignment - HW5\n",
    "\n",
    "\n",
    "---\n",
    "__Name:__  Nicholas W. Chen   \n",
    "__Class:__ MIDS w261 (Section Summer 2017 Group 1)     \n",
    "__Email:__  nwchen24@iSchool.Berkeley.edu     \n",
    "__StudentId__  3032079012    __End of StudentId__     \n",
    "__Week:__   5\n",
    "\n",
    "__NOTE:__ please replace `1234567` with your student id above      \n",
    "__Due Time:__ HW is due the Tuesday of the following week by 8AM (West coast time). I.e., Tuesday, Feb 14, 2017 in the case of this homework. \n",
    "\n",
    "* __HW5 Phase 1__ \n",
    "This can be done on a local machine (with a unit test on the cloud such as AltaScale's PaaS or on AWS) and is due Tuesday, Week 6 by 8AM (West coast time). It will primarily focus on building a unit/systems and for pairwise similarity calculations pipeline (for stripe documents)\n",
    "\n",
    "* __HW5 Phase 2__ \n",
    "This will require the AltaScale cluster and will be due Tuesday, Feb 21 by 8AM (West coast time). \n",
    "The focus of  HW5 Phase 2  will be to scale up the unit/systems tests to the Google 5 gram corpus. This will be a group exercise \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents <a name=\"TOC\"></a> \n",
    "\n",
    "1.  [HW Intructions](#1)   \n",
    "2.  [HW References](#2)\n",
    "3.  [HW Problems](#3)   \n",
    "1.  [HW Introduction](#1)   \n",
    "2.  [HW References](#2)\n",
    "3.  [HW  Problems](#3)   \n",
    "    5.0.  [HW5.0](#5.0)   \n",
    "    5.1.  [HW5.1](#5.1)   \n",
    "    5.2.  [HW5.2](#5.2)   \n",
    "    5.3.  [HW5.3](#5.3)    \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "# 1 Instructions\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "MIDS UC Berkeley, Machine Learning at Scale   \n",
    "DATSCIW261 ASSIGNMENT #5\n",
    "\n",
    "Version 2017-9-2 \n",
    "\n",
    "\n",
    "### IMPORTANT\n",
    "\n",
    "This homework can be completed locally on your computer \n",
    "\n",
    "### === INSTRUCTIONS for SUBMISSIONS ===   \n",
    "Follow the instructions for submissions carefully.\n",
    "\n",
    "Each student has a `HW-<user>` repository for all assignments.   \n",
    "\n",
    "Click this link to enable you to create a github repo within the MIDS261 Classroom:   \n",
    "https://classroom.github.com/assignment-invitations/3b1d6c8e58351209f9dd865537111ff8   \n",
    "and follow the instructions to create a HW repo.\n",
    "\n",
    "Push the following to your HW github repo into the master branch:\n",
    "* Your local HW5 directory. Your repo file structure should look like this:\n",
    "\n",
    "```\n",
    "HW-<user>\n",
    "    --HW3\n",
    "       |__MIDS-W261-HW-03-<Student_id>.ipynb\n",
    "       |__MIDS-W261-HW-03-<Student_id>.pdf\n",
    "       |__some other hw3 file\n",
    "    --HW4\n",
    "       |__MIDS-W261-HW-04-<Student_id>.ipynb\n",
    "       |__MIDS-W261-HW-04-<Student_id>.pdf\n",
    "       |__some other hw4 file\n",
    "    etc..\n",
    "```    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "<a name=\"2\">\n",
    "# 2 Useful References\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "* See async and live lectures for this week\n",
    "\n",
    "<a name=\"3\">\n",
    "# HW Problems\n",
    "[Back to Table of Contents](#TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.0  data warehouse; star schema<a name=\"5.0\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "- What is a data warehouse? What is a Star schema? When is it used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Data warehouses are where organizations store their data. Historically, data warehouses were typically relational in nature and often organized using a Star schema. Today, however, data warehouses are evolving and are being used to store both structured and unstructured data.  \n",
    "\n",
    "A Star schema is a way of organizing a relational database when the database is organized around a central table that contains various dimensions about a given metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.1 Databases: 3NF; denormalized <a name=\"5.1\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "- In the database world What is 3NF? Does machine learning use data in 3NF? If so why? \n",
    "- In what form does ML consume data?\n",
    "- Why would one use log files that are denormalized?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "3NF stands for third normal form. This means that the data are stored in separate tables that relate to one another by various identifiers. The reason for storing data in third normal form is to minimize data duplication.  \n",
    "\n",
    "Machine learning does not use data in third normal form because it needs to have all of the information for each record available at the same time. In third normal form, the information for a single record can be distributed across numerous tables.  \n",
    "\n",
    "ML consumes data in denormalized form. This means that the numerous tables in third normal form have been recombined into a single record so that each record fed to the ML algorithm is complete.  \n",
    "\n",
    "One would use log files that are denormalized so that each record is complete and can be processed by an ML algorithm. A downside of denormalized form is that it can take up much more space than data stored in third normal form because  data is duplicated in denormalized form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.2  Memory-backed map-side<a name=\"5.2\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "Using MRJob, implement a hashside join (memory-backed map-side) for left, right and inner joins. Use the following tables for this HW and join based on the country code (third column of the transactions table and the second column of the Countries table:\n",
    "\n",
    "<PRE>\n",
    "transactions.dat\n",
    "Alice Bob|$10|US\n",
    "Sam Sneed|$1|CA\n",
    "Jon Sneed|$20|CA\n",
    "Arnold Wesise|$400|UK\n",
    "Henry Bob|$2|US\n",
    "Yo Yo Ma|$2|CA\n",
    "Jon York|$44|CA\n",
    "Alex Ball|$5|UK\n",
    "Jim Davis|$66|JA\n",
    "\n",
    "Countries.dat\n",
    "United States|US\n",
    "Canada|CA\n",
    "United Kingdom|UK\n",
    "Italy|IT\n",
    "\n",
    "</PRE>\n",
    "\n",
    "Justify which table you chose as the Left table in this hashside join.\n",
    "\n",
    "Please report the number of rows resulting from:\n",
    "\n",
    "- (1) Left joining Table Left with Table Right\n",
    "- (2) Right joining Table Left with Table Right\n",
    "- (3) Inner joining Table Left with Table Right\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting transactions.dat\n"
     ]
    }
   ],
   "source": [
    "%%writefile transactions.dat\n",
    "Alice Bob|$10|US\n",
    "Sam Sneed|$1|CA\n",
    "Jon Sneed|$20|CA\n",
    "Arnold Wesise|$400|UK\n",
    "Henry Bob|$2|US\n",
    "Yo Yo Ma|$2|CA\n",
    "Jon York|$44|CA\n",
    "Alex Ball|$5|UK\n",
    "Jim Davis|$66|JA\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Countries.dat\n"
     ]
    }
   ],
   "source": [
    "%%writefile Countries.dat\n",
    "United States|US\n",
    "Canada|CA\n",
    "United Kingdom|UK\n",
    "Italy|IT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Left Side Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Left_join_52.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Left_join_52.py\n",
    "#!/opt/anaconda/bin/python\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import ast\n",
    "\n",
    "class MR_Left_Join_52(MRJob):\n",
    "    \n",
    "    # OUTPUT_PROTOCOL = JSONValueProtocol\n",
    "    MRJob.SORT_VALUES = True\n",
    "\n",
    "    #For in-memory mapper-side join, read Countries data into memory\n",
    "    def mapper_init(self):\n",
    "\n",
    "        self.countries_file = open(\"Countries.dat\").readlines()\n",
    "        self.countries_dict = {}\n",
    "\n",
    "        for line in self.countries_file:\n",
    "            #parse each line\n",
    "            Country_full,Country_abbrev = line.split(\"|\")\n",
    "            #update the dict with the abbreviated verision as the key\n",
    "            self.countries_dict[Country_abbrev.strip().translate(None,\"\\n\")] = Country_full\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        #read the input\n",
    "        Customer,Amount,Country_abbrev = line.split(\"|\")\n",
    "        \n",
    "        #This is the join step\n",
    "        try:\n",
    "            #get the full country from the dict\n",
    "            Country_full = self.countries_dict[Country_abbrev.strip().translate(None,\"\\n\")]\n",
    "            \n",
    "            #Yield the joined records\n",
    "            yield Customer, (Country_full, Amount, Country_abbrev)\n",
    "        \n",
    "        #don't emit anything if the country abbreviation not found in the dict - this makes this join an inner join\n",
    "        except KeyError:\n",
    "            pass\n",
    "        \n",
    "        \n",
    "    def reducer_init_written(self):\n",
    "\n",
    "        #Create a dict where we will keep track of which countries have not been encountered in the transaction data\n",
    "        self.countries_encountered_dict = {}\n",
    "\n",
    "        self.countries_file = open(\"Countries.dat\").readlines()\n",
    "\n",
    "        for line in self.countries_file:\n",
    "            #parse each line\n",
    "            Country_full,Country_abbrev = line.split(\"|\")\n",
    "            #update the dict with the abbreviated verision as the key\n",
    "            self.countries_encountered_dict[Country_abbrev.strip().translate(None,\"\\n\")] = Country_full\n",
    "        #yield customer, input_data\n",
    "                  \n",
    "    def reducer(self, customer, inputdata):\n",
    "        \n",
    "        #When a country code is encountered, remove it from the countries file\n",
    "        for Country_full, Amount, Country_abbrev in inputdata:\n",
    "        \n",
    "            try:\n",
    "                del self.countries_encountered_dict[Country_abbrev]\n",
    "            except KeyError:\n",
    "                pass\n",
    "        \n",
    "        #emit the record passed from the mapper\n",
    "        yield customer, (Country_full, Amount, Country_abbrev)\n",
    "\n",
    "    def reducer_final(self):\n",
    "        \n",
    "        #if there are still country entries in the dict that have not been encountered, emit these (this step completes the left join)\n",
    "        for key in self.countries_encountered_dict:\n",
    "            yield None, (self.countries_encountered_dict[key],None,key)\n",
    "       \n",
    "    def steps(self):\n",
    "\n",
    "        return [\n",
    "            MRStep(mapper_init=self.mapper_init,\n",
    "                   mapper=self.mapper,\n",
    "                   reducer_init=self.reducer_init_written,\n",
    "                   reducer=self.reducer,\n",
    "                   reducer_final=self.reducer_final\n",
    "                  )\n",
    "                ]\n",
    "          \n",
    "if __name__ == '__main__':\n",
    "    MR_Left_Join_52.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Make the file executable\n",
    "!chmod +x Left_join_52.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /etc/mrjob.conf\n",
      "Looking for hadoop binary in $PATH...\n",
      "Found hadoop binary: /usr/bin/hadoop\n",
      "Using Hadoop version 2.6.0\n",
      "Looking for Hadoop streaming jar in /home/hadoop/contrib...\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce...\n",
      "Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
      "Creating temp directory /tmp/Left_join_52.root.20170608.143129.465678\n",
      "Copying local files to hdfs:///user/root/tmp/mrjob/Left_join_52.root.20170608.143129.465678/files/...\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.6.0:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 1 of 1...\n",
      "  mapred.text.key.partitioner.options is deprecated. Instead, use mapreduce.partition.keypartitioner.options\n",
      "  packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob5993728315874543417.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1496922884250_0025\n",
      "  Submitted application application_1496922884250_0025\n",
      "  The url to track the job: http://quickstart.cloudera:8088/proxy/application_1496922884250_0025/\n",
      "  Running job: job_1496922884250_0025\n",
      "  Job job_1496922884250_0025 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1496922884250_0025 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/Left_join_52.root.20170608.143129.465678/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=228\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=326\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=331\n",
      "\t\tFILE: Number of bytes written=357292\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=562\n",
      "\t\tHDFS: Number of bytes written=326\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=6390784\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2955264\n",
      "\t\tTotal time spent by all map tasks (ms)=6241\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=6241\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2886\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2886\n",
      "\t\tTotal vcore-seconds taken by all map tasks=6241\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2886\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1670\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=60\n",
      "\t\tInput split bytes=334\n",
      "\t\tMap input records=9\n",
      "\t\tMap output bytes=309\n",
      "\t\tMap output materialized bytes=337\n",
      "\t\tMap output records=8\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=754266112\n",
      "\t\tReduce input groups=8\n",
      "\t\tReduce input records=8\n",
      "\t\tReduce output records=9\n",
      "\t\tReduce shuffle bytes=337\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=16\n",
      "\t\tTotal committed heap usage (bytes)=635961344\n",
      "\t\tVirtual memory (bytes) snapshot=4072845312\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/Left_join_52.root.20170608.143129.465678/output...\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/Left_join_52.root.20170608.143129.465678...\n",
      "Removing temp directory /tmp/Left_join_52.root.20170608.143129.465678...\n"
     ]
    }
   ],
   "source": [
    "#Run the job\n",
    "!python Left_join_52.py -r hadoop --file=Countries.dat transactions.dat > Left_join_52_Output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Alex Ball\"\t[\"United Kingdom\",\"$5\",\"UK\"]\r\n",
      "\"Alice Bob\"\t[\"United States\",\"$10\",\"US\"]\r\n",
      "\"Arnold Wesise\"\t[\"United Kingdom\",\"$400\",\"UK\"]\r\n",
      "\"Henry Bob\"\t[\"United States\",\"$2\",\"US\"]\r\n",
      "\"Jon Sneed\"\t[\"Canada\",\"$20\",\"CA\"]\r\n",
      "\"Jon York\"\t[\"Canada\",\"$44\",\"CA\"]\r\n",
      "\"Sam Sneed\"\t[\"Canada\",\"$1\",\"CA\"]\r\n",
      "\"Yo Yo Ma\"\t[\"Canada\",\"$2\",\"CA\"]\r\n",
      "null\t[\"Italy\",null,\"IT\"]\r\n"
     ]
    }
   ],
   "source": [
    "!head Left_join_52_Output.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inner Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Inner_join_52.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Inner_join_52.py\n",
    "#!/opt/anaconda/bin/python\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class MR_Inner_Join_52(MRJob):\n",
    "    \n",
    "    MRJob.SORT_VALUES = True\n",
    "\n",
    "    #For in-memory mapper-side join, read Countries data into memory\n",
    "    def mapper_init(self):\n",
    "\n",
    "        self.countries_file = open(\"Countries.dat\").readlines()\n",
    "        self.countries_dict = {}\n",
    "\n",
    "        for line in self.countries_file:\n",
    "            #parse each line\n",
    "            Country_full,Country_abbrev = line.split(\"|\")\n",
    "            #update the dict with the abbreviated verision as the key\n",
    "            self.countries_dict[Country_abbrev.strip().translate(None,\"\\n\")] = Country_full\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        #read the input\n",
    "        Customer,Amount,Country_abbrev = line.split(\"|\")\n",
    "        \n",
    "        #This is the join step\n",
    "        try:\n",
    "            #get the full country from the dict\n",
    "            Country_full = self.countries_dict[Country_abbrev.strip().translate(None,\"\\n\")]\n",
    "            #Yield the joined records\n",
    "            yield Customer, (Country_full, Amount, Country_abbrev)\n",
    "        \n",
    "        #don't emit anything if the country abbreviation not found in the dict - this makes this join an inner join\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "    def steps(self):\n",
    "\n",
    "        return [\n",
    "            MRStep(mapper_init=self.mapper_init,\n",
    "                   mapper=self.mapper\n",
    "                  )\n",
    "                ]\n",
    "          \n",
    "if __name__ == '__main__':\n",
    "    MR_Inner_Join_52.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Make the file executable\n",
    "!chmod +x Inner_join_52.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /etc/mrjob.conf\n",
      "Looking for hadoop binary in $PATH...\n",
      "Found hadoop binary: /usr/bin/hadoop\n",
      "Using Hadoop version 2.6.0\n",
      "Looking for Hadoop streaming jar in /home/hadoop/contrib...\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce...\n",
      "Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
      "Creating temp directory /tmp/Inner_join_52.root.20170608.120132.316834\n",
      "Copying local files to hdfs:///user/root/tmp/mrjob/Inner_join_52.root.20170608.120132.316834/files/...\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.6.0:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 1 of 1...\n",
      "  mapred.text.key.partitioner.options is deprecated. Instead, use mapreduce.partition.keypartitioner.options\n",
      "  packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob550166282218208420.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1496922884250_0002\n",
      "  Submitted application application_1496922884250_0002\n",
      "  The url to track the job: http://quickstart.cloudera:8088/proxy/application_1496922884250_0002/\n",
      "  Running job: job_1496922884250_0002\n",
      "  Job job_1496922884250_0002 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "  Job job_1496922884250_0002 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/Inner_join_52.root.20170608.120132.316834/output\n",
      "Counters: 30\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=228\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=309\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=236948\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=564\n",
      "\t\tHDFS: Number of bytes written=309\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=10\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=6496256\n",
      "\t\tTotal time spent by all map tasks (ms)=6344\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=6344\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal vcore-seconds taken by all map tasks=6344\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1070\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=34\n",
      "\t\tInput split bytes=336\n",
      "\t\tMap input records=9\n",
      "\t\tMap output records=8\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tPhysical memory (bytes) snapshot=366735360\n",
      "\t\tSpilled Records=0\n",
      "\t\tTotal committed heap usage (bytes)=284164096\n",
      "\t\tVirtual memory (bytes) snapshot=2714689536\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/Inner_join_52.root.20170608.120132.316834/output...\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/Inner_join_52.root.20170608.120132.316834...\n",
      "Removing temp directory /tmp/Inner_join_52.root.20170608.120132.316834...\n"
     ]
    }
   ],
   "source": [
    "#Run the job\n",
    "!python Inner_join_52.py -r hadoop --file=Countries.dat transactions.dat > Inner_Join_52_Output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Alice Bob\"\t[\"United States\",\"$10\",\"US\"]\t\r\n",
      "\"Sam Sneed\"\t[\"Canada\",\"$1\",\"CA\"]\t\r\n",
      "\"Jon Sneed\"\t[\"Canada\",\"$20\",\"CA\"]\t\r\n",
      "\"Arnold Wesise\"\t[\"United Kingdom\",\"$400\",\"UK\"]\t\r\n",
      "\"Henry Bob\"\t[\"United States\",\"$2\",\"US\"]\t\r\n",
      "\"Yo Yo Ma\"\t[\"Canada\",\"$2\",\"CA\"]\t\r\n",
      "\"Jon York\"\t[\"Canada\",\"$44\",\"CA\"]\t\r\n",
      "\"Alex Ball\"\t[\"United Kingdom\",\"$5\",\"UK\"]\t\r\n"
     ]
    }
   ],
   "source": [
    "!head Inner_Join_52_Output.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Right Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Right_join_52.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Right_join_52.py\n",
    "#!/opt/anaconda/bin/python\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class MR_Right_Join_52(MRJob):\n",
    "    \n",
    "    MRJob.SORT_VALUES = True\n",
    "\n",
    "    #For in-memory mapper-side join, read Countries data into memory\n",
    "    def mapper_init(self):\n",
    "\n",
    "        self.countries_file = open(\"Countries.dat\").readlines()\n",
    "        self.countries_dict = {}\n",
    "\n",
    "        for line in self.countries_file:\n",
    "            #parse each line\n",
    "            Country_full,Country_abbrev = line.split(\"|\")\n",
    "            #update the dict with the abbreviated verision as the key\n",
    "            self.countries_dict[Country_abbrev.strip().translate(None,\"\\n\")] = Country_full\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        #read the input\n",
    "        Customer,Amount,Country_abbrev = line.split(\"|\")\n",
    "        \n",
    "        #This is the join step\n",
    "        try:\n",
    "            #get the full country from the dict\n",
    "            Country_full = self.countries_dict[Country_abbrev.strip().translate(None,\"\\n\")]\n",
    "            #Yield the joined records\n",
    "            yield Customer, (Country_full, Amount, Country_abbrev)\n",
    "        \n",
    "        #emit the transaction line even if the country abbreviation not found in the dict - this makes this join a right join\n",
    "        except KeyError:\n",
    "            yield Customer, (None, Amount, Country_abbrev.strip().translate(None,\"\\n\"))\n",
    "\n",
    "    def steps(self):\n",
    "\n",
    "        return [\n",
    "            MRStep(mapper_init=self.mapper_init,\n",
    "                   mapper=self.mapper\n",
    "                  )\n",
    "                ]\n",
    "          \n",
    "if __name__ == '__main__':\n",
    "    MR_Right_Join_52.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Make the file executable\n",
    "!chmod +x Right_join_52.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /etc/mrjob.conf\n",
      "Looking for hadoop binary in $PATH...\n",
      "Found hadoop binary: /usr/bin/hadoop\n",
      "Using Hadoop version 2.6.0\n",
      "Looking for Hadoop streaming jar in /home/hadoop/contrib...\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce...\n",
      "Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
      "Creating temp directory /tmp/Right_join_52.root.20170608.120833.725291\n",
      "Copying local files to hdfs:///user/root/tmp/mrjob/Right_join_52.root.20170608.120833.725291/files/...\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.6.0:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 1 of 1...\n",
      "  mapred.text.key.partitioner.options is deprecated. Instead, use mapreduce.partition.keypartitioner.options\n",
      "  packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob6811294315927257045.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1496922884250_0004\n",
      "  Submitted application application_1496922884250_0004\n",
      "  The url to track the job: http://quickstart.cloudera:8088/proxy/application_1496922884250_0004/\n",
      "  Running job: job_1496922884250_0004\n",
      "  Job job_1496922884250_0004 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "  Job job_1496922884250_0004 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/Right_join_52.root.20170608.120833.725291/output\n",
      "Counters: 30\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=228\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=340\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=236950\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=564\n",
      "\t\tHDFS: Number of bytes written=340\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=10\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=6627328\n",
      "\t\tTotal time spent by all map tasks (ms)=6472\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=6472\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal vcore-seconds taken by all map tasks=6472\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1020\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=32\n",
      "\t\tInput split bytes=336\n",
      "\t\tMap input records=9\n",
      "\t\tMap output records=9\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tPhysical memory (bytes) snapshot=353923072\n",
      "\t\tSpilled Records=0\n",
      "\t\tTotal committed heap usage (bytes)=286261248\n",
      "\t\tVirtual memory (bytes) snapshot=2714537984\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/Right_join_52.root.20170608.120833.725291/output...\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/Right_join_52.root.20170608.120833.725291...\n",
      "Removing temp directory /tmp/Right_join_52.root.20170608.120833.725291...\n"
     ]
    }
   ],
   "source": [
    "#Run the job\n",
    "!python Right_join_52.py -r hadoop --file=Countries.dat transactions.dat > Right_join_52_Output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Alice Bob\"\t[\"United States\",\"$10\",\"US\"]\t\r\n",
      "\"Sam Sneed\"\t[\"Canada\",\"$1\",\"CA\"]\t\r\n",
      "\"Jon Sneed\"\t[\"Canada\",\"$20\",\"CA\"]\t\r\n",
      "\"Arnold Wesise\"\t[\"United Kingdom\",\"$400\",\"UK\"]\t\r\n",
      "\"Henry Bob\"\t[\"United States\",\"$2\",\"US\"]\t\r\n",
      "\"Yo Yo Ma\"\t[\"Canada\",\"$2\",\"CA\"]\t\r\n",
      "\"Jon York\"\t[\"Canada\",\"$44\",\"CA\"]\t\r\n",
      "\"Alex Ball\"\t[\"United Kingdom\",\"$5\",\"UK\"]\t\r\n",
      "\"Jim Davis\"\t[null,\"$66\",\"JA\"]\t\r\n"
     ]
    }
   ],
   "source": [
    "!head Right_join_52_Output.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.2.1 (OPTIONAL) Almost stateless reducer-side join  <a name=\"5.2.1\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "The following MRJob code, implements a reduce-side join for an inner join. The reducer is almost stateless, i.e., uses as little memory as possible. Use the tables from HW5.2 for this HW and join based on the country code (third column of the transactions table and the second column of the Countries table perform. Perform  an left, right, inner joins using the code provided below and report the number of rows resulting from:\n",
    "\n",
    "- (1) Left joining Table Left with Table Right\n",
    "- (2) Right joining Table Left with Table Right\n",
    "- (3) Inner joining Table Left with Table Right\n",
    "\n",
    "Again make smart decisions about which table should be the left table (i.e., crosscheck the code). \n",
    "\n",
    "__Some notes on the code__ \n",
    "Here, the mapper receives its set of input splits either from the transaction table or from the countries table and makes the appropriate transformations: splitting the line into fields, and emitting a key/value. The key is the join key - in this case, the country code field of both sets of records. The mapper knows which file and type of record it is receiving based on the length of the fields. The records it emits contain the join field as the key, which acts as the partitioning key; We use the SORT_VALUES option, which ensures the values are sorted as well. Then, we employ a trick to ensure that for each join key, country records are seen always before transaction records. We achieve this by adding an arbitrary key to the front of the value: 'A' for countries, 'B' for customers. This makes countries sort before customers for each and every join/partition key. After that trick, the join is simply a matter of storing countries ('A' records) and crossing this array with each customer record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, os, re\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class MRJoin(MRJob):\n",
    "\n",
    "  # Performs secondary sort\n",
    "  SORT_VALUES = True\n",
    "\n",
    "  def mapper(self, _, line):\n",
    "    splits = line.rstrip(\"\\n\").split(\"|\")\n",
    "\n",
    "    if len(splits) == 2: # country data\n",
    "      symbol = 'A' # make country sort before transaction data\n",
    "      country2digit = splits[1]\n",
    "      yield country2digit, [symbol, splits]\n",
    "    else: # person data\n",
    "      symbol = 'B'\n",
    "      country2digit = splits[2]\n",
    "      yield country2digit, [symbol, splits]\n",
    "\n",
    "  def reducer(self, key, values):\n",
    "    countries = [] # should come first, as they are sorted on artificia key 'A'\n",
    "    for value in values:\n",
    "      if value[0] == 'A':\n",
    "        countries.append(value)\n",
    "      if value[0] == 'B':\n",
    "        for country in countries:\n",
    "          yield key, country[1:] + value[1:]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  MRJoin.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.3 Pairwise similarity  - PHASE 1 <a name=\"5.3\"></a>\n",
    "\n",
    "In this part of the assignment we will focus on developing methods for detecting synonyms, using the Google 5-grams dataset. To accomplish this you must script two main tasks using MRJob:\n",
    "\n",
    "\n",
    "#### (1) Using the systems tests data sets, write mrjob code to build the stripes\n",
    "#### (2) Write mrjob code to build an inverted index from the stripes\n",
    "#### (3) Using two (symmetric) comparison methods of your choice (e.g., correlations, distances, similarities), pairwise compare all stripes (vectors), and output to a file.   \n",
    "\n",
    "__==Design notes for (1)== __  \n",
    "For this task you will be able to modify the pattern we used in HW 3.2 (feel free to use the solution as reference). To total the word counts across the n-grams, output the support from the mappers using the total order inversion pattern:\n",
    "\n",
    "<*word,count>   \n",
    "\n",
    "to ensure that the support arrives before the cooccurrences.   \n",
    "\n",
    "In addition to ensuring the determination of the total word counts, the mapper must also output co-occurrence counts for the pairs of words inside of each n-gram. Treat these words as a basket, as we have in HW 3, but count all stripes or pairs in both orders, i.e., count both orderings: (word1,word2), and (word2,word1), to preserve\n",
    "symmetry in our output for (2).\n",
    "\n",
    "__==Design notes for (3)==__   \n",
    "For this task you will have to determine a method of comparison.\n",
    "Here are a few that you might consider:\n",
    "\n",
    " - Jaccard\n",
    " - Cosine similarity\n",
    " - Spearman correlation\n",
    " - Euclidean distance\n",
    " - Taxicab (Manhattan) distance\n",
    " - Shortest path graph distance (a graph, because our data is symmetric!)\n",
    " - Pearson correlation\n",
    " - Kendall correlation\n",
    " ...\n",
    "\n",
    "However, be cautioned that some comparison methods are more difficult to parallelize than others, and do not perform more associations than is necessary, since your choice of association will be symmetric.\n",
    "\n",
    "Please use the inverted index (discussed in live session #5) based pattern to compute the pairwise (term-by-term) similarity matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting buildStripes.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile buildStripes.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __future__ import division\n",
    "import re\n",
    "import mrjob\n",
    "import json\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import itertools\n",
    "import collections as cl\n",
    "import ast\n",
    "\n",
    "class MRbuildStripes(MRJob):\n",
    "  \n",
    "    #START SUDENT CODE531_STRIPES\n",
    "    \n",
    "    #OUTPUT_PROTOCOL = JSONValueProtocol\n",
    "    \n",
    "    def steps(self):\n",
    "        \n",
    "        JOBCONF_STEP = {\n",
    "            'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'stream.map.output.field.separator':'\\t',    \n",
    "            'mapreduce.partition.keycomparator.options': '-k1,1',\n",
    "            'mapreduce.job.reduces': '1'\n",
    "        }\n",
    "        \n",
    "        return [\n",
    "            MRStep(jobconf=JOBCONF_STEP,\n",
    "                   mapper=self.mapper,\n",
    "                   reducer_init=self.reducer_init,\n",
    "                   reducer=self.reducer\n",
    "                  )\n",
    "        ]\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        \n",
    "        #instantiate dict to hold stripes\n",
    "        stripes_dict = {}\n",
    "        \n",
    "        #parse the co-occurrence count from the line\n",
    "        co_occur_count = line.strip().split()[-3]\n",
    "\n",
    "        #get pairs of words in the 5-gram and create a stripe for each unique word\n",
    "        #Note the set here ensures that a word that appears more than once in a 5-gram won't be counted as co-occurring with itself\n",
    "        #Note the permutations here will count both (word1,word2) and (word2,word1)\n",
    "        for subset in itertools.permutations(sorted(set(line.strip().lower().split()[:-3])), 2):\n",
    "            if subset[0] not in stripes_dict.keys():\n",
    "                stripes_dict[subset[0]] = {}\n",
    "                stripes_dict[subset[0]][subset[1]] = int(co_occur_count)\n",
    "                \n",
    "            elif subset[1] not in stripes_dict[subset[0]]:\n",
    "                stripes_dict[subset[0]][subset[1]] = int(co_occur_count)\n",
    "                \n",
    "            else:\n",
    "                stripes_dict[subset[0]][subset[1]] += int(co_occur_count)\n",
    "        \n",
    "        for key in stripes_dict:\n",
    "            #yield key, json.dumps(stripes_dict[key])\n",
    "            yield key, json.dumps(stripes_dict[key])\n",
    "\n",
    "    def reducer_init(self):\n",
    "        #instantiate placholder current key and counter\n",
    "        self.placeholder_for_reducer = 0\n",
    "        \n",
    "    def reducer(self,key,stripe):\n",
    "        \n",
    "        #instantiate counter to hold combined stripes\n",
    "        cur_counter = cl.Counter()\n",
    "                \n",
    "        for stripe_dict in stripe:\n",
    "            cur_key = key\n",
    "            \n",
    "            #load the stripe into a dict\n",
    "            stripe_dict_to_add = json.loads(stripe_dict)\n",
    "            \n",
    "            #update with each stripe\n",
    "            cur_counter.update(stripe_dict_to_add)\n",
    "\n",
    "        #output the key and the counter with the sum of all co-occurrences for that key\n",
    "        yield key, cur_counter\n",
    "        \n",
    "        #The placeholder only increments when a new key is encountered\n",
    "        #reducer seems to get chunks of key value pairs from the mapper with the same key all together\n",
    "        self.placeholder_for_reducer += 1\n",
    "\n",
    "    #END SUDENT CODE531_STRIPES\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRbuildStripes.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting invertedIndex.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile invertedIndex.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __future__ import division\n",
    "import collections as cl\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import itertools\n",
    "import mrjob\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import sys\n",
    "\n",
    "class MRinvertedIndex(MRJob):\n",
    "    #START SUDENT CODE531_INV_INDEX\n",
    "    \n",
    "    #OUTPUT_PROTOCOL = JSONValueProtocol\n",
    "\n",
    "    \n",
    "    def steps(self):\n",
    "        \n",
    "        JOBCONF_STEP = {\n",
    "            'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'stream.map.output.field.separator':'\\t',    \n",
    "            'mapreduce.partition.keycomparator.options': '-k1,1',\n",
    "            'mapreduce.job.reduces': '1'\n",
    "        }\n",
    "        \n",
    "        return [\n",
    "            MRStep(jobconf=JOBCONF_STEP,\n",
    "                   mapper=self.mapper,\n",
    "                   reducer=self.reducer\n",
    "                  )\n",
    "        ]\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        \n",
    "        #Read the key and the stripe from the input\n",
    "        key_stripe_list = line.translate(None,\"\\n\").split(\"\\t\")\n",
    "        \n",
    "        key = json.loads(key_stripe_list[0])\n",
    "        stripe = json.loads(key_stripe_list[1])\n",
    "        \n",
    "        #get length of the stripe\n",
    "        stripe_len = len(stripe)\n",
    "        \n",
    "        #create dict that will hold doc title and the stripe length\n",
    "        stripe_len_dict = {key:stripe_len}\n",
    "        \n",
    "        for term in stripe:\n",
    "            yield term, stripe_len_dict\n",
    "  \n",
    "    def reducer(self, key, stripe):\n",
    "\n",
    "        #instantiate a dict where we will combine stripe_len_dicts emitted from mapper\n",
    "        stripe_len_dict_combined = {}\n",
    "        \n",
    "        #Read the incremental stripe length dit from the mapper output\n",
    "        for stripe_len_dict in stripe:\n",
    "            \n",
    "            #update the dict above with each incremental stripe length dict\n",
    "            stripe_len_dict_combined.update(stripe_len_dict)\n",
    "                    \n",
    "        yield key, stripe_len_dict_combined        \n",
    "        \n",
    "        #END SUDENT CODE531_INV_INDEX\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    MRinvertedIndex.run() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting similarity.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile similarity.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __future__ import division\n",
    "import collections\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import itertools\n",
    "import mrjob\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class MRsimilarity(MRJob):\n",
    "  \n",
    "    #START SUDENT CODE531_SIMILARITY\n",
    "    #OUTPUT_PROTOCOL = JSONValueProtocol\n",
    "    \n",
    "    def steps(self):\n",
    "        \n",
    "        JOBCONF_STEP = {\n",
    "            'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'stream.map.output.field.separator':'\\t',    \n",
    "            'stream.num.map.output.key.fields': '2',\n",
    "            'mapreduce.partition.keycomparator.options': '-k1,1',\n",
    "            'mapreduce.job.reduces': '1'\n",
    "        }\n",
    "        \n",
    "        return [\n",
    "            MRStep(jobconf=JOBCONF_STEP,\n",
    "                   mapper=self.mapper,\n",
    "                   reducer=self.reducer\n",
    "                  )\n",
    "        ]\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        \n",
    "        #Read the key and the stripe from the input\n",
    "        key_index_list = line.translate(None,\"\\n\").split(\"\\t\")\n",
    "        \n",
    "        key = json.loads(key_index_list[0])\n",
    "        inv_index = json.loads(key_index_list[1])\n",
    "        \n",
    "        for subset in itertools.permutations(sorted(set(inv_index.keys())), 2):\n",
    "            \n",
    "            #get the lengths of each item that we want to output\n",
    "            item_1_len = inv_index[subset[0]]\n",
    "            item_2_len = inv_index[subset[1]]\n",
    "            \n",
    "            #create a two item dict to hold the item names and their lengths\n",
    "            item_len_dict = {}\n",
    "            item_len_dict[subset[0]] = item_1_len\n",
    "            item_len_dict[subset[1]] = item_2_len\n",
    "            \n",
    "            #when output like this, can index with key[0] to get subset[0] in the reducer\n",
    "            yield (subset[0], subset[1]), (1,item_len_dict)\n",
    "  \n",
    "    def reducer(self, key, stripe):\n",
    "\n",
    "        #instantiate object to hold the number of times the terms / documents intersect\n",
    "        items_intersect = 0\n",
    "        cosine_sim_product = 0\n",
    "        \n",
    "        #the reducer gets output from the mapper in chunks grouped by key\n",
    "        for n,item_len_dict in stripe:\n",
    "            \n",
    "            #get the length of item 1 and item 2\n",
    "            item_1_len = item_len_dict[key[0]]\n",
    "            item_2_len = item_len_dict[key[1]]\n",
    "            \n",
    "            #increment the intersect count\n",
    "            items_intersect = items_intersect + n\n",
    "            \n",
    "            cosine_sim_product = cosine_sim_product + (1/np.sqrt(item_1_len)) * (1/np.sqrt(item_2_len))\n",
    "            \n",
    "        #calculate jacard similarity score\n",
    "        jacard_sim_score = float(items_intersect)/ float(item_1_len + item_2_len - items_intersect)\n",
    "        \n",
    "        #Record cosine similarity score before moving on to the next chunk\n",
    "        cosine_sim_score = cosine_sim_product\n",
    "        \n",
    "        yield key, (jacard_sim_score, cosine_sim_score)   \n",
    "    \n",
    "    #END SUDENT CODE531_SIMILARITY\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRsimilarity.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW5.3.1   Run Systems tests locally on small datasets (PHASE1) <a name=\"5.3.1\"></a>  \n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "Complete 5.3 and systems test using the below test datasets. Phase 2 will focus on the entire Ngram dataset.\n",
    "\n",
    "To help you through these tasks please verify that your code gives the results below (for stripes, inverted index, and pairwise similarities).\n",
    "\n",
    "Test datasets:\n",
    "\n",
    "* googlebooks-eng-all-5gram-20090715-0-filtered.txt [see below]\n",
    "* atlas-boon-test [see below]\n",
    "* stripe-docs-test [see below]\n",
    "\n",
    "\n",
    "A large subset of the Google n-grams dataset\n",
    "\n",
    "https://aws.amazon.com/datasets/google-books-ngrams/\n",
    "\n",
    "which we have placed in a bucket/folder on Dropbox and on s3:\n",
    "\n",
    "https://www.dropbox.com/sh/tmqpc4o0xswhkvz/AACUifrl6wrMrlK6a3X3lZ9Ea?dl=0 \n",
    "\n",
    "s3://filtered-5grams/\n",
    "\n",
    "In particular, this bucket contains (~200) files (10Meg each) in the format:\n",
    "\n",
    "\t(ngram) \\t (count) \\t (pages_count) \\t (books_count)\n",
    "\n",
    "The next cell shows the first 10 lines of the googlebooks-eng-all-5gram-20090715-0-filtered.txt file.\n",
    "\n",
    "\n",
    "__DISCLAIMER__: Each record is already a 5-gram. In real life, we would calculate the stripes cooccurrence data from the raw text by windowing over the raw text and not from the 5-gram preprocessed data (as we are doing here).  Calculatating pairs on this 5-gram is a little corrupt as we will be double counting cooccurences. Having said that this exercise can still pull out some simialr terms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1: unit/systems first-10-lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting googlebooks-eng-all-5gram-20090715-0-filtered-first-10-lines.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile googlebooks-eng-all-5gram-20090715-0-filtered-first-10-lines.txt\n",
    "A BILL FOR ESTABLISHING RELIGIOUS\t59\t59\t54\n",
    "A Biography of General George\t92\t90\t74\n",
    "A Case Study in Government\t102\t102\t78\n",
    "A Case Study of Female\t447\t447\t327\n",
    "A Case Study of Limited\t55\t55\t43\n",
    "A Child's Christmas in Wales\t1099\t1061\t866\n",
    "A Circumstantial Narrative of the\t62\t62\t50\n",
    "A City by the Sea\t62\t60\t49\n",
    "A Collection of Fairy Tales\t123\t117\t80\n",
    "A Collection of Forms of\t116\t103\t82"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2: unit/systems atlas-boon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting atlas-boon-systems-test.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile atlas-boon-systems-test.txt\n",
    "atlas boon\t50\t50\t50\n",
    "boon cava dipped\t10\t10\t10\n",
    "atlas dipped\t15\t15\t15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3: unit/systems stripe-docs-test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three terms, A,B,C and their corresponding stripe-docs of co-occurring terms\n",
    "\n",
    "- DocA {X:20, Y:30, Z:5}\n",
    "- DocB {X:100, Y:20}\n",
    "- DocC {M:5, N:20, Z:5}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) build stripes for all the test data sets - run the commands and insure that your output matches the output below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: Unknown command\n",
      "Did you mean -rm?  This command begins with a dash.\n",
      "Using configs in /etc/mrjob.conf\n",
      "Creating temp directory /tmp/buildStripes.root.20170609.193359.649160\n",
      "Running step 1 of 1...\n",
      "Streaming final output from /tmp/buildStripes.root.20170609.193359.649160/output...\n",
      "Removing temp directory /tmp/buildStripes.root.20170609.193359.649160...\n"
     ]
    }
   ],
   "source": [
    "###########################################################################\n",
    "# Make Stripes from ngrams for systems test 1\n",
    "###########################################################################\n",
    "\n",
    "!hdfs dfs rm --recursive systems_test_stripes_1\n",
    "\n",
    "#Local works\n",
    "!python buildStripes.py -r local googlebooks-eng-all-5gram-20090715-0-filtered-first-10-lines.txt > systems_test_stripes_1   \n",
    "\n",
    "#Hadoop also works\n",
    "#!python buildStripes.py -r hadoop googlebooks-eng-all-5gram-20090715-0-filtered-first-10-lines.txt > systems_test_stripes_1   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"a\"\t{\"limited\":55,\"female\":447,\"general\":92,\"sea\":62,\"in\":1201,\"religious\":59,\"george\":92,\"biography\":92,\"city\":62,\"for\":59,\"tales\":123,\"child's\":1099,\"forms\":116,\"wales\":1099,\"christmas\":1099,\"government\":102,\"collection\":239,\"by\":62,\"case\":604,\"circumstantial\":62,\"fairy\":123,\"of\":895,\"study\":604,\"bill\":59,\"establishing\":59,\"narrative\":62,\"the\":124}\r\n",
      "\"bill\"\t{\"a\":59,\"religious\":59,\"for\":59,\"establishing\":59}\r\n",
      "\"biography\"\t{\"a\":92,\"of\":92,\"george\":92,\"general\":92}\r\n",
      "\"by\"\t{\"a\":62,\"city\":62,\"the\":62,\"sea\":62}\r\n",
      "\"case\"\t{\"a\":604,\"limited\":55,\"government\":102,\"of\":502,\"study\":604,\"female\":447,\"in\":102}\r\n",
      "\"child's\"\t{\"a\":1099,\"wales\":1099,\"christmas\":1099,\"in\":1099}\r\n",
      "\"christmas\"\t{\"a\":1099,\"wales\":1099,\"in\":1099,\"child's\":1099}\r\n",
      "\"circumstantial\"\t{\"a\":62,\"of\":62,\"the\":62,\"narrative\":62}\r\n",
      "\"city\"\t{\"a\":62,\"the\":62,\"by\":62,\"sea\":62}\r\n",
      "\"collection\"\t{\"a\":239,\"forms\":116,\"fairy\":123,\"tales\":123,\"of\":239}\r\n",
      "\"establishing\"\t{\"a\":59,\"bill\":59,\"religious\":59,\"for\":59}\r\n",
      "\"fairy\"\t{\"a\":123,\"of\":123,\"tales\":123,\"collection\":123}\r\n",
      "\"female\"\t{\"a\":447,\"case\":447,\"study\":447,\"of\":447}\r\n",
      "\"for\"\t{\"a\":59,\"bill\":59,\"religious\":59,\"establishing\":59}\r\n",
      "\"forms\"\t{\"a\":116,\"of\":116,\"collection\":116}\r\n",
      "\"general\"\t{\"a\":92,\"of\":92,\"george\":92,\"biography\":92}\r\n",
      "\"george\"\t{\"a\":92,\"of\":92,\"biography\":92,\"general\":92}\r\n",
      "\"government\"\t{\"a\":102,\"case\":102,\"study\":102,\"in\":102}\r\n",
      "\"in\"\t{\"a\":1201,\"case\":102,\"government\":102,\"study\":102,\"child's\":1099,\"wales\":1099,\"christmas\":1099}\r\n",
      "\"limited\"\t{\"a\":55,\"case\":55,\"study\":55,\"of\":55}\r\n",
      "\"narrative\"\t{\"a\":62,\"of\":62,\"the\":62,\"circumstantial\":62}\r\n",
      "\"of\"\t{\"a\":895,\"case\":502,\"circumstantial\":62,\"george\":92,\"limited\":55,\"tales\":123,\"collection\":239,\"the\":62,\"forms\":116,\"female\":447,\"narrative\":62,\"fairy\":123,\"general\":92,\"study\":502,\"biography\":92}\r\n",
      "\"religious\"\t{\"a\":59,\"bill\":59,\"for\":59,\"establishing\":59}\r\n",
      "\"sea\"\t{\"a\":62,\"city\":62,\"the\":62,\"by\":62}\r\n",
      "\"study\"\t{\"a\":604,\"case\":604,\"limited\":55,\"government\":102,\"of\":502,\"female\":447,\"in\":102}\r\n",
      "\"tales\"\t{\"a\":123,\"of\":123,\"fairy\":123,\"collection\":123}\r\n",
      "\"the\"\t{\"a\":124,\"city\":62,\"circumstantial\":62,\"of\":62,\"sea\":62,\"narrative\":62,\"by\":62}\r\n",
      "\"wales\"\t{\"a\":1099,\"in\":1099,\"christmas\":1099,\"child's\":1099}\r\n"
     ]
    }
   ],
   "source": [
    "!cat systems_test_stripes_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "\"a\"\t{\"limited\": 55, \"sea\": 62, \"general\": 92, \"female\": 447, \"in\": 1201, \"religious\": 59, \"george\": 92, \"biography\": 92, \"city\": 62, \"for\": 59, \"tales\": 123, \"child's\": 1099, \"forms\": 116, \"wales\": 1099, \"christmas\": 1099, \"government\": 102, \"collection\": 239, \"by\": 62, \"case\": 604, \"circumstantial\": 62, \"fairy\": 123, \"of\": 1011, \"study\": 604, \"bill\": 59, \"establishing\": 59, \"narrative\": 62, \"the\": 124}\n",
    "\"bill\"\t{\"a\": 59, \"religious\": 59, \"for\": 59, \"establishing\": 59}\n",
    "\"biography\"\t{\"a\": 92, \"of\": 92, \"george\": 92, \"general\": 92}\n",
    "\"by\"\t{\"a\": 62, \"city\": 62, \"the\": 62, \"sea\": 62}\n",
    "\"case\"\t{\"a\": 604, \"limited\": 55, \"government\": 102, \"of\": 502, \"study\": 604, \"female\": 447, \"in\": 102}\n",
    "\"child's\"\t{\"a\": 1099, \"wales\": 1099, \"christmas\": 1099, \"in\": 1099}\n",
    "\"christmas\"\t{\"a\": 1099, \"wales\": 1099, \"in\": 1099, \"child's\": 1099}\n",
    "\"circumstantial\"\t{\"a\": 62, \"of\": 62, \"the\": 62, \"narrative\": 62}\n",
    "\"city\"\t{\"a\": 62, \"the\": 62, \"by\": 62, \"sea\": 62}\n",
    "\"collection\"\t{\"a\": 239, \"of\": 355, \"fairy\": 123, \"tales\": 123, \"forms\": 116}\n",
    "\"establishing\"\t{\"a\": 59, \"bill\": 59, \"religious\": 59, \"for\": 59}\n",
    "\"fairy\"\t{\"a\": 123, \"of\": 123, \"tales\": 123, \"collection\": 123}\n",
    "\"female\"\t{\"a\": 447, \"case\": 447, \"study\": 447, \"of\": 447}\n",
    "\"for\"\t{\"a\": 59, \"bill\": 59, \"religious\": 59, \"establishing\": 59}\n",
    "\"forms\"\t{\"a\": 116, \"of\": 232, \"collection\": 116}\n",
    "\"general\"\t{\"a\": 92, \"of\": 92, \"george\": 92, \"biography\": 92}\n",
    "\"george\"\t{\"a\": 92, \"of\": 92, \"biography\": 92, \"general\": 92}\n",
    "\"government\"\t{\"a\": 102, \"case\": 102, \"study\": 102, \"in\": 102}\n",
    "\"in\"\t{\"a\": 1201, \"case\": 102, \"government\": 102, \"study\": 102, \"child's\": 1099, \"wales\": 1099, \"christmas\": 1099}\n",
    "\"limited\"\t{\"a\": 55, \"case\": 55, \"study\": 55, \"of\": 55}\n",
    "\"narrative\"\t{\"a\": 62, \"of\": 62, \"the\": 62, \"circumstantial\": 62}\n",
    "\"of\"\t{\"a\": 1127, \"case\": 502, \"circumstantial\": 62, \"george\": 92, \"limited\": 55, \"tales\": 123, \"collection\": 471, \"general\": 92, \"forms\": 348, \"female\": 447, \"narrative\": 62, \"study\": 502, \"fairy\": 123, \"the\": 62, \"biography\": 92}\n",
    "\"religious\"\t{\"a\": 59, \"bill\": 59, \"for\": 59, \"establishing\": 59}\n",
    "\"sea\"\t{\"a\": 62, \"city\": 62, \"the\": 62, \"by\": 62}\n",
    "\"study\"\t{\"a\": 604, \"case\": 604, \"limited\": 55, \"government\": 102, \"of\": 502, \"female\": 447, \"in\": 102}\n",
    "\"tales\"\t{\"a\": 123, \"of\": 123, \"fairy\": 123, \"collection\": 123}\n",
    "\"the\"\t{\"a\": 124, \"city\": 62, \"circumstantial\": 62, \"of\": 62, \"sea\": 62, \"narrative\": 62, \"by\": 62}\n",
    "\"wales\"\t{\"a\": 1099, \"in\": 1099, \"christmas\": 1099, \"child's\": 1099}\n",
    "\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: Unknown command\n",
      "Did you mean -rm?  This command begins with a dash.\n",
      "Using configs in /etc/mrjob.conf\n",
      "Looking for hadoop binary in $PATH...\n",
      "Found hadoop binary: /usr/bin/hadoop\n",
      "Using Hadoop version 2.6.0\n",
      "Looking for Hadoop streaming jar in /home/hadoop/contrib...\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce...\n",
      "Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
      "Creating temp directory /tmp/buildStripes.root.20170609.193705.029966\n",
      "Copying local files to hdfs:///user/root/tmp/mrjob/buildStripes.root.20170609.193705.029966/files/...\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob6846033796536965021.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1497021168065_0003\n",
      "  Submitted application application_1497021168065_0003\n",
      "  The url to track the job: http://quickstart.cloudera:8088/proxy/application_1497021168065_0003/\n",
      "  Running job: job_1497021168065_0003\n",
      "  Job job_1497021168065_0003 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497021168065_0003 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/buildStripes.root.20170609.193705.029966/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=101\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=147\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=244\n",
      "\t\tFILE: Number of bytes written=355903\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=457\n",
      "\t\tHDFS: Number of bytes written=147\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=7559168\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=3177472\n",
      "\t\tTotal time spent by all map tasks (ms)=7382\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=7382\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3103\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3103\n",
      "\t\tTotal vcore-seconds taken by all map tasks=7382\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=3103\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1810\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=63\n",
      "\t\tInput split bytes=356\n",
      "\t\tMap input records=3\n",
      "\t\tMap output bytes=224\n",
      "\t\tMap output materialized bytes=250\n",
      "\t\tMap output records=7\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=742252544\n",
      "\t\tReduce input groups=4\n",
      "\t\tReduce input records=7\n",
      "\t\tReduce output records=4\n",
      "\t\tReduce shuffle bytes=250\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=14\n",
      "\t\tTotal committed heap usage (bytes)=639631360\n",
      "\t\tVirtual memory (bytes) snapshot=4091858944\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/buildStripes.root.20170609.193705.029966/output...\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/buildStripes.root.20170609.193705.029966...\n",
      "Removing temp directory /tmp/buildStripes.root.20170609.193705.029966...\n"
     ]
    }
   ],
   "source": [
    "###########################################################################\n",
    "# Make Stripes from ngrams for systems test 2\n",
    "###########################################################################\n",
    "\n",
    "!hdfs dfs rm --recursive systems_test_stripes_2\n",
    "\n",
    "#Local mode works\n",
    "#!python buildStripes.py -r local atlas-boon-systems-test.txt > systems_test_stripes_2\n",
    "\n",
    "#Hadoop mode\n",
    "!python buildStripes.py -r hadoop atlas-boon-systems-test.txt > systems_test_stripes_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"atlas\"\t{\"dipped\":15,\"boon\":50}\r\n",
      "\"boon\"\t{\"atlas\":50,\"dipped\":10,\"cava\":10}\r\n",
      "\"cava\"\t{\"dipped\":10,\"boon\":10}\r\n",
      "\"dipped\"\t{\"atlas\":15,\"boon\":10,\"cava\":10}\r\n"
     ]
    }
   ],
   "source": [
    "!cat systems_test_stripes_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<pre>\n",
    "\"atlas\"   {\"dipped\": 15, \"boon\": 50}   \n",
    "\"boon\"    {\"atlas\": 50, \"dipped\": 10, \"cava\": 10}   \n",
    "\"cava\"    {\"dipped\": 10, \"boon\": 10} \n",
    "\"dipped\"  {\"atlas\": 15, \"boon\": 10, \"cava\": 10}\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"DocA\"\t{\"X\":20, \"Y\":30, \"Z\":5}\r\n",
      "\"DocB\"\t{\"X\":100, \"Y\":20}\r\n",
      "\"DocC\"\t{\"M\":5, \"N\":20, \"Z\":5, \"Y\":1}\r\n"
     ]
    }
   ],
   "source": [
    "########################################################################\n",
    "# Stripes for systems test 3 (given, no need to build stripes)\n",
    "########################################################################\n",
    "\n",
    "with open(\"systems_test_stripes_3\", \"w\") as f:\n",
    "    f.writelines([\n",
    "        '\"DocA\"\\t{\"X\":20, \"Y\":30, \"Z\":5}\\n',\n",
    "        '\"DocB\"\\t{\"X\":100, \"Y\":20}\\n',  \n",
    "        '\"DocC\"\\t{\"M\":5, \"N\":20, \"Z\":5, \"Y\":1}\\n'\n",
    "    ])\n",
    "!cat systems_test_stripes_3   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Build Inverted Index - run the commands and insure that your output matches the output below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /etc/mrjob.conf\n",
      "Creating temp directory /tmp/invertedIndex.root.20170610.191444.468690\n",
      "Running step 1 of 1...\n",
      "Streaming final output from /tmp/invertedIndex.root.20170610.191444.468690/output...\n",
      "Removing temp directory /tmp/invertedIndex.root.20170610.191444.468690...\n"
     ]
    }
   ],
   "source": [
    "#Local works\n",
    "#!python invertedIndex.py -r local --file=systems_test_stripes_1  systems_test_stripes_1 > systems_test_index_1\n",
    "!python invertedIndex.py -r local systems_test_stripes_1 > systems_test_index_1\n",
    "\n",
    "\n",
    "#Hadoop mode also works\n",
    "#!python invertedIndex.py -r hadoop systems_test_stripes_1 > systems_test_index_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /etc/mrjob.conf\n",
      "Creating temp directory /tmp/invertedIndex.root.20170610.191446.131567\n",
      "Running step 1 of 1...\n",
      "Streaming final output from /tmp/invertedIndex.root.20170610.191446.131567/output...\n",
      "Removing temp directory /tmp/invertedIndex.root.20170610.191446.131567...\n"
     ]
    }
   ],
   "source": [
    "#Local works\n",
    "!python invertedIndex.py -r local systems_test_stripes_2 > systems_test_index_2\n",
    "\n",
    "#Hadoop mode also works\n",
    "#!python invertedIndex.py -r hadoop systems_test_stripes_2 > systems_test_index_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /etc/mrjob.conf\n",
      "Creating temp directory /tmp/invertedIndex.root.20170610.191503.808725\n",
      "Running step 1 of 1...\n",
      "Streaming final output from /tmp/invertedIndex.root.20170610.191503.808725/output...\n",
      "Removing temp directory /tmp/invertedIndex.root.20170610.191503.808725...\n"
     ]
    }
   ],
   "source": [
    "#Local works\n",
    "!python invertedIndex.py -r local systems_test_stripes_3 > systems_test_index_3\n",
    "\n",
    "#Hadoop mode also works\n",
    "#!python invertedIndex.py -r hadoop systems_test_stripes_3 > systems_test_index_3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Systems test  1  - Inverted Index\n",
      "\n",
      "             \"a\" |       limited 4 |           sea 4 |       general 4\n",
      "          \"bill\" |            a 27 |     religious 4 |           for 4\n",
      "     \"biography\" |            a 27 |           of 15 |        george 4\n",
      "            \"by\" |            a 27 |          city 4 |           the 7\n",
      "          \"case\" |            a 27 |       limited 4 |    government 4\n",
      "       \"child's\" |            a 27 |         wales 4 |     christmas 4\n",
      "     \"christmas\" |            a 27 |         wales 4 |       child's 4\n",
      "\"circumstantial\" |            a 27 |           of 15 |           the 7\n",
      "          \"city\" |            a 27 |           the 7 |            by 4\n",
      "    \"collection\" |            a 27 |         forms 3 |         fairy 4\n",
      "  \"establishing\" |            a 27 |          bill 4 |     religious 4\n",
      "         \"fairy\" |            a 27 |           of 15 |         tales 4\n",
      "        \"female\" |            a 27 |          case 7 |         study 7\n",
      "           \"for\" |            a 27 |          bill 4 |     religious 4\n",
      "         \"forms\" |            a 27 |           of 15 |    collection 5\n",
      "       \"general\" |            a 27 |           of 15 |        george 4\n",
      "        \"george\" |            a 27 |           of 15 |     biography 4\n",
      "    \"government\" |            a 27 |          case 7 |         study 7\n",
      "            \"in\" |            a 27 |          case 7 |       child's 4\n",
      "       \"limited\" |            a 27 |          case 7 |         study 7\n",
      "     \"narrative\" |            a 27 |           of 15 |           the 7\n",
      "            \"of\" |            a 27 |          case 7 |circumstantial 4\n",
      "     \"religious\" |            a 27 |          bill 4 |           for 4\n",
      "           \"sea\" |            a 27 |          city 4 |           the 7\n",
      "         \"study\" |            a 27 |          case 7 |       limited 4\n",
      "         \"tales\" |            a 27 |           of 15 |         fairy 4\n",
      "           \"the\" |            a 27 |          city 4 |circumstantial 4\n",
      "         \"wales\" |            a 27 |       child's 4 |     christmas 4\n",
      "\n",
      "Systems test  2  - Inverted Index\n",
      "\n",
      "         \"atlas\" |        dipped 3 |          boon 3 |                \n",
      "          \"boon\" |         atlas 2 |        dipped 3 |          cava 2\n",
      "          \"cava\" |        dipped 3 |          boon 3 |                \n",
      "        \"dipped\" |         atlas 2 |          boon 3 |          cava 2\n",
      "\n",
      "Systems test  3  - Inverted Index\n",
      "\n",
      "             \"M\" |          DocC 4 |                 |                \n",
      "             \"N\" |          DocC 4 |                 |                \n",
      "             \"X\" |          DocB 2 |          DocA 3 |                \n",
      "             \"Y\" |          DocB 2 |          DocC 4 |          DocA 3\n",
      "             \"Z\" |          DocC 4 |          DocA 3 |                \n"
     ]
    }
   ],
   "source": [
    "##########################################################\n",
    "# Pretty print systems tests for generating Inverted Index\n",
    "##########################################################\n",
    "\n",
    "import json\n",
    "\n",
    "for i in range(1,4):\n",
    "    print \"\"*100\n",
    "    print \"Systems test \",i,\" - Inverted Index\"\n",
    "    print \"\"*100  \n",
    "    with open(\"systems_test_index_\"+str(i),\"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            word,stripe = line.split(\"\\t\")\n",
    "            stripe = json.loads(stripe)\n",
    "            #stripe.extend([[\"\",\"\"] for _ in xrange(3 - len(stripe))])\n",
    "            if len(stripe) > 2:\n",
    "                print \"{0:>16} |{1:>16} |{2:>16} |{3:>16}\".format(\n",
    "                (word), stripe.keys()[0]+\" \"+str(stripe[stripe.keys()[0]]), stripe.keys()[1]+\" \"+str(stripe[stripe.keys()[1]]), stripe.keys()[2]+\" \"+str(stripe[stripe.keys()[2]]))    \n",
    "            elif len(stripe) == 2:\n",
    "                print \"{0:>16} |{1:>16} |{2:>16} |{3:>16}\".format(\n",
    "                (word), stripe.keys()[0]+\" \"+str(stripe[stripe.keys()[0]]), stripe.keys()[1]+\" \"+str(stripe[stripe.keys()[1]]), \" \")\n",
    "            else:\n",
    "                print \"{0:>16} |{1:>16} |{2:>16} |{3:>16}\".format(\n",
    "                (word), stripe.keys()[0]+\" \"+str(stripe[stripe.keys()[0]]),\" \", \" \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "Systems test  1  - Inverted Index\n",
    "\n",
    "             \"a\" |          bill 4 |     biography 4 |            by 4\n",
    "          \"bill\" |            a 27 |  establishing 4 |           for 4\n",
    "     \"biography\" |            a 27 |       general 4 |        george 4\n",
    "            \"by\" |            a 27 |          city 4 |           sea 4\n",
    "          \"case\" |            a 27 |        female 4 |    government 4\n",
    "       \"child's\" |            a 27 |     christmas 4 |            in 7\n",
    "     \"christmas\" |            a 27 |       child's 4 |            in 7\n",
    "\"circumstantial\" |            a 27 |     narrative 4 |           of 15\n",
    "          \"city\" |            a 27 |            by 4 |           sea 4\n",
    "    \"collection\" |            a 27 |         fairy 4 |         forms 3\n",
    "  \"establishing\" |            a 27 |          bill 4 |           for 4\n",
    "         \"fairy\" |            a 27 |    collection 5 |           of 15\n",
    "        \"female\" |            a 27 |          case 7 |           of 15\n",
    "           \"for\" |            a 27 |          bill 4 |  establishing 4\n",
    "         \"forms\" |            a 27 |    collection 5 |           of 15\n",
    "       \"general\" |            a 27 |     biography 4 |        george 4\n",
    "        \"george\" |            a 27 |     biography 4 |       general 4\n",
    "    \"government\" |            a 27 |          case 7 |            in 7\n",
    "            \"in\" |            a 27 |          case 7 |       child's 4\n",
    "       \"limited\" |            a 27 |          case 7 |           of 15\n",
    "     \"narrative\" |            a 27 |circumstantial 4 |           of 15\n",
    "            \"of\" |            a 27 |     biography 4 |          case 7\n",
    "     \"religious\" |            a 27 |          bill 4 |  establishing 4\n",
    "           \"sea\" |            a 27 |            by 4 |          city 4\n",
    "         \"study\" |            a 27 |          case 7 |        female 4\n",
    "         \"tales\" |            a 27 |    collection 5 |         fairy 4\n",
    "           \"the\" |            a 27 |            by 4 |circumstantial 4\n",
    "         \"wales\" |            a 27 |       child's 4 |     christmas 4\n",
    "\n",
    "Systems test  2  - Inverted Index\n",
    "\n",
    "         \"atlas\" |          boon 3 |        dipped 3 |                \n",
    "          \"boon\" |         atlas 2 |          cava 2 |        dipped 3\n",
    "          \"cava\" |          boon 3 |        dipped 3 |                \n",
    "        \"dipped\" |         atlas 2 |          boon 3 |          cava 2\n",
    "\n",
    "Systems test  3  - Inverted Index\n",
    "\n",
    "             \"M\" |          DocC 4 |                 |                \n",
    "             \"N\" |          DocC 4 |                 |                \n",
    "             \"X\" |          DocA 3 |          DocB 2 |                \n",
    "             \"Y\" |          DocA 3 |          DocB 2 |          DocC 4\n",
    "             \"Z\" |          DocA 3 |          DocC 4 |                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Calculate similarities - run the commands and insure that your output matches the output below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE: you must run in hadoop mode to generate sorted similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /etc/mrjob.conf\n",
      "Looking for hadoop binary in $PATH...\n",
      "Found hadoop binary: /usr/bin/hadoop\n",
      "Using Hadoop version 2.6.0\n",
      "Looking for Hadoop streaming jar in /home/hadoop/contrib...\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce...\n",
      "Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
      "Creating temp directory /tmp/similarity.root.20170612.022355.751309\n",
      "Copying local files to hdfs:///user/root/tmp/mrjob/similarity.root.20170612.022355.751309/files/...\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob4205294873810135897.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1497107873528_0029\n",
      "  Submitted application application_1497107873528_0029\n",
      "  The url to track the job: http://quickstart.cloudera:8088/proxy/application_1497107873528_0029/\n",
      "  Running job: job_1497107873528_0029\n",
      "  Job job_1497107873528_0029 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497107873528_0029 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/similarity.root.20170612.022355.751309/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2856\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=31230\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=67178\n",
      "\t\tFILE: Number of bytes written=490083\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3194\n",
      "\t\tHDFS: Number of bytes written=31230\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=7565312\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=3251200\n",
      "\t\tTotal time spent by all map tasks (ms)=7388\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=7388\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3175\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3175\n",
      "\t\tTotal vcore-seconds taken by all map tasks=7388\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=3175\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2330\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=79\n",
      "\t\tInput split bytes=338\n",
      "\t\tMap input records=28\n",
      "\t\tMap output bytes=64480\n",
      "\t\tMap output materialized bytes=67184\n",
      "\t\tMap output records=1346\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=802611200\n",
      "\t\tReduce input groups=756\n",
      "\t\tReduce input records=1346\n",
      "\t\tReduce output records=756\n",
      "\t\tReduce shuffle bytes=67184\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=2692\n",
      "\t\tTotal committed heap usage (bytes)=633864192\n",
      "\t\tVirtual memory (bytes) snapshot=4129398784\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/similarity.root.20170612.022355.751309/output...\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/similarity.root.20170612.022355.751309...\n",
      "Removing temp directory /tmp/similarity.root.20170612.022355.751309...\n"
     ]
    }
   ],
   "source": [
    "#Hadoop mode also works\n",
    "!python similarity.py -r hadoop systems_test_index_1 > systems_test_similarities_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /etc/mrjob.conf\n",
      "Looking for hadoop binary in $PATH...\n",
      "Found hadoop binary: /usr/bin/hadoop\n",
      "Using Hadoop version 2.6.0\n",
      "Looking for Hadoop streaming jar in /home/hadoop/contrib...\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce...\n",
      "Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
      "Creating temp directory /tmp/similarity.root.20170611.023918.305852\n",
      "Copying local files to hdfs:///user/root/tmp/mrjob/similarity.root.20170611.023918.305852/files/...\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob8878015087281128230.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1497107873528_0027\n",
      "  Submitted application application_1497107873528_0027\n",
      "  The url to track the job: http://quickstart.cloudera:8088/proxy/application_1497107873528_0027/\n",
      "  Running job: job_1497107873528_0027\n",
      "  Job job_1497107873528_0027 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497107873528_0027 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/similarity.root.20170611.023918.305852/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=206\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=428\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=742\n",
      "\t\tFILE: Number of bytes written=357211\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=544\n",
      "\t\tHDFS: Number of bytes written=428\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=6602752\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2971648\n",
      "\t\tTotal time spent by all map tasks (ms)=6448\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=6448\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2902\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2902\n",
      "\t\tTotal vcore-seconds taken by all map tasks=6448\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2902\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1660\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=68\n",
      "\t\tInput split bytes=338\n",
      "\t\tMap input records=4\n",
      "\t\tMap output bytes=704\n",
      "\t\tMap output materialized bytes=748\n",
      "\t\tMap output records=16\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=760885248\n",
      "\t\tReduce input groups=12\n",
      "\t\tReduce input records=16\n",
      "\t\tReduce output records=12\n",
      "\t\tReduce shuffle bytes=748\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=32\n",
      "\t\tTotal committed heap usage (bytes)=639631360\n",
      "\t\tVirtual memory (bytes) snapshot=4096851968\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/similarity.root.20170611.023918.305852/output...\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/similarity.root.20170611.023918.305852...\n",
      "Removing temp directory /tmp/similarity.root.20170611.023918.305852...\n"
     ]
    }
   ],
   "source": [
    "#Local mode\n",
    "!python similarity.py -r hadoop systems_test_index_2 > systems_test_similarities_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /etc/mrjob.conf\n",
      "Looking for hadoop binary in $PATH...\n",
      "Found hadoop binary: /usr/bin/hadoop\n",
      "Using Hadoop version 2.6.0\n",
      "Looking for Hadoop streaming jar in /home/hadoop/contrib...\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce...\n",
      "Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
      "Creating temp directory /tmp/similarity.root.20170611.024046.301768\n",
      "Copying local files to hdfs:///user/root/tmp/mrjob/similarity.root.20170611.024046.301768/files/...\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob4692597056649005003.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1497107873528_0028\n",
      "  Submitted application application_1497107873528_0028\n",
      "  The url to track the job: http://quickstart.cloudera:8088/proxy/application_1497107873528_0028/\n",
      "  Running job: job_1497107873528_0028\n",
      "  Job job_1497107873528_0028 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497107873528_0028 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/similarity.root.20170611.024046.301768/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=167\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=228\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=436\n",
      "\t\tFILE: Number of bytes written=356599\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=505\n",
      "\t\tHDFS: Number of bytes written=228\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=6434816\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=3045376\n",
      "\t\tTotal time spent by all map tasks (ms)=6284\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=6284\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2974\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2974\n",
      "\t\tTotal vcore-seconds taken by all map tasks=6284\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2974\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1730\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=48\n",
      "\t\tInput split bytes=338\n",
      "\t\tMap input records=5\n",
      "\t\tMap output bytes=410\n",
      "\t\tMap output materialized bytes=442\n",
      "\t\tMap output records=10\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=797200384\n",
      "\t\tReduce input groups=6\n",
      "\t\tReduce input records=10\n",
      "\t\tReduce output records=6\n",
      "\t\tReduce shuffle bytes=442\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=20\n",
      "\t\tTotal committed heap usage (bytes)=635961344\n",
      "\t\tVirtual memory (bytes) snapshot=4108509184\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/similarity.root.20170611.024046.301768/output...\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/similarity.root.20170611.024046.301768...\n",
      "Removing temp directory /tmp/similarity.root.20170611.024046.301768...\n"
     ]
    }
   ],
   "source": [
    "!python similarity.py -r hadoop systems_test_index_3 > systems_test_similarities_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "############################################\n",
    "# Pretty print systems tests\n",
    "############################################\n",
    "\n",
    "import json\n",
    "for i in range(1,4):\n",
    "  print ''*110\n",
    "  print \"Systems test \",i,\" - Similarity measures\"\n",
    "  print ''*110\n",
    "  print \"{0:>15} |{1:>15} |{2:>15} |{3:>15} |{4:>15} |{5:>15}\".format(\n",
    "          \"average\", \"pair\", \"cosine\", \"jaccard\", \"overlap\", \"dice\")\n",
    "  print '-'*110\n",
    "\n",
    "  with open(\"systems_test_similarities_\"+str(i),\"r\") as f:\n",
    "      lines = f.readlines()\n",
    "      for line in lines:\n",
    "          line = line.strip()\n",
    "          avg,stripe = line.split(\"\\t\")\n",
    "          stripe = json.loads(stripe)\n",
    "\n",
    "          print \"{0:>15f} |{1:>15} |{2:>15f} |{3:>15f} |{4:>15f} |{5:>15f}\".format(\n",
    "              float(avg), stripe[0], float(stripe[1]), float(stripe[2]), float(stripe[3]), float(stripe[4]))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"atlas\",\"boon\"]\t[0.25,0.4082482905]\r\n",
      "[\"atlas\",\"cava\"]\t[1.0,1.0]\r\n",
      "[\"atlas\",\"dipped\"]\t[0.25,0.4082482905]\r\n",
      "[\"boon\",\"atlas\"]\t[0.25,0.4082482905]\r\n",
      "[\"boon\",\"cava\"]\t[0.25,0.4082482905]\r\n",
      "[\"boon\",\"dipped\"]\t[0.5,0.6666666667]\r\n",
      "[\"cava\",\"atlas\"]\t[1.0,1.0]\r\n",
      "[\"cava\",\"boon\"]\t[0.25,0.4082482905]\r\n",
      "[\"cava\",\"dipped\"]\t[0.25,0.4082482905]\r\n",
      "[\"dipped\",\"atlas\"]\t[0.25,0.4082482905]\r\n",
      "[\"dipped\",\"boon\"]\t[0.5,0.6666666667]\r\n",
      "[\"dipped\",\"cava\"]\t[0.25,0.4082482905]\r\n"
     ]
    }
   ],
   "source": [
    "!cat systems_test_similarities_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Systems test  1  - Similarity measures\n",
      "\n",
      "                     pair |        jaccard |         cosine\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "             [\"a\",\"bill\"] |       0.107143 |       0.288675\n",
      "        [\"a\",\"biography\"] |       0.107143 |       0.288675\n",
      "               [\"a\",\"by\"] |       0.107143 |       0.288675\n",
      "             [\"a\",\"case\"] |       0.214286 |       0.436436\n",
      "          [\"a\",\"child's\"] |       0.107143 |       0.288675\n",
      "        [\"a\",\"christmas\"] |       0.107143 |       0.288675\n",
      "   [\"a\",\"circumstantial\"] |       0.107143 |       0.288675\n",
      "             [\"a\",\"city\"] |       0.107143 |       0.288675\n",
      "       [\"a\",\"collection\"] |       0.142857 |       0.344265\n",
      "     [\"a\",\"establishing\"] |       0.107143 |       0.288675\n",
      "            [\"a\",\"fairy\"] |       0.107143 |       0.288675\n",
      "           [\"a\",\"female\"] |       0.107143 |       0.288675\n",
      "              [\"a\",\"for\"] |       0.107143 |       0.288675\n",
      "            [\"a\",\"forms\"] |       0.071429 |       0.222222\n",
      "          [\"a\",\"general\"] |       0.107143 |       0.288675\n",
      "\n",
      "Systems test  2  - Similarity measures\n",
      "\n",
      "                     pair |        jaccard |         cosine\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "         [\"atlas\",\"boon\"] |       0.250000 |       0.408248\n",
      "         [\"atlas\",\"cava\"] |       1.000000 |       1.000000\n",
      "       [\"atlas\",\"dipped\"] |       0.250000 |       0.408248\n",
      "         [\"boon\",\"atlas\"] |       0.250000 |       0.408248\n",
      "          [\"boon\",\"cava\"] |       0.250000 |       0.408248\n",
      "        [\"boon\",\"dipped\"] |       0.500000 |       0.666667\n",
      "         [\"cava\",\"atlas\"] |       1.000000 |       1.000000\n",
      "          [\"cava\",\"boon\"] |       0.250000 |       0.408248\n",
      "        [\"cava\",\"dipped\"] |       0.250000 |       0.408248\n",
      "       [\"dipped\",\"atlas\"] |       0.250000 |       0.408248\n",
      "        [\"dipped\",\"boon\"] |       0.500000 |       0.666667\n",
      "        [\"dipped\",\"cava\"] |       0.250000 |       0.408248\n",
      "\n",
      "Systems test  3  - Similarity measures\n",
      "\n",
      "                     pair |        jaccard |         cosine\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "          [\"DocA\",\"DocB\"] |       0.666667 |       0.816497\n",
      "          [\"DocA\",\"DocC\"] |       0.400000 |       0.577350\n",
      "          [\"DocB\",\"DocA\"] |       0.666667 |       0.816497\n",
      "          [\"DocB\",\"DocC\"] |       0.200000 |       0.353553\n",
      "          [\"DocC\",\"DocA\"] |       0.400000 |       0.577350\n",
      "          [\"DocC\",\"DocB\"] |       0.200000 |       0.353553\n"
     ]
    }
   ],
   "source": [
    "#Pretty print the similarities\n",
    "\n",
    "import ast\n",
    "\n",
    "for i in range(1,4):\n",
    "\n",
    "    #Print titles\n",
    "    print ''*110\n",
    "    print \"Systems test \",i,\" - Similarity measures\"\n",
    "    print ''*110\n",
    "    print \"{0:>25} |{1:>15} |{2:>15}\".format(\n",
    "          \"pair\", \"jaccard\", \"cosine\")\n",
    "    print '-'*110\n",
    "\n",
    "    #read file and keep only first 15 lines\n",
    "    f = open('systems_test_similarities_'+str(i)).readlines()\n",
    "    first_15 = f[:15]\n",
    "\n",
    "    #print the first 15 lines of each output\n",
    "    for line in first_15:\n",
    "        terms_sims_list = line.split('\\t')\n",
    "        sims = ast.literal_eval(terms_sims_list[1])\n",
    "        #print the calculated similarities\n",
    "        print \"{0:>25s} |{1:>15f} |{2:>15f}\".format(\n",
    "                  terms_sims_list[0], sims[0], sims[1])\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairwise Similairity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "Systems test  1  - Similarity measures\n",
    "\n",
    "   average |                pair |         cosine |        jaccard |        overlap |           dice\n",
    "--------------------------------------------------------------------------------------------------------------\n",
    "  1.000000 |    female - limited |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
    "  0.868292 |       fairy - forms |       0.866025 |       0.750000 |       1.000000 |       0.857143\n",
    "  0.868292 |       forms - tales |       0.866025 |       0.750000 |       1.000000 |       0.857143\n",
    "  0.830357 |        case - study |       0.857143 |       0.750000 |       0.857143 |       0.857143\n",
    "  0.712500 | bill - establishing |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
    "  0.712500 |   christmas - wales |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
    "  0.712500 |circumstantial - narrative |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
    "  0.712500 |            by - sea |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
    "  0.712500 |           by - city |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
    "  0.712500 |     child's - wales |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
    "  0.712500 |  biography - george |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
    "  0.712500 | child's - christmas |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
    "  ...\n",
    "  \n",
    "\n",
    "Systems test  2  - Similarity measures\n",
    "\n",
    "   average |                pair |         cosine |        jaccard |        overlap |           dice\n",
    "--------------------------------------------------------------------------------------------------------------\n",
    "  1.000000 |        atlas - cava |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
    "  0.625000 |       boon - dipped |       0.666667 |       0.500000 |       0.666667 |       0.666667\n",
    "  0.389562 |       cava - dipped |       0.408248 |       0.250000 |       0.500000 |       0.400000\n",
    "  0.389562 |         boon - cava |       0.408248 |       0.250000 |       0.500000 |       0.400000\n",
    "  0.389562 |      atlas - dipped |       0.408248 |       0.250000 |       0.500000 |       0.400000\n",
    "  0.389562 |        atlas - boon |       0.408248 |       0.250000 |       0.500000 |       0.400000\n",
    "\n",
    "Systems test  3  - Similarity measures\n",
    "\n",
    "   average |                pair |         cosine |        jaccard |        overlap |           dice\n",
    "--------------------------------------------------------------------------------------------------------------\n",
    "  0.820791 |         DocA - DocB |       0.816497 |       0.666667 |       1.000000 |       0.800000\n",
    "  0.553861 |         DocA - DocC |       0.577350 |       0.400000 |       0.666667 |       0.571429\n",
    "  0.346722 |         DocB - DocC |       0.353553 |       0.200000 |       0.500000 |       0.333333\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# === END OF PHASE 1 ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "512px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": false,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
