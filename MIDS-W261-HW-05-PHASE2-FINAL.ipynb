{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIDS - w261 Machine Learning At Scale\n",
    "__Course Lead:__ Dr James G. Shanahan (__email__ Jimi via  James.Shanahan _AT_ gmail.com)\n",
    "\n",
    "## Assignment - HW5\n",
    "\n",
    "\n",
    "---\n",
    "__Name:__  Nicholas Chen, Ramsey Aweti, David Skarbrevik  \n",
    "__Class:__ MIDS w261 (Section Summer 2017, Group 1)     \n",
    "__Email:__  nwchen24@iSchool.Berkeley.edu  \n",
    "__Week:__   5\n",
    "\n",
    "__Due Time:__ 2 Phases. \n",
    "\n",
    "* __HW5 Phase 1__ \n",
    "This can be done on a local machine (with a unit test on the cloud such as AltaScale's PaaS or on AWS) and is due Tuesday, Week 6 by 8AM (West coast time). It will primarily focus on building a unit/systems and for pairwise similarity calculations pipeline (for stripe documents)\n",
    "\n",
    "* __HW5 Phase 2__ \n",
    "This will require the AltaScale cluster and will be due Tuesday, Week 7 by 8AM (West coast time). \n",
    "The focus of  HW5 Phase 2  will be to scale up the unit/systems tests to the Google 5 gram corpus. This will be a group exercise \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents <a name=\"TOC\"></a> \n",
    "\n",
    "1.  [HW Instructions](#1)   \n",
    "2.  [HW References](#2)\n",
    "3.  [HW Problems](#3)   \n",
    "       \n",
    "    5.4.  [HW5.4](#5.4)    \n",
    "    5.5.  [HW5.5](#5.5)    \n",
    "    5.6.  [HW5.6](#5.6)    \n",
    "    5.7.  [HW5.7](#5.7)    \n",
    "    5.8.  [HW5.8](#5.8)    \n",
    "    5.9.  [HW5.9](#5.9)    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "# 1 Instructions\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "MIDS UC Berkeley, Machine Learning at Scale   \n",
    "DATSCIW261 ASSIGNMENT #5\n",
    "\n",
    "Version 2017-9-2 \n",
    "\n",
    "\n",
    "### IMPORTANT\n",
    "\n",
    "This homework must be completed in the cloud \n",
    "\n",
    "### === INSTRUCTIONS for SUBMISSIONS ===   \n",
    "Follow the instructions for submissions carefully.\n",
    "\n",
    "Each student has a `HW-<user>` repository for all assignments.   \n",
    "\n",
    "Click this link to enable you to create a github repo within the MIDS261 Classroom:   \n",
    "https://classroom.github.com/assignment-invitations/3b1d6c8e58351209f9dd865537111ff8   \n",
    "and follow the instructions to create a HW repo.\n",
    "\n",
    "Push the following to your HW github repo into the master branch:\n",
    "* Your local HW5 directory. Your repo file structure should look like this:\n",
    "\n",
    "```\n",
    "HW-<user>\n",
    "    --HW3\n",
    "       |__MIDS-W261-HW-03-<Student_id>.ipynb\n",
    "       |__MIDS-W261-HW-03-<Student_id>.pdf\n",
    "       |__some other hw3 file\n",
    "    --HW4\n",
    "       |__MIDS-W261-HW-04-<Student_id>.ipynb\n",
    "       |__MIDS-W261-HW-04-<Student_id>.pdf\n",
    "       |__some other hw4 file\n",
    "    etc..\n",
    "```    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2\">\n",
    "# 2 Useful References\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "* See async and live lectures for this week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3\">\n",
    "# 3 HW Problems\n",
    "[Back to Table of Contents](#TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"5.4\"></a> \n",
    "# PHASE 2\n",
    "----------\n",
    "\n",
    "# HW 5.4   \n",
    "## Full-scale experiment on Google N-gram data on the CLOUD\n",
    "__ Once you are happy with your test results __ proceed to generating  your results on the Google n-grams dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.4.0  <a name=\"5.4.0\"></a> Run systems tests on the CLOUD  (PHASE 2)\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "Repeat HW5.3.0 (using the same small data sources that were used in HW5.3.0) on ** the cloud** (e.g., AltaScale / AWS/ SoftLayer/ Azure). Make sure all tests give correct results! Good luck out there!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download a single sample of the google n-grams  \n",
    "\n",
    "This sample is for testing on local docker container where "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2017-06-20 03:46:48--  https://www.dropbox.com/sh/tmqpc4o0xswhkvz/AACr50woxiBWoaiiLmnwduX8a/googlebooks-eng-all-5gram-20090715-0-filtered.txt?dl=0\n",
      "Resolving www.dropbox.com... 162.125.6.1\n",
      "Connecting to www.dropbox.com|162.125.6.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://dl.dropboxusercontent.com/content_link/xmU2rIQ3uzN3jEpjGXajt5YJtK5XaeiGDZkixPXj9REAx7Em3FdtLeLJtVOJVueC/file [following]\n",
      "--2017-06-20 03:46:48--  https://dl.dropboxusercontent.com/content_link/xmU2rIQ3uzN3jEpjGXajt5YJtK5XaeiGDZkixPXj9REAx7Em3FdtLeLJtVOJVueC/file\n",
      "Resolving dl.dropboxusercontent.com... 162.125.17.6\n",
      "Connecting to dl.dropboxusercontent.com|162.125.17.6|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 11444614 (11M) [text/plain]\n",
      "Saving to: `google-5gram-sample.txt'\n",
      "\n",
      "100%[======================================>] 11,444,614  8.65M/s   in 1.3s    \n",
      "\n",
      "2017-06-20 03:46:52 (8.65 MB/s) - `google-5gram-sample.txt' saved [11444614/11444614]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -O google-5gram-sample.txt https://www.dropbox.com/sh/tmqpc4o0xswhkvz/AACr50woxiBWoaiiLmnwduX8a/googlebooks-eng-all-5gram-20090715-0-filtered.txt?dl=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/nwchen24/.conda/envs/py27/bin/python'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting googlebooks-eng-all-5gram-20090715-0-filtered-first-10-lines.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile googlebooks-eng-all-5gram-20090715-0-filtered-first-10-lines.txt\n",
    "A BILL FOR ESTABLISHING RELIGIOUS\t59\t59\t54\n",
    "A Biography of General George\t92\t90\t74\n",
    "A Case Study in Government\t102\t102\t78\n",
    "A Case Study of Female\t447\t447\t327\n",
    "A Case Study of Limited\t55\t55\t43\n",
    "A Child's Christmas in Wales\t1099\t1061\t866\n",
    "A Circumstantial Narrative of the\t62\t62\t50\n",
    "A City by the Sea\t62\t60\t49\n",
    "A Collection of Fairy Tales\t123\t117\t80\n",
    "A Collection of Forms of\t116\t103\t82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting atlas-boon-systems-test.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile atlas-boon-systems-test.txt\n",
    "atlas boon\t50\t50\t50\n",
    "boon cava dipped\t10\t10\t10\n",
    "atlas dipped\t15\t15\t15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"DocA\"\t{\"X\":20, \"Y\":30, \"Z\":5}\r\n",
      "\"DocB\"\t{\"X\":100, \"Y\":20}\r\n",
      "\"DocC\"\t{\"M\":5, \"N\":20, \"Z\":5, \"Y\":1}\r\n"
     ]
    }
   ],
   "source": [
    "########################################################################\n",
    "# Stripes for systems test 3 (given, no need to build stripes)\n",
    "########################################################################\n",
    "\n",
    "with open(\"systems_test_stripes_3\", \"w\") as f:\n",
    "    f.writelines([\n",
    "        '\"DocA\"\\t{\"X\":20, \"Y\":30, \"Z\":5}\\n',\n",
    "        '\"DocB\"\\t{\"X\":100, \"Y\":20}\\n',  \n",
    "        '\"DocC\"\\t{\"M\":5, \"N\":20, \"Z\":5, \"Y\":1}\\n'\n",
    "    ])\n",
    "!cat systems_test_stripes_3   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stripes Systems Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections as cl\n",
    "\n",
    "test = cl.Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting buildStripes.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile buildStripes.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __future__ import division\n",
    "import re\n",
    "import mrjob\n",
    "import json\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import itertools\n",
    "import collections as cl\n",
    "import ast\n",
    "\n",
    "class MRbuildStripes(MRJob):\n",
    "  \n",
    "    #START SUDENT CODE531_STRIPES\n",
    "    \n",
    "    #OUTPUT_PROTOCOL = JSONValueProtocol\n",
    "    \n",
    "    def steps(self):\n",
    "        \n",
    "        JOBCONF_STEP = {\n",
    "            'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'stream.map.output.field.separator':'\\t',    \n",
    "            'mapreduce.partition.keycomparator.options': '-k1,1',\n",
    "            'mapreduce.job.reduces': '1'\n",
    "        }\n",
    "        \n",
    "        return [\n",
    "            MRStep(jobconf=JOBCONF_STEP,\n",
    "                   mapper=self.mapper,\n",
    "                   reducer_init=self.reducer_init,\n",
    "                   reducer=self.reducer\n",
    "                  )\n",
    "        ]\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        \n",
    "        #instantiate dict to hold stripes\n",
    "        stripes_dict = {}\n",
    "        \n",
    "        #parse the co-occurrence count from the line\n",
    "        co_occur_count = line.strip().split()[-3]\n",
    "\n",
    "        #get pairs of words in the 5-gram and create a stripe for each unique word\n",
    "        #Note the set here ensures that a word that appears more than once in a 5-gram won't be counted as co-occurring with itself\n",
    "        #Note the permutations here will count both (word1,word2) and (word2,word1)\n",
    "        for subset in itertools.permutations(sorted(set(line.strip().lower().split()[:-3])), 2):\n",
    "            if subset[0] not in stripes_dict.keys():\n",
    "                stripes_dict[subset[0]] = {}\n",
    "                stripes_dict[subset[0]][subset[1]] = int(co_occur_count)\n",
    "                \n",
    "            elif subset[1] not in stripes_dict[subset[0]]:\n",
    "                stripes_dict[subset[0]][subset[1]] = int(co_occur_count)\n",
    "                \n",
    "            else:\n",
    "                stripes_dict[subset[0]][subset[1]] += int(co_occur_count)\n",
    "        \n",
    "        for key in stripes_dict:\n",
    "            #yield key, json.dumps(stripes_dict[key])\n",
    "            yield key, json.dumps(stripes_dict[key])\n",
    "\n",
    "    def reducer_init(self):\n",
    "        #instantiate placholder current key and counter\n",
    "        self.placeholder_for_reducer = 0\n",
    "        \n",
    "    def reducer(self,key,stripe):\n",
    "        \n",
    "        #instantiate counter to hold combined stripes\n",
    "        cur_counter = cl.Counter()\n",
    "                \n",
    "        for stripe_dict in stripe:\n",
    "            cur_key = key\n",
    "            \n",
    "            #load the stripe into a dict\n",
    "            stripe_dict_to_add = json.loads(stripe_dict)\n",
    "            \n",
    "            #update with each stripe\n",
    "            cur_counter.update(stripe_dict_to_add)\n",
    "\n",
    "        #output the key and the counter with the sum of all co-occurrences for that key\n",
    "        yield key, cur_counter\n",
    "        \n",
    "        #The placeholder only increments when a new key is encountered\n",
    "        #reducer seems to get chunks of key value pairs from the mapper with the same key all together\n",
    "        self.placeholder_for_reducer += 1\n",
    "\n",
    "    #END SUDENT CODE531_STRIPES\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRbuildStripes.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `systems_test_stripes_1': No such file or directory\n",
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/buildStripes.nwchen24.20170620.041422.380269\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/nwchen24/tmp/mrjob/buildStripes.nwchen24.20170620.041422.380269/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob3607556336422736993.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1497906899862_0312\n",
      "  Submitted application application_1497906899862_0312\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1497906899862_0312/\n",
      "  Running job: job_1497906899862_0312\n",
      "  Job job_1497906899862_0312 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497906899862_0312 completed successfully\n",
      "  Output directory: hdfs:///user/nwchen24/tmp/mrjob/buildStripes.nwchen24.20170620.041422.380269/output\n",
      "Counters: 50\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=563\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2118\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1235\n",
      "\t\tFILE: Number of bytes written=402257\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1019\n",
      "\t\tHDFS: Number of bytes written=2118\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=52846080\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=49372160\n",
      "\t\tTotal time spent by all map tasks (ms)=34405\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=103215\n",
      "\t\tTotal time spent by all reduce tasks (ms)=19286\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=96430\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=34405\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=19286\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=4380\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=388\n",
      "\t\tInput split bytes=456\n",
      "\t\tMap input records=10\n",
      "\t\tMap output bytes=3408\n",
      "\t\tMap output materialized bytes=1258\n",
      "\t\tMap output records=49\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1912930304\n",
      "\t\tReduce input groups=28\n",
      "\t\tReduce input records=49\n",
      "\t\tReduce output records=28\n",
      "\t\tReduce shuffle bytes=1258\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=98\n",
      "\t\tTotal committed heap usage (bytes)=5218762752\n",
      "\t\tVirtual memory (bytes) snapshot=7754362880\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/nwchen24/tmp/mrjob/buildStripes.nwchen24.20170620.041422.380269/output...\n",
      "Removing HDFS temp directory hdfs:///user/nwchen24/tmp/mrjob/buildStripes.nwchen24.20170620.041422.380269...\n",
      "Removing temp directory /tmp/buildStripes.nwchen24.20170620.041422.380269...\n"
     ]
    }
   ],
   "source": [
    "###########################################################################\n",
    "# Make Stripes from ngrams for systems test 1\n",
    "###########################################################################\n",
    "\n",
    "!hdfs dfs -rm -r systems_test_stripes_1\n",
    "\n",
    "#Hadoop for altiscale\n",
    "!python buildStripes.py -r hadoop googlebooks-eng-all-5gram-20090715-0-filtered-first-10-lines.txt > systems_test_stripes_1 \\\n",
    "    --python-bin=./my_env27_for_hadoop_upload/bin/python \\\n",
    "    --archive=hdfs:///user/nwchen24/virtualenv/my_env27_for_hadoop_upload.zip#my_env27_for_hadoop_upload\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"a\"\t{\"limited\":55,\"female\":447,\"general\":92,\"sea\":62,\"in\":1201,\"religious\":59,\"george\":92,\"biography\":92,\"city\":62,\"for\":59,\"tales\":123,\"government\":102,\"by\":62,\"forms\":116,\"wales\":1099,\"christmas\":1099,\"child's\":1099,\"collection\":239,\"the\":124,\"case\":604,\"circumstantial\":62,\"of\":895,\"study\":604,\"bill\":59,\"establishing\":59,\"narrative\":62,\"fairy\":123}\r\n",
      "\"bill\"\t{\"a\":59,\"religious\":59,\"for\":59,\"establishing\":59}\r\n",
      "\"biography\"\t{\"a\":92,\"of\":92,\"george\":92,\"general\":92}\r\n",
      "\"by\"\t{\"a\":62,\"city\":62,\"the\":62,\"sea\":62}\r\n",
      "\"case\"\t{\"a\":604,\"limited\":55,\"government\":102,\"of\":502,\"study\":604,\"female\":447,\"in\":102}\r\n"
     ]
    }
   ],
   "source": [
    "!head -5 systems_test_stripes_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `systems_test_stripes_2': No such file or directory\n",
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/buildStripes.nwchen24.20170620.035158.798049\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/nwchen24/tmp/mrjob/buildStripes.nwchen24.20170620.035158.798049/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob2839351626151996338.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1497906899862_0283\n",
      "  Submitted application application_1497906899862_0283\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1497906899862_0283/\n",
      "  Running job: job_1497906899862_0283\n",
      "  Job job_1497906899862_0283 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 33% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 83% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497906899862_0283 completed successfully\n",
      "  Output directory: hdfs:///user/nwchen24/tmp/mrjob/buildStripes.nwchen24.20170620.035158.798049/output\n",
      "Counters: 50\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=101\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=147\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=150\n",
      "\t\tFILE: Number of bytes written=399994\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=483\n",
      "\t\tHDFS: Number of bytes written=147\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=3\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=3\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=91717632\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=63101440\n",
      "\t\tTotal time spent by all map tasks (ms)=59712\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=179136\n",
      "\t\tTotal time spent by all reduce tasks (ms)=24649\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=123245\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=59712\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=24649\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=5950\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=632\n",
      "\t\tInput split bytes=382\n",
      "\t\tMap input records=3\n",
      "\t\tMap output bytes=224\n",
      "\t\tMap output materialized bytes=191\n",
      "\t\tMap output records=7\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1929097216\n",
      "\t\tReduce input groups=4\n",
      "\t\tReduce input records=7\n",
      "\t\tReduce output records=4\n",
      "\t\tReduce shuffle bytes=191\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=14\n",
      "\t\tTotal committed heap usage (bytes)=5218762752\n",
      "\t\tVirtual memory (bytes) snapshot=7760134144\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/nwchen24/tmp/mrjob/buildStripes.nwchen24.20170620.035158.798049/output...\n",
      "Removing HDFS temp directory hdfs:///user/nwchen24/tmp/mrjob/buildStripes.nwchen24.20170620.035158.798049...\n",
      "Removing temp directory /tmp/buildStripes.nwchen24.20170620.035158.798049...\n"
     ]
    }
   ],
   "source": [
    "###########################################################################\n",
    "# Make Stripes from ngrams for systems test 2\n",
    "###########################################################################\n",
    "\n",
    "!hdfs dfs -rm -r systems_test_stripes_2\n",
    "\n",
    "#Hadoop for altiscale\n",
    "!python buildStripes.py -r hadoop atlas-boon-systems-test.txt > systems_test_stripes_2 \\\n",
    "    --python-bin=./my_env27_for_hadoop_upload/bin/python \\\n",
    "    --archive=hdfs:///user/nwchen24/virtualenv/my_env27_for_hadoop_upload.zip#my_env27_for_hadoop_upload\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"atlas\"\t{\"dipped\":15,\"boon\":50}\r\n",
      "\"boon\"\t{\"atlas\":50,\"dipped\":10,\"cava\":10}\r\n",
      "\"cava\"\t{\"dipped\":10,\"boon\":10}\r\n",
      "\"dipped\"\t{\"atlas\":15,\"boon\":10,\"cava\":10}\r\n"
     ]
    }
   ],
   "source": [
    "!cat systems_test_stripes_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverted Index Systems Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing invertedIndex.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile invertedIndex.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __future__ import division\n",
    "import collections as cl\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import itertools\n",
    "import mrjob\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import sys\n",
    "\n",
    "class MRinvertedIndex(MRJob):\n",
    "    #START SUDENT CODE531_INV_INDEX\n",
    "    \n",
    "    #OUTPUT_PROTOCOL = JSONValueProtocol\n",
    "\n",
    "    \n",
    "    def steps(self):\n",
    "        \n",
    "        JOBCONF_STEP = {\n",
    "            'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'stream.map.output.field.separator':'\\t',    \n",
    "            'mapreduce.partition.keycomparator.options': '-k1,1',\n",
    "            'mapreduce.job.reduces': '1'\n",
    "        }\n",
    "        \n",
    "        return [\n",
    "            MRStep(jobconf=JOBCONF_STEP,\n",
    "                   mapper=self.mapper,\n",
    "                   reducer=self.reducer\n",
    "                  )\n",
    "        ]\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        \n",
    "        #Read the key and the stripe from the input\n",
    "        key_stripe_list = line.translate(None,\"\\n\").split(\"\\t\")\n",
    "        \n",
    "        key = json.loads(key_stripe_list[0])\n",
    "        stripe = json.loads(key_stripe_list[1])\n",
    "        \n",
    "        #get length of the stripe\n",
    "        stripe_len = len(stripe)\n",
    "        \n",
    "        #create dict that will hold doc title and the stripe length\n",
    "        stripe_len_dict = {key:stripe_len}\n",
    "        \n",
    "        for term in stripe:\n",
    "            yield term, stripe_len_dict\n",
    "  \n",
    "    def reducer(self, key, stripe):\n",
    "\n",
    "        #instantiate a dict where we will combine stripe_len_dicts emitted from mapper\n",
    "        stripe_len_dict_combined = {}\n",
    "        \n",
    "        #Read the incremental stripe length dit from the mapper output\n",
    "        for stripe_len_dict in stripe:\n",
    "            \n",
    "            #update the dict above with each incremental stripe length dict\n",
    "            stripe_len_dict_combined.update(stripe_len_dict)\n",
    "                    \n",
    "        yield key, stripe_len_dict_combined        \n",
    "        \n",
    "        #END SUDENT CODE531_INV_INDEX\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    MRinvertedIndex.run() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `systems_test_inv_index_1': No such file or directory\n",
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/invertedIndex.nwchen24.20170620.041811.628831\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/nwchen24/tmp/mrjob/invertedIndex.nwchen24.20170620.041811.628831/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob8136557555499013975.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1497906899862_0316\n",
      "  Submitted application application_1497906899862_0316\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1497906899862_0316/\n",
      "  Running job: job_1497906899862_0316\n",
      "  Job job_1497906899862_0316 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497906899862_0316 completed successfully\n",
      "  Output directory: hdfs:///user/nwchen24/tmp/mrjob/invertedIndex.nwchen24.20170620.041811.628831/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3177\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1904\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1390\n",
      "\t\tFILE: Number of bytes written=402659\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3551\n",
      "\t\tHDFS: Number of bytes written=1904\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=21175296\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=10772480\n",
      "\t\tTotal time spent by all map tasks (ms)=13786\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=41358\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4208\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=21040\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=13786\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=4208\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2590\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=137\n",
      "\t\tInput split bytes=374\n",
      "\t\tMap input records=28\n",
      "\t\tMap output bytes=3150\n",
      "\t\tMap output materialized bytes=1586\n",
      "\t\tMap output records=158\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1896603648\n",
      "\t\tReduce input groups=28\n",
      "\t\tReduce input records=158\n",
      "\t\tReduce output records=28\n",
      "\t\tReduce shuffle bytes=1586\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=316\n",
      "\t\tTotal committed heap usage (bytes)=5218762752\n",
      "\t\tVirtual memory (bytes) snapshot=7741526016\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/nwchen24/tmp/mrjob/invertedIndex.nwchen24.20170620.041811.628831/output...\n",
      "Removing HDFS temp directory hdfs:///user/nwchen24/tmp/mrjob/invertedIndex.nwchen24.20170620.041811.628831...\n",
      "Removing temp directory /tmp/invertedIndex.nwchen24.20170620.041811.628831...\n"
     ]
    }
   ],
   "source": [
    "###########################################################################\n",
    "# Make Inverted Index from stripes in systems test 1\n",
    "###########################################################################\n",
    "\n",
    "!hdfs dfs -rm -r systems_test_inv_index_1\n",
    "\n",
    "#Hadoop for altiscale \n",
    "!python invertedIndex.py -r hadoop systems_test_stripes_1 > systems_test_inv_index_1 \\\n",
    "    --python-bin=./my_env27_for_hadoop_upload/bin/python \\\n",
    "    --archive=hdfs:///user/nwchen24/virtualenv/my_env27_for_hadoop_upload.zip#my_env27_for_hadoop_upload\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"a\"\t{\"limited\":4,\"bill\":4,\"sea\":4,\"government\":4,\"collection\":5,\"general\":4,\"female\":4,\"in\":7,\"establishing\":4,\"religious\":4,\"by\":4,\"biography\":4,\"christmas\":4,\"case\":7,\"city\":4,\"circumstantial\":4,\"fairy\":4,\"for\":4,\"of\":15,\"tales\":4,\"child's\":4,\"george\":4,\"forms\":3,\"narrative\":4,\"wales\":4,\"the\":7,\"study\":7}\r\n",
      "\"bill\"\t{\"a\":27,\"religious\":4,\"for\":4,\"establishing\":4}\r\n",
      "\"biography\"\t{\"a\":27,\"of\":15,\"george\":4,\"general\":4}\r\n",
      "\"by\"\t{\"a\":27,\"city\":4,\"the\":7,\"sea\":4}\r\n",
      "\"case\"\t{\"a\":27,\"limited\":4,\"female\":4,\"in\":7,\"of\":15,\"study\":7,\"government\":4}\r\n",
      "\"child's\"\t{\"a\":27,\"wales\":4,\"christmas\":4,\"in\":7}\r\n",
      "\"christmas\"\t{\"a\":27,\"wales\":4,\"child's\":4,\"in\":7}\r\n",
      "\"circumstantial\"\t{\"a\":27,\"of\":15,\"the\":7,\"narrative\":4}\r\n",
      "\"city\"\t{\"a\":27,\"the\":7,\"by\":4,\"sea\":4}\r\n",
      "\"collection\"\t{\"a\":27,\"forms\":3,\"fairy\":4,\"tales\":4,\"of\":15}\r\n",
      "\"establishing\"\t{\"a\":27,\"bill\":4,\"religious\":4,\"for\":4}\r\n",
      "\"fairy\"\t{\"a\":27,\"of\":15,\"tales\":4,\"collection\":5}\r\n",
      "\"female\"\t{\"case\":7,\"of\":15,\"study\":7,\"a\":27}\r\n",
      "\"for\"\t{\"a\":27,\"bill\":4,\"religious\":4,\"establishing\":4}\r\n",
      "\"forms\"\t{\"a\":27,\"of\":15,\"collection\":5}\r\n",
      "\"general\"\t{\"a\":27,\"of\":15,\"george\":4,\"biography\":4}\r\n",
      "\"george\"\t{\"a\":27,\"of\":15,\"biography\":4,\"general\":4}\r\n",
      "\"government\"\t{\"case\":7,\"a\":27,\"study\":7,\"in\":7}\r\n",
      "\"in\"\t{\"a\":27,\"case\":7,\"government\":4,\"wales\":4,\"study\":7,\"child's\":4,\"christmas\":4}\r\n",
      "\"limited\"\t{\"case\":7,\"a\":27,\"study\":7,\"of\":15}\r\n",
      "\"narrative\"\t{\"a\":27,\"of\":15,\"the\":7,\"circumstantial\":4}\r\n",
      "\"of\"\t{\"case\":7,\"a\":27,\"circumstantial\":4,\"limited\":4,\"the\":7,\"study\":7,\"collection\":5,\"general\":4,\"forms\":3,\"tales\":4,\"female\":4,\"narrative\":4,\"fairy\":4,\"george\":4,\"biography\":4}\r\n",
      "\"religious\"\t{\"a\":27,\"bill\":4,\"for\":4,\"establishing\":4}\r\n",
      "\"sea\"\t{\"a\":27,\"city\":4,\"the\":7,\"by\":4}\r\n",
      "\"study\"\t{\"case\":7,\"limited\":4,\"a\":27,\"female\":4,\"government\":4,\"of\":15,\"in\":7}\r\n",
      "\"tales\"\t{\"a\":27,\"of\":15,\"fairy\":4,\"collection\":5}\r\n",
      "\"the\"\t{\"a\":27,\"city\":4,\"circumstantial\":4,\"sea\":4,\"narrative\":4,\"of\":15,\"by\":4}\r\n",
      "\"wales\"\t{\"a\":27,\"in\":7,\"christmas\":4,\"child's\":4}\r\n"
     ]
    }
   ],
   "source": [
    "!cat systems_test_inv_index_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `systems_test_inv_index_2': No such file or directory\n",
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/invertedIndex.nwchen24.20170620.035905.029874\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/nwchen24/tmp/mrjob/invertedIndex.nwchen24.20170620.035905.029874/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob2932600839324587182.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1497906899862_0292\n",
      "  Submitted application application_1497906899862_0292\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1497906899862_0292/\n",
      "  Running job: job_1497906899862_0292\n",
      "  Job job_1497906899862_0292 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497906899862_0292 completed successfully\n",
      "  Output directory: hdfs:///user/nwchen24/tmp/mrjob/invertedIndex.nwchen24.20170620.035905.029874/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=221\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=137\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=141\n",
      "\t\tFILE: Number of bytes written=400008\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=595\n",
      "\t\tHDFS: Number of bytes written=137\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=13628928\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=17262080\n",
      "\t\tTotal time spent by all map tasks (ms)=8873\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=26619\n",
      "\t\tTotal time spent by all reduce tasks (ms)=6743\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=33715\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=8873\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=6743\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2770\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=156\n",
      "\t\tInput split bytes=374\n",
      "\t\tMap input records=4\n",
      "\t\tMap output bytes=196\n",
      "\t\tMap output materialized bytes=184\n",
      "\t\tMap output records=10\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1905311744\n",
      "\t\tReduce input groups=4\n",
      "\t\tReduce input records=10\n",
      "\t\tReduce output records=4\n",
      "\t\tReduce shuffle bytes=184\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=20\n",
      "\t\tTotal committed heap usage (bytes)=5218762752\n",
      "\t\tVirtual memory (bytes) snapshot=7772811264\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/nwchen24/tmp/mrjob/invertedIndex.nwchen24.20170620.035905.029874/output...\n",
      "Removing HDFS temp directory hdfs:///user/nwchen24/tmp/mrjob/invertedIndex.nwchen24.20170620.035905.029874...\n",
      "Removing temp directory /tmp/invertedIndex.nwchen24.20170620.035905.029874...\n"
     ]
    }
   ],
   "source": [
    "###########################################################################\n",
    "# Make Inverted Index from stripes in systems test 2\n",
    "###########################################################################\n",
    "\n",
    "!hdfs dfs -rm -r systems_test_inv_index_2\n",
    "\n",
    "#Hadoop for altiscale\n",
    "!python invertedIndex.py -r hadoop systems_test_stripes_2 > systems_test_inv_index_2 \\\n",
    "    --python-bin=./my_env27_for_hadoop_upload/bin/python \\\n",
    "    --archive=hdfs:///user/nwchen24/virtualenv/my_env27_for_hadoop_upload.zip#my_env27_for_hadoop_upload\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"atlas\"\t{\"dipped\":3,\"boon\":3}\r\n",
      "\"boon\"\t{\"atlas\":2,\"dipped\":3,\"cava\":2}\r\n",
      "\"cava\"\t{\"dipped\":3,\"boon\":3}\r\n",
      "\"dipped\"\t{\"atlas\":2,\"boon\":3,\"cava\":2}\r\n"
     ]
    }
   ],
   "source": [
    "!cat systems_test_inv_index_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `systems_test_inv_index_3': No such file or directory\n",
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/invertedIndex.nwchen24.20170620.040135.528208\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/nwchen24/tmp/mrjob/invertedIndex.nwchen24.20170620.040135.528208/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob440172574859841418.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1497906899862_0295\n",
      "  Submitted application application_1497906899862_0295\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1497906899862_0295/\n",
      "  Running job: job_1497906899862_0295\n",
      "  Job job_1497906899862_0295 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 33% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 83% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497906899862_0295 completed successfully\n",
      "  Output directory: hdfs:///user/nwchen24/tmp/mrjob/invertedIndex.nwchen24.20170620.040135.528208/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=140\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=111\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=90\n",
      "\t\tFILE: Number of bytes written=399895\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=514\n",
      "\t\tHDFS: Number of bytes written=111\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=87168000\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=53852160\n",
      "\t\tTotal time spent by all map tasks (ms)=56750\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=170250\n",
      "\t\tTotal time spent by all reduce tasks (ms)=21036\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=105180\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=56750\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=21036\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=8160\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=336\n",
      "\t\tInput split bytes=374\n",
      "\t\tMap input records=3\n",
      "\t\tMap output bytes=135\n",
      "\t\tMap output materialized bytes=125\n",
      "\t\tMap output records=9\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1897029632\n",
      "\t\tReduce input groups=5\n",
      "\t\tReduce input records=9\n",
      "\t\tReduce output records=5\n",
      "\t\tReduce shuffle bytes=125\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=18\n",
      "\t\tTotal committed heap usage (bytes)=5218762752\n",
      "\t\tVirtual memory (bytes) snapshot=7770423296\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/nwchen24/tmp/mrjob/invertedIndex.nwchen24.20170620.040135.528208/output...\n",
      "Removing HDFS temp directory hdfs:///user/nwchen24/tmp/mrjob/invertedIndex.nwchen24.20170620.040135.528208...\n",
      "Removing temp directory /tmp/invertedIndex.nwchen24.20170620.040135.528208...\n"
     ]
    }
   ],
   "source": [
    "###########################################################################\n",
    "# Make Inverted Index from stripes in systems test 3\n",
    "###########################################################################\n",
    "\n",
    "!hdfs dfs -rm -r systems_test_inv_index_3\n",
    "\n",
    "#Hadoop for altiscale\n",
    "!python invertedIndex.py -r hadoop systems_test_stripes_3 > systems_test_inv_index_3 \\\n",
    "    --python-bin=./my_env27_for_hadoop_upload/bin/python \\\n",
    "    --archive=hdfs:///user/nwchen24/virtualenv/my_env27_for_hadoop_upload.zip#my_env27_for_hadoop_upload\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"M\"\t{\"DocC\":4}\r\n",
      "\"N\"\t{\"DocC\":4}\r\n",
      "\"X\"\t{\"DocB\":2,\"DocA\":3}\r\n",
      "\"Y\"\t{\"DocB\":2,\"DocC\":4,\"DocA\":3}\r\n",
      "\"Z\"\t{\"DocC\":4,\"DocA\":3}\r\n"
     ]
    }
   ],
   "source": [
    "!cat systems_test_inv_index_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Systems Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting similarity.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile similarity.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __future__ import division\n",
    "import collections\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import itertools\n",
    "import mrjob\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class MRsimilarity(MRJob):\n",
    "  \n",
    "    #START SUDENT CODE531_SIMILARITY\n",
    "    #OUTPUT_PROTOCOL = JSONValueProtocol\n",
    "    \n",
    "    def steps(self):\n",
    "        \n",
    "        JOBCONF_STEP = {\n",
    "            'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'stream.map.output.field.separator':'\\t',    \n",
    "            'stream.num.map.output.key.fields': '2',\n",
    "            'mapreduce.partition.keycomparator.options': '-k1,1',\n",
    "            'mapreduce.job.reduces': '1'\n",
    "        }\n",
    "        \n",
    "        return [\n",
    "            MRStep(jobconf=JOBCONF_STEP,\n",
    "                   mapper=self.mapper,\n",
    "                   reducer=self.reducer\n",
    "                  )\n",
    "        ]\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        \n",
    "        #Read the key and the stripe from the input\n",
    "        key_index_list = line.translate(None,\"\\n\").split(\"\\t\")\n",
    "        \n",
    "        key = json.loads(key_index_list[0])\n",
    "        inv_index = json.loads(key_index_list[1])\n",
    "        \n",
    "        for subset in itertools.permutations(sorted(set(inv_index.keys())), 2):\n",
    "            \n",
    "            #get the lengths of each item that we want to output\n",
    "            item_1_len = inv_index[subset[0]]\n",
    "            item_2_len = inv_index[subset[1]]\n",
    "            \n",
    "            #create a two item dict to hold the item names and their lengths\n",
    "            item_len_dict = {}\n",
    "            item_len_dict[subset[0]] = item_1_len\n",
    "            item_len_dict[subset[1]] = item_2_len\n",
    "            \n",
    "            #when output like this, can index with key[0] to get subset[0] in the reducer\n",
    "            yield (subset[0], subset[1]), (1,item_len_dict)\n",
    "  \n",
    "    def reducer(self, key, stripe):\n",
    "\n",
    "        #instantiate object to hold the number of times the terms / documents intersect\n",
    "        items_intersect = 0\n",
    "        cosine_sim_product = 0\n",
    "        \n",
    "        #the reducer gets output from the mapper in chunks grouped by key\n",
    "        for n,item_len_dict in stripe:\n",
    "            \n",
    "            #get the length of item 1 and item 2\n",
    "            item_1_len = item_len_dict[key[0]]\n",
    "            item_2_len = item_len_dict[key[1]]\n",
    "            \n",
    "            #increment the intersect count\n",
    "            items_intersect = items_intersect + n\n",
    "            \n",
    "            cosine_sim_product = cosine_sim_product + (1/np.sqrt(item_1_len)) * (1/np.sqrt(item_2_len))\n",
    "            \n",
    "        #calculate jacard similarity score\n",
    "        jacard_sim_score = float(items_intersect)/ float(item_1_len + item_2_len - items_intersect)\n",
    "        \n",
    "        #Record cosine similarity score before moving on to the next chunk\n",
    "        cosine_sim_score = cosine_sim_product\n",
    "        \n",
    "        yield key, (jacard_sim_score, cosine_sim_score)   \n",
    "    \n",
    "    #END SUDENT CODE531_SIMILARITY\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRsimilarity.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `systems_test_doc_similarity_1': No such file or directory\n",
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/similarity.nwchen24.20170620.042027.319313\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/nwchen24/tmp/mrjob/similarity.nwchen24.20170620.042027.319313/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob2316812653356586156.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1497906899862_0320\n",
      "  Submitted application application_1497906899862_0320\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1497906899862_0320/\n",
      "  Running job: job_1497906899862_0320\n",
      "  Job job_1497906899862_0320 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497906899862_0320 completed successfully\n",
      "  Output directory: hdfs:///user/nwchen24/tmp/mrjob/similarity.nwchen24.20170620.042027.319313/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2856\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=31230\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=12642\n",
      "\t\tFILE: Number of bytes written=428379\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3228\n",
      "\t\tHDFS: Number of bytes written=31230\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=20278272\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=17538560\n",
      "\t\tTotal time spent by all map tasks (ms)=13202\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=39606\n",
      "\t\tTotal time spent by all reduce tasks (ms)=6851\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=34255\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=13202\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=6851\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=3030\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=83\n",
      "\t\tInput split bytes=372\n",
      "\t\tMap input records=28\n",
      "\t\tMap output bytes=64480\n",
      "\t\tMap output materialized bytes=15760\n",
      "\t\tMap output records=1346\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1901879296\n",
      "\t\tReduce input groups=756\n",
      "\t\tReduce input records=1346\n",
      "\t\tReduce output records=756\n",
      "\t\tReduce shuffle bytes=15760\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=2692\n",
      "\t\tTotal committed heap usage (bytes)=5218762752\n",
      "\t\tVirtual memory (bytes) snapshot=7762829312\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/nwchen24/tmp/mrjob/similarity.nwchen24.20170620.042027.319313/output...\n",
      "Removing HDFS temp directory hdfs:///user/nwchen24/tmp/mrjob/similarity.nwchen24.20170620.042027.319313...\n",
      "Removing temp directory /tmp/similarity.nwchen24.20170620.042027.319313...\n"
     ]
    }
   ],
   "source": [
    "###########################################################################\n",
    "# Document similarity from inverted index\n",
    "###########################################################################\n",
    "\n",
    "!hdfs dfs -rm -r systems_test_doc_similarity_1\n",
    "\n",
    "#Hadoop for altiscale\n",
    "!python similarity.py -r hadoop systems_test_inv_index_1 > systems_test_doc_similarity_1 \\\n",
    "    --python-bin=./my_env27_for_hadoop_upload/bin/python \\\n",
    "    --archive=hdfs:///user/nwchen24/virtualenv/my_env27_for_hadoop_upload.zip#my_env27_for_hadoop_upload\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"a\",\"bill\"]\t[0.1071428571,0.2886751346]\r\n",
      "[\"a\",\"biography\"]\t[0.1071428571,0.2886751346]\r\n",
      "[\"a\",\"by\"]\t[0.1071428571,0.2886751346]\r\n",
      "[\"a\",\"case\"]\t[0.2142857143,0.4364357805]\r\n",
      "[\"a\",\"child's\"]\t[0.1071428571,0.2886751346]\r\n",
      "[\"a\",\"christmas\"]\t[0.1071428571,0.2886751346]\r\n",
      "[\"a\",\"circumstantial\"]\t[0.1071428571,0.2886751346]\r\n",
      "[\"a\",\"city\"]\t[0.1071428571,0.2886751346]\r\n",
      "[\"a\",\"collection\"]\t[0.1428571429,0.3442651863]\r\n",
      "[\"a\",\"establishing\"]\t[0.1071428571,0.2886751346]\r\n",
      "[\"a\",\"fairy\"]\t[0.1071428571,0.2886751346]\r\n",
      "[\"a\",\"female\"]\t[0.1071428571,0.2886751346]\r\n",
      "[\"a\",\"for\"]\t[0.1071428571,0.2886751346]\r\n",
      "[\"a\",\"forms\"]\t[0.0714285714,0.2222222222]\r\n",
      "[\"a\",\"general\"]\t[0.1071428571,0.2886751346]\r\n",
      "[\"a\",\"george\"]\t[0.1071428571,0.2886751346]\r\n",
      "[\"a\",\"government\"]\t[0.1071428571,0.2886751346]\r\n",
      "[\"a\",\"in\"]\t[0.2142857143,0.4364357805]\r\n",
      "[\"a\",\"limited\"]\t[0.1071428571,0.2886751346]\r\n",
      "[\"a\",\"narrative\"]\t[0.1071428571,0.2886751346]\r\n",
      "[\"a\",\"of\"]\t[0.5,0.695665593]\r\n",
      "[\"a\",\"religious\"]\t[0.1071428571,0.2886751346]\r\n",
      "[\"a\",\"sea\"]\t[0.1071428571,0.2886751346]\r\n",
      "[\"a\",\"study\"]\t[0.2142857143,0.4364357805]\r\n",
      "[\"a\",\"tales\"]\t[0.1071428571,0.2886751346]\r\n",
      "[\"a\",\"the\"]\t[0.2142857143,0.4364357805]\r\n",
      "[\"a\",\"wales\"]\t[0.1071428571,0.2886751346]\r\n",
      "[\"bill\",\"a\"]\t[0.1071428571,0.2886751346]\r\n",
      "[\"bill\",\"biography\"]\t[0.1428571429,0.25]\r\n",
      "[\"bill\",\"by\"]\t[0.1428571429,0.25]\r\n",
      "[\"bill\",\"case\"]\t[0.1,0.1889822365]\r\n",
      "[\"bill\",\"child's\"]\t[0.1428571429,0.25]\r\n",
      "[\"bill\",\"christmas\"]\t[0.1428571429,0.25]\r\n",
      "[\"bill\",\"circumstantial\"]\t[0.1428571429,0.25]\r\n",
      "[\"bill\",\"city\"]\t[0.1428571429,0.25]\r\n",
      "[\"bill\",\"collection\"]\t[0.125,0.2236067977]\r\n",
      "[\"bill\",\"establishing\"]\t[0.6,0.75]\r\n",
      "[\"bill\",\"fairy\"]\t[0.1428571429,0.25]\r\n",
      "[\"bill\",\"female\"]\t[0.1428571429,0.25]\r\n",
      "[\"bill\",\"for\"]\t[0.6,0.75]\r\n",
      "[\"bill\",\"forms\"]\t[0.1666666667,0.2886751346]\r\n",
      "[\"bill\",\"general\"]\t[0.1428571429,0.25]\r\n",
      "[\"bill\",\"george\"]\t[0.1428571429,0.25]\r\n",
      "[\"bill\",\"government\"]\t[0.1428571429,0.25]\r\n",
      "[\"bill\",\"in\"]\t[0.1,0.1889822365]\r\n",
      "[\"bill\",\"limited\"]\t[0.1428571429,0.25]\r\n",
      "[\"bill\",\"narrative\"]\t[0.1428571429,0.25]\r\n",
      "[\"bill\",\"of\"]\t[0.0555555556,0.1290994449]\r\n",
      "[\"bill\",\"religious\"]\t[0.6,0.75]\r\n",
      "[\"bill\",\"sea\"]\t[0.1428571429,0.25]\r\n",
      "[\"bill\",\"study\"]\t[0.1,0.1889822365]\r\n",
      "[\"bill\",\"tales\"]\t[0.1428571429,0.25]\r\n",
      "[\"bill\",\"the\"]\t[0.1,0.1889822365]\r\n",
      "[\"bill\",\"wales\"]\t[0.1428571429,0.25]\r\n",
      "[\"biography\",\"a\"]\t[0.1071428571,0.2886751346]\r\n",
      "[\"biography\",\"bill\"]\t[0.1428571429,0.25]\r\n",
      "[\"biography\",\"by\"]\t[0.1428571429,0.25]\r\n",
      "[\"biography\",\"case\"]\t[0.2222222222,0.377964473]\r\n",
      "[\"biography\",\"child's\"]\t[0.1428571429,0.25]\r\n",
      "[\"biography\",\"christmas\"]\t[0.1428571429,0.25]\r\n",
      "[\"biography\",\"circumstantial\"]\t[0.3333333333,0.5]\r\n",
      "[\"biography\",\"city\"]\t[0.1428571429,0.25]\r\n",
      "[\"biography\",\"collection\"]\t[0.2857142857,0.4472135955]\r\n",
      "[\"biography\",\"establishing\"]\t[0.1428571429,0.25]\r\n",
      "[\"biography\",\"fairy\"]\t[0.3333333333,0.5]\r\n",
      "[\"biography\",\"female\"]\t[0.3333333333,0.5]\r\n",
      "[\"biography\",\"for\"]\t[0.1428571429,0.25]\r\n",
      "[\"biography\",\"forms\"]\t[0.4,0.5773502692]\r\n",
      "[\"biography\",\"general\"]\t[0.6,0.75]\r\n",
      "[\"biography\",\"george\"]\t[0.6,0.75]\r\n",
      "[\"biography\",\"government\"]\t[0.1428571429,0.25]\r\n",
      "[\"biography\",\"in\"]\t[0.1,0.1889822365]\r\n",
      "[\"biography\",\"limited\"]\t[0.3333333333,0.5]\r\n",
      "[\"biography\",\"narrative\"]\t[0.3333333333,0.5]\r\n",
      "[\"biography\",\"of\"]\t[0.1875,0.3872983346]\r\n",
      "[\"biography\",\"religious\"]\t[0.1428571429,0.25]\r\n",
      "[\"biography\",\"sea\"]\t[0.1428571429,0.25]\r\n",
      "[\"biography\",\"study\"]\t[0.2222222222,0.377964473]\r\n",
      "[\"biography\",\"tales\"]\t[0.3333333333,0.5]\r\n",
      "[\"biography\",\"the\"]\t[0.2222222222,0.377964473]\r\n",
      "[\"biography\",\"wales\"]\t[0.1428571429,0.25]\r\n",
      "[\"by\",\"a\"]\t[0.1071428571,0.2886751346]\r\n",
      "[\"by\",\"bill\"]\t[0.1428571429,0.25]\r\n",
      "[\"by\",\"biography\"]\t[0.1428571429,0.25]\r\n",
      "[\"by\",\"case\"]\t[0.1,0.1889822365]\r\n",
      "[\"by\",\"child's\"]\t[0.1428571429,0.25]\r\n",
      "[\"by\",\"christmas\"]\t[0.1428571429,0.25]\r\n",
      "[\"by\",\"circumstantial\"]\t[0.3333333333,0.5]\r\n",
      "[\"by\",\"city\"]\t[0.6,0.75]\r\n",
      "[\"by\",\"collection\"]\t[0.125,0.2236067977]\r\n",
      "[\"by\",\"establishing\"]\t[0.1428571429,0.25]\r\n",
      "[\"by\",\"fairy\"]\t[0.1428571429,0.25]\r\n",
      "[\"by\",\"female\"]\t[0.1428571429,0.25]\r\n",
      "[\"by\",\"for\"]\t[0.1428571429,0.25]\r\n",
      "[\"by\",\"forms\"]\t[0.1666666667,0.2886751346]\r\n",
      "[\"by\",\"general\"]\t[0.1428571429,0.25]\r\n",
      "[\"by\",\"george\"]\t[0.1428571429,0.25]\r\n",
      "[\"by\",\"government\"]\t[0.1428571429,0.25]\r\n",
      "[\"by\",\"in\"]\t[0.1,0.1889822365]\r\n",
      "[\"by\",\"limited\"]\t[0.1428571429,0.25]\r\n",
      "[\"by\",\"narrative\"]\t[0.3333333333,0.5]\r\n",
      "[\"by\",\"of\"]\t[0.1176470588,0.2581988897]\r\n",
      "[\"by\",\"religious\"]\t[0.1428571429,0.25]\r\n",
      "[\"by\",\"sea\"]\t[0.6,0.75]\r\n",
      "[\"by\",\"study\"]\t[0.1,0.1889822365]\r\n",
      "[\"by\",\"tales\"]\t[0.1428571429,0.25]\r\n",
      "[\"by\",\"the\"]\t[0.375,0.5669467095]\r\n",
      "[\"by\",\"wales\"]\t[0.1428571429,0.25]\r\n",
      "[\"case\",\"a\"]\t[0.2142857143,0.4364357805]\r\n",
      "[\"case\",\"bill\"]\t[0.1,0.1889822365]\r\n",
      "[\"case\",\"biography\"]\t[0.2222222222,0.377964473]\r\n",
      "[\"case\",\"by\"]\t[0.1,0.1889822365]\r\n",
      "[\"case\",\"child's\"]\t[0.2222222222,0.377964473]\r\n",
      "[\"case\",\"christmas\"]\t[0.2222222222,0.377964473]\r\n",
      "[\"case\",\"circumstantial\"]\t[0.2222222222,0.377964473]\r\n",
      "[\"case\",\"city\"]\t[0.1,0.1889822365]\r\n",
      "[\"case\",\"collection\"]\t[0.2,0.3380617019]\r\n",
      "[\"case\",\"establishing\"]\t[0.1,0.1889822365]\r\n",
      "[\"case\",\"fairy\"]\t[0.2222222222,0.377964473]\r\n",
      "[\"case\",\"female\"]\t[0.375,0.5669467095]\r\n",
      "[\"case\",\"for\"]\t[0.1,0.1889822365]\r\n",
      "[\"case\",\"forms\"]\t[0.25,0.4364357805]\r\n",
      "[\"case\",\"general\"]\t[0.2222222222,0.377964473]\r\n",
      "[\"case\",\"george\"]\t[0.2222222222,0.377964473]\r\n",
      "[\"case\",\"government\"]\t[0.375,0.5669467095]\r\n",
      "[\"case\",\"in\"]\t[0.2727272727,0.4285714286]\r\n",
      "[\"case\",\"limited\"]\t[0.375,0.5669467095]\r\n",
      "[\"case\",\"narrative\"]\t[0.2222222222,0.377964473]\r\n",
      "[\"case\",\"of\"]\t[0.2222222222,0.3903600292]\r\n",
      "[\"case\",\"religious\"]\t[0.1,0.1889822365]\r\n",
      "[\"case\",\"sea\"]\t[0.1,0.1889822365]\r\n",
      "[\"case\",\"study\"]\t[0.75,0.8571428571]\r\n",
      "[\"case\",\"tales\"]\t[0.2222222222,0.377964473]\r\n",
      "[\"case\",\"the\"]\t[0.1666666667,0.2857142857]\r\n",
      "[\"case\",\"wales\"]\t[0.2222222222,0.377964473]\r\n",
      "[\"child's\",\"a\"]\t[0.1071428571,0.2886751346]\r\n",
      "[\"child's\",\"bill\"]\t[0.1428571429,0.25]\r\n",
      "[\"child's\",\"biography\"]\t[0.1428571429,0.25]\r\n",
      "[\"child's\",\"by\"]\t[0.1428571429,0.25]\r\n",
      "[\"child's\",\"case\"]\t[0.2222222222,0.377964473]\r\n",
      "[\"child's\",\"christmas\"]\t[0.6,0.75]\r\n",
      "[\"child's\",\"circumstantial\"]\t[0.1428571429,0.25]\r\n",
      "[\"child's\",\"city\"]\t[0.1428571429,0.25]\r\n",
      "[\"child's\",\"collection\"]\t[0.125,0.2236067977]\r\n",
      "[\"child's\",\"establishing\"]\t[0.1428571429,0.25]\r\n",
      "[\"child's\",\"fairy\"]\t[0.1428571429,0.25]\r\n",
      "[\"child's\",\"female\"]\t[0.1428571429,0.25]\r\n",
      "[\"child's\",\"for\"]\t[0.1428571429,0.25]\r\n",
      "[\"child's\",\"forms\"]\t[0.1666666667,0.2886751346]\r\n",
      "[\"child's\",\"general\"]\t[0.1428571429,0.25]\r\n",
      "[\"child's\",\"george\"]\t[0.1428571429,0.25]\r\n",
      "[\"child's\",\"government\"]\t[0.3333333333,0.5]\r\n",
      "[\"child's\",\"in\"]\t[0.375,0.5669467095]\r\n",
      "[\"child's\",\"limited\"]\t[0.1428571429,0.25]\r\n",
      "[\"child's\",\"narrative\"]\t[0.1428571429,0.25]\r\n",
      "[\"child's\",\"of\"]\t[0.0555555556,0.1290994449]\r\n",
      "[\"child's\",\"religious\"]\t[0.1428571429,0.25]\r\n",
      "[\"child's\",\"sea\"]\t[0.1428571429,0.25]\r\n",
      "[\"child's\",\"study\"]\t[0.2222222222,0.377964473]\r\n",
      "[\"child's\",\"tales\"]\t[0.1428571429,0.25]\r\n",
      "[\"child's\",\"the\"]\t[0.1,0.1889822365]\r\n",
      "[\"child's\",\"wales\"]\t[0.6,0.75]\r\n",
      "[\"christmas\",\"a\"]\t[0.1071428571,0.2886751346]\r\n",
      "[\"christmas\",\"bill\"]\t[0.1428571429,0.25]\r\n",
      "[\"christmas\",\"biography\"]\t[0.1428571429,0.25]\r\n",
      "[\"christmas\",\"by\"]\t[0.1428571429,0.25]\r\n",
      "[\"christmas\",\"case\"]\t[0.2222222222,0.377964473]\r\n",
      "[\"christmas\",\"child's\"]\t[0.6,0.75]\r\n",
      "[\"christmas\",\"circumstantial\"]\t[0.1428571429,0.25]\r\n",
      "[\"christmas\",\"city\"]\t[0.1428571429,0.25]\r\n",
      "[\"christmas\",\"collection\"]\t[0.125,0.2236067977]\r\n",
      "[\"christmas\",\"establishing\"]\t[0.1428571429,0.25]\r\n",
      "[\"christmas\",\"fairy\"]\t[0.1428571429,0.25]\r\n",
      "[\"christmas\",\"female\"]\t[0.1428571429,0.25]\r\n",
      "[\"christmas\",\"for\"]\t[0.1428571429,0.25]\r\n",
      "[\"christmas\",\"forms\"]\t[0.1666666667,0.2886751346]\r\n",
      "[\"christmas\",\"general\"]\t[0.1428571429,0.25]\r\n",
      "[\"christmas\",\"george\"]\t[0.1428571429,0.25]\r\n",
      "[\"christmas\",\"government\"]\t[0.3333333333,0.5]\r\n",
      "[\"christmas\",\"in\"]\t[0.375,0.5669467095]\r\n",
      "[\"christmas\",\"limited\"]\t[0.1428571429,0.25]\r\n",
      "[\"christmas\",\"narrative\"]\t[0.1428571429,0.25]\r\n",
      "[\"christmas\",\"of\"]\t[0.0555555556,0.1290994449]\r\n",
      "[\"christmas\",\"religious\"]\t[0.1428571429,0.25]\r\n",
      "[\"christmas\",\"sea\"]\t[0.1428571429,0.25]\r\n",
      "[\"christmas\",\"study\"]\t[0.2222222222,0.377964473]\r\n",
      "[\"christmas\",\"tales\"]\t[0.1428571429,0.25]\r\n",
      "[\"christmas\",\"the\"]\t[0.1,0.1889822365]\r\n",
      "[\"christmas\",\"wales\"]\t[0.6,0.75]\r\n",
      "[\"circumstantial\",\"a\"]\t[0.1071428571,0.2886751346]\r\n",
      "[\"circumstantial\",\"bill\"]\t[0.1428571429,0.25]\r\n",
      "[\"circumstantial\",\"biography\"]\t[0.3333333333,0.5]\r\n",
      "[\"circumstantial\",\"by\"]\t[0.3333333333,0.5]\r\n",
      "[\"circumstantial\",\"case\"]\t[0.2222222222,0.377964473]\r\n",
      "[\"circumstantial\",\"child's\"]\t[0.1428571429,0.25]\r\n",
      "[\"circumstantial\",\"christmas\"]\t[0.1428571429,0.25]\r\n",
      "[\"circumstantial\",\"city\"]\t[0.3333333333,0.5]\r\n",
      "[\"circumstantial\",\"collection\"]\t[0.2857142857,0.4472135955]\r\n",
      "[\"circumstantial\",\"establishing\"]\t[0.1428571429,0.25]\r\n",
      "[\"circumstantial\",\"fairy\"]\t[0.3333333333,0.5]\r\n",
      "[\"circumstantial\",\"female\"]\t[0.3333333333,0.5]\r\n",
      "[\"circumstantial\",\"for\"]\t[0.1428571429,0.25]\r\n",
      "[\"circumstantial\",\"forms\"]\t[0.4,0.5773502692]\r\n",
      "[\"circumstantial\",\"general\"]\t[0.3333333333,0.5]\r\n",
      "[\"circumstantial\",\"george\"]\t[0.3333333333,0.5]\r\n",
      "[\"circumstantial\",\"government\"]\t[0.1428571429,0.25]\r\n",
      "[\"circumstantial\",\"in\"]\t[0.1,0.1889822365]\r\n",
      "[\"circumstantial\",\"limited\"]\t[0.3333333333,0.5]\r\n",
      "[\"circumstantial\",\"narrative\"]\t[0.6,0.75]\r\n",
      "[\"circumstantial\",\"of\"]\t[0.1875,0.3872983346]\r\n",
      "[\"circumstantial\",\"religious\"]\t[0.1428571429,0.25]\r\n",
      "[\"circumstantial\",\"sea\"]\t[0.3333333333,0.5]\r\n",
      "[\"circumstantial\",\"study\"]\t[0.2222222222,0.377964473]\r\n",
      "[\"circumstantial\",\"tales\"]\t[0.3333333333,0.5]\r\n",
      "[\"circumstantial\",\"the\"]\t[0.375,0.5669467095]\r\n",
      "[\"circumstantial\",\"wales\"]\t[0.1428571429,0.25]\r\n",
      "[\"city\",\"a\"]\t[0.1071428571,0.2886751346]\r\n",
      "[\"city\",\"bill\"]\t[0.1428571429,0.25]\r\n",
      "[\"city\",\"biography\"]\t[0.1428571429,0.25]\r\n",
      "[\"city\",\"by\"]\t[0.6,0.75]\r\n",
      "[\"city\",\"case\"]\t[0.1,0.1889822365]\r\n",
      "[\"city\",\"child's\"]\t[0.1428571429,0.25]\r\n",
      "[\"city\",\"christmas\"]\t[0.1428571429,0.25]\r\n",
      "[\"city\",\"circumstantial\"]\t[0.3333333333,0.5]\r\n",
      "[\"city\",\"collection\"]\t[0.125,0.2236067977]\r\n",
      "[\"city\",\"establishing\"]\t[0.1428571429,0.25]\r\n",
      "[\"city\",\"fairy\"]\t[0.1428571429,0.25]\r\n",
      "[\"city\",\"female\"]\t[0.1428571429,0.25]\r\n",
      "[\"city\",\"for\"]\t[0.1428571429,0.25]\r\n",
      "[\"city\",\"forms\"]\t[0.1666666667,0.2886751346]\r\n",
      "[\"city\",\"general\"]\t[0.1428571429,0.25]\r\n",
      "[\"city\",\"george\"]\t[0.1428571429,0.25]\r\n",
      "[\"city\",\"government\"]\t[0.1428571429,0.25]\r\n",
      "[\"city\",\"in\"]\t[0.1,0.1889822365]\r\n",
      "[\"city\",\"limited\"]\t[0.1428571429,0.25]\r\n",
      "[\"city\",\"narrative\"]\t[0.3333333333,0.5]\r\n",
      "[\"city\",\"of\"]\t[0.1176470588,0.2581988897]\r\n",
      "[\"city\",\"religious\"]\t[0.1428571429,0.25]\r\n",
      "[\"city\",\"sea\"]\t[0.6,0.75]\r\n",
      "[\"city\",\"study\"]\t[0.1,0.1889822365]\r\n",
      "[\"city\",\"tales\"]\t[0.1428571429,0.25]\r\n",
      "[\"city\",\"the\"]\t[0.375,0.5669467095]\r\n",
      "[\"city\",\"wales\"]\t[0.1428571429,0.25]\r\n",
      "[\"collection\",\"a\"]\t[0.1428571429,0.3442651863]\r\n",
      "[\"collection\",\"bill\"]\t[0.125,0.2236067977]\r\n",
      "[\"collection\",\"biography\"]\t[0.2857142857,0.4472135955]\r\n",
      "[\"collection\",\"by\"]\t[0.125,0.2236067977]\r\n",
      "[\"collection\",\"case\"]\t[0.2,0.3380617019]\r\n",
      "[\"collection\",\"child's\"]\t[0.125,0.2236067977]\r\n",
      "[\"collection\",\"christmas\"]\t[0.125,0.2236067977]\r\n",
      "[\"collection\",\"circumstantial\"]\t[0.2857142857,0.4472135955]\r\n",
      "[\"collection\",\"city\"]\t[0.125,0.2236067977]\r\n",
      "[\"collection\",\"establishing\"]\t[0.125,0.2236067977]\r\n",
      "[\"collection\",\"fairy\"]\t[0.5,0.6708203932]\r\n",
      "[\"collection\",\"female\"]\t[0.2857142857,0.4472135955]\r\n",
      "[\"collection\",\"for\"]\t[0.125,0.2236067977]\r\n",
      "[\"collection\",\"forms\"]\t[0.3333333333,0.5163977795]\r\n",
      "[\"collection\",\"general\"]\t[0.2857142857,0.4472135955]\r\n",
      "[\"collection\",\"george\"]\t[0.2857142857,0.4472135955]\r\n",
      "[\"collection\",\"government\"]\t[0.125,0.2236067977]\r\n",
      "[\"collection\",\"in\"]\t[0.0909090909,0.1690308509]\r\n",
      "[\"collection\",\"limited\"]\t[0.2857142857,0.4472135955]\r\n",
      "[\"collection\",\"narrative\"]\t[0.2857142857,0.4472135955]\r\n",
      "[\"collection\",\"of\"]\t[0.25,0.4618802154]\r\n",
      "[\"collection\",\"religious\"]\t[0.125,0.2236067977]\r\n",
      "[\"collection\",\"sea\"]\t[0.125,0.2236067977]\r\n",
      "[\"collection\",\"study\"]\t[0.2,0.3380617019]\r\n",
      "[\"collection\",\"tales\"]\t[0.5,0.6708203932]\r\n",
      "[\"collection\",\"the\"]\t[0.2,0.3380617019]\r\n",
      "[\"collection\",\"wales\"]\t[0.125,0.2236067977]\r\n",
      "[\"establishing\",\"a\"]\t[0.1071428571,0.2886751346]\r\n",
      "[\"establishing\",\"bill\"]\t[0.6,0.75]\r\n",
      "[\"establishing\",\"biography\"]\t[0.1428571429,0.25]\r\n",
      "[\"establishing\",\"by\"]\t[0.1428571429,0.25]\r\n",
      "[\"establishing\",\"case\"]\t[0.1,0.1889822365]\r\n",
      "[\"establishing\",\"child's\"]\t[0.1428571429,0.25]\r\n",
      "[\"establishing\",\"christmas\"]\t[0.1428571429,0.25]\r\n",
      "[\"establishing\",\"circumstantial\"]\t[0.1428571429,0.25]\r\n",
      "[\"establishing\",\"city\"]\t[0.1428571429,0.25]\r\n",
      "[\"establishing\",\"collection\"]\t[0.125,0.2236067977]\r\n",
      "[\"establishing\",\"fairy\"]\t[0.1428571429,0.25]\r\n",
      "[\"establishing\",\"female\"]\t[0.1428571429,0.25]\r\n",
      "[\"establishing\",\"for\"]\t[0.6,0.75]\r\n",
      "[\"establishing\",\"forms\"]\t[0.1666666667,0.2886751346]\r\n",
      "[\"establishing\",\"general\"]\t[0.1428571429,0.25]\r\n",
      "[\"establishing\",\"george\"]\t[0.1428571429,0.25]\r\n",
      "[\"establishing\",\"government\"]\t[0.1428571429,0.25]\r\n",
      "[\"establishing\",\"in\"]\t[0.1,0.1889822365]\r\n",
      "[\"establishing\",\"limited\"]\t[0.1428571429,0.25]\r\n",
      "[\"establishing\",\"narrative\"]\t[0.1428571429,0.25]\r\n",
      "[\"establishing\",\"of\"]\t[0.0555555556,0.1290994449]\r\n",
      "[\"establishing\",\"religious\"]\t[0.6,0.75]\r\n",
      "[\"establishing\",\"sea\"]\t[0.1428571429,0.25]\r\n",
      "[\"establishing\",\"study\"]\t[0.1,0.1889822365]\r\n",
      "[\"establishing\",\"tales\"]\t[0.1428571429,0.25]\r\n",
      "[\"establishing\",\"the\"]\t[0.1,0.1889822365]\r\n",
      "[\"establishing\",\"wales\"]\t[0.1428571429,0.25]\r\n",
      "[\"fairy\",\"a\"]\t[0.1071428571,0.2886751346]\r\n",
      "[\"fairy\",\"bill\"]\t[0.1428571429,0.25]\r\n",
      "[\"fairy\",\"biography\"]\t[0.3333333333,0.5]\r\n",
      "[\"fairy\",\"by\"]\t[0.1428571429,0.25]\r\n",
      "[\"fairy\",\"case\"]\t[0.2222222222,0.377964473]\r\n",
      "[\"fairy\",\"child's\"]\t[0.1428571429,0.25]\r\n",
      "[\"fairy\",\"christmas\"]\t[0.1428571429,0.25]\r\n",
      "[\"fairy\",\"circumstantial\"]\t[0.3333333333,0.5]\r\n",
      "[\"fairy\",\"city\"]\t[0.1428571429,0.25]\r\n",
      "[\"fairy\",\"collection\"]\t[0.5,0.6708203932]\r\n",
      "[\"fairy\",\"establishing\"]\t[0.1428571429,0.25]\r\n",
      "[\"fairy\",\"female\"]\t[0.3333333333,0.5]\r\n",
      "[\"fairy\",\"for\"]\t[0.1428571429,0.25]\r\n",
      "[\"fairy\",\"forms\"]\t[0.75,0.8660254038]\r\n",
      "[\"fairy\",\"general\"]\t[0.3333333333,0.5]\r\n",
      "[\"fairy\",\"george\"]\t[0.3333333333,0.5]\r\n",
      "[\"fairy\",\"government\"]\t[0.1428571429,0.25]\r\n",
      "[\"fairy\",\"in\"]\t[0.1,0.1889822365]\r\n",
      "[\"fairy\",\"limited\"]\t[0.3333333333,0.5]\r\n",
      "[\"fairy\",\"narrative\"]\t[0.3333333333,0.5]\r\n",
      "[\"fairy\",\"of\"]\t[0.1875,0.3872983346]\r\n",
      "[\"fairy\",\"religious\"]\t[0.1428571429,0.25]\r\n",
      "[\"fairy\",\"sea\"]\t[0.1428571429,0.25]\r\n",
      "[\"fairy\",\"study\"]\t[0.2222222222,0.377964473]\r\n",
      "[\"fairy\",\"tales\"]\t[0.6,0.75]\r\n",
      "[\"fairy\",\"the\"]\t[0.2222222222,0.377964473]\r\n",
      "[\"fairy\",\"wales\"]\t[0.1428571429,0.25]\r\n",
      "[\"female\",\"a\"]\t[0.1071428571,0.2886751346]\r\n",
      "[\"female\",\"bill\"]\t[0.1428571429,0.25]\r\n",
      "[\"female\",\"biography\"]\t[0.3333333333,0.5]\r\n",
      "[\"female\",\"by\"]\t[0.1428571429,0.25]\r\n",
      "[\"female\",\"case\"]\t[0.375,0.5669467095]\r\n",
      "[\"female\",\"child's\"]\t[0.1428571429,0.25]\r\n",
      "[\"female\",\"christmas\"]\t[0.1428571429,0.25]\r\n",
      "[\"female\",\"circumstantial\"]\t[0.3333333333,0.5]\r\n",
      "[\"female\",\"city\"]\t[0.1428571429,0.25]\r\n",
      "[\"female\",\"collection\"]\t[0.2857142857,0.4472135955]\r\n",
      "[\"female\",\"establishing\"]\t[0.1428571429,0.25]\r\n",
      "[\"female\",\"fairy\"]\t[0.3333333333,0.5]\r\n",
      "[\"female\",\"for\"]\t[0.1428571429,0.25]\r\n",
      "[\"female\",\"forms\"]\t[0.4,0.5773502692]\r\n",
      "[\"female\",\"general\"]\t[0.3333333333,0.5]\r\n",
      "[\"female\",\"george\"]\t[0.3333333333,0.5]\r\n",
      "[\"female\",\"government\"]\t[0.6,0.75]\r\n",
      "[\"female\",\"in\"]\t[0.375,0.5669467095]\r\n",
      "[\"female\",\"limited\"]\t[1.0,1.0]\r\n",
      "[\"female\",\"narrative\"]\t[0.3333333333,0.5]\r\n",
      "[\"female\",\"of\"]\t[0.1875,0.3872983346]\r\n",
      "[\"female\",\"religious\"]\t[0.1428571429,0.25]\r\n",
      "[\"female\",\"sea\"]\t[0.1428571429,0.25]\r\n",
      "[\"female\",\"study\"]\t[0.375,0.5669467095]\r\n",
      "[\"female\",\"tales\"]\t[0.3333333333,0.5]\r\n",
      "[\"female\",\"the\"]\t[0.2222222222,0.377964473]\r\n",
      "[\"female\",\"wales\"]\t[0.1428571429,0.25]\r\n",
      "[\"for\",\"a\"]\t[0.1071428571,0.2886751346]\r\n",
      "[\"for\",\"bill\"]\t[0.6,0.75]\r\n",
      "[\"for\",\"biography\"]\t[0.1428571429,0.25]\r\n",
      "[\"for\",\"by\"]\t[0.1428571429,0.25]\r\n",
      "[\"for\",\"case\"]\t[0.1,0.1889822365]\r\n",
      "[\"for\",\"child's\"]\t[0.1428571429,0.25]\r\n",
      "[\"for\",\"christmas\"]\t[0.1428571429,0.25]\r\n",
      "[\"for\",\"circumstantial\"]\t[0.1428571429,0.25]\r\n",
      "[\"for\",\"city\"]\t[0.1428571429,0.25]\r\n",
      "[\"for\",\"collection\"]\t[0.125,0.2236067977]\r\n",
      "[\"for\",\"establishing\"]\t[0.6,0.75]\r\n",
      "[\"for\",\"fairy\"]\t[0.1428571429,0.25]\r\n",
      "[\"for\",\"female\"]\t[0.1428571429,0.25]\r\n",
      "[\"for\",\"forms\"]\t[0.1666666667,0.2886751346]\r\n",
      "[\"for\",\"general\"]\t[0.1428571429,0.25]\r\n",
      "[\"for\",\"george\"]\t[0.1428571429,0.25]\r\n",
      "[\"for\",\"government\"]\t[0.1428571429,0.25]\r\n",
      "[\"for\",\"in\"]\t[0.1,0.1889822365]\r\n",
      "[\"for\",\"limited\"]\t[0.1428571429,0.25]\r\n",
      "[\"for\",\"narrative\"]\t[0.1428571429,0.25]\r\n",
      "[\"for\",\"of\"]\t[0.0555555556,0.1290994449]\r\n",
      "[\"for\",\"religious\"]\t[0.6,0.75]\r\n",
      "[\"for\",\"sea\"]\t[0.1428571429,0.25]\r\n",
      "[\"for\",\"study\"]\t[0.1,0.1889822365]\r\n",
      "[\"for\",\"tales\"]\t[0.1428571429,0.25]\r\n",
      "[\"for\",\"the\"]\t[0.1,0.1889822365]\r\n",
      "[\"for\",\"wales\"]\t[0.1428571429,0.25]\r\n",
      "[\"forms\",\"a\"]\t[0.0714285714,0.2222222222]\r\n",
      "[\"forms\",\"bill\"]\t[0.1666666667,0.2886751346]\r\n",
      "[\"forms\",\"biography\"]\t[0.4,0.5773502692]\r\n",
      "[\"forms\",\"by\"]\t[0.1666666667,0.2886751346]\r\n",
      "[\"forms\",\"case\"]\t[0.25,0.4364357805]\r\n",
      "[\"forms\",\"child's\"]\t[0.1666666667,0.2886751346]\r\n",
      "[\"forms\",\"christmas\"]\t[0.1666666667,0.2886751346]\r\n",
      "[\"forms\",\"circumstantial\"]\t[0.4,0.5773502692]\r\n",
      "[\"forms\",\"city\"]\t[0.1666666667,0.2886751346]\r\n",
      "[\"forms\",\"collection\"]\t[0.3333333333,0.5163977795]\r\n",
      "[\"forms\",\"establishing\"]\t[0.1666666667,0.2886751346]\r\n",
      "[\"forms\",\"fairy\"]\t[0.75,0.8660254038]\r\n",
      "[\"forms\",\"female\"]\t[0.4,0.5773502692]\r\n",
      "[\"forms\",\"for\"]\t[0.1666666667,0.2886751346]\r\n",
      "[\"forms\",\"general\"]\t[0.4,0.5773502692]\r\n",
      "[\"forms\",\"george\"]\t[0.4,0.5773502692]\r\n",
      "[\"forms\",\"government\"]\t[0.1666666667,0.2886751346]\r\n",
      "[\"forms\",\"in\"]\t[0.1111111111,0.2182178902]\r\n",
      "[\"forms\",\"limited\"]\t[0.4,0.5773502692]\r\n",
      "[\"forms\",\"narrative\"]\t[0.4,0.5773502692]\r\n",
      "[\"forms\",\"of\"]\t[0.125,0.298142397]\r\n",
      "[\"forms\",\"religious\"]\t[0.1666666667,0.2886751346]\r\n",
      "[\"forms\",\"sea\"]\t[0.1666666667,0.2886751346]\r\n",
      "[\"forms\",\"study\"]\t[0.25,0.4364357805]\r\n",
      "[\"forms\",\"tales\"]\t[0.75,0.8660254038]\r\n",
      "[\"forms\",\"the\"]\t[0.25,0.4364357805]\r\n",
      "[\"forms\",\"wales\"]\t[0.1666666667,0.2886751346]\r\n",
      "[\"general\",\"a\"]\t[0.1071428571,0.2886751346]\r\n",
      "[\"general\",\"bill\"]\t[0.1428571429,0.25]\r\n",
      "[\"general\",\"biography\"]\t[0.6,0.75]\r\n",
      "[\"general\",\"by\"]\t[0.1428571429,0.25]\r\n",
      "[\"general\",\"case\"]\t[0.2222222222,0.377964473]\r\n",
      "[\"general\",\"child's\"]\t[0.1428571429,0.25]\r\n",
      "[\"general\",\"christmas\"]\t[0.1428571429,0.25]\r\n",
      "[\"general\",\"circumstantial\"]\t[0.3333333333,0.5]\r\n",
      "[\"general\",\"city\"]\t[0.1428571429,0.25]\r\n",
      "[\"general\",\"collection\"]\t[0.2857142857,0.4472135955]\r\n",
      "[\"general\",\"establishing\"]\t[0.1428571429,0.25]\r\n",
      "[\"general\",\"fairy\"]\t[0.3333333333,0.5]\r\n",
      "[\"general\",\"female\"]\t[0.3333333333,0.5]\r\n",
      "[\"general\",\"for\"]\t[0.1428571429,0.25]\r\n",
      "[\"general\",\"forms\"]\t[0.4,0.5773502692]\r\n",
      "[\"general\",\"george\"]\t[0.6,0.75]\r\n",
      "[\"general\",\"government\"]\t[0.1428571429,0.25]\r\n",
      "[\"general\",\"in\"]\t[0.1,0.1889822365]\r\n",
      "[\"general\",\"limited\"]\t[0.3333333333,0.5]\r\n",
      "[\"general\",\"narrative\"]\t[0.3333333333,0.5]\r\n",
      "[\"general\",\"of\"]\t[0.1875,0.3872983346]\r\n",
      "[\"general\",\"religious\"]\t[0.1428571429,0.25]\r\n",
      "[\"general\",\"sea\"]\t[0.1428571429,0.25]\r\n",
      "[\"general\",\"study\"]\t[0.2222222222,0.377964473]\r\n",
      "[\"general\",\"tales\"]\t[0.3333333333,0.5]\r\n",
      "[\"general\",\"the\"]\t[0.2222222222,0.377964473]\r\n",
      "[\"general\",\"wales\"]\t[0.1428571429,0.25]\r\n",
      "[\"george\",\"a\"]\t[0.1071428571,0.2886751346]\r\n",
      "[\"george\",\"bill\"]\t[0.1428571429,0.25]\r\n",
      "[\"george\",\"biography\"]\t[0.6,0.75]\r\n",
      "[\"george\",\"by\"]\t[0.1428571429,0.25]\r\n",
      "[\"george\",\"case\"]\t[0.2222222222,0.377964473]\r\n",
      "[\"george\",\"child's\"]\t[0.1428571429,0.25]\r\n",
      "[\"george\",\"christmas\"]\t[0.1428571429,0.25]\r\n",
      "[\"george\",\"circumstantial\"]\t[0.3333333333,0.5]\r\n",
      "[\"george\",\"city\"]\t[0.1428571429,0.25]\r\n",
      "[\"george\",\"collection\"]\t[0.2857142857,0.4472135955]\r\n",
      "[\"george\",\"establishing\"]\t[0.1428571429,0.25]\r\n",
      "[\"george\",\"fairy\"]\t[0.3333333333,0.5]\r\n",
      "[\"george\",\"female\"]\t[0.3333333333,0.5]\r\n",
      "[\"george\",\"for\"]\t[0.1428571429,0.25]\r\n",
      "[\"george\",\"forms\"]\t[0.4,0.5773502692]\r\n",
      "[\"george\",\"general\"]\t[0.6,0.75]\r\n",
      "[\"george\",\"government\"]\t[0.1428571429,0.25]\r\n",
      "[\"george\",\"in\"]\t[0.1,0.1889822365]\r\n",
      "[\"george\",\"limited\"]\t[0.3333333333,0.5]\r\n",
      "[\"george\",\"narrative\"]\t[0.3333333333,0.5]\r\n",
      "[\"george\",\"of\"]\t[0.1875,0.3872983346]\r\n",
      "[\"george\",\"religious\"]\t[0.1428571429,0.25]\r\n",
      "[\"george\",\"sea\"]\t[0.1428571429,0.25]\r\n",
      "[\"george\",\"study\"]\t[0.2222222222,0.377964473]\r\n",
      "[\"george\",\"tales\"]\t[0.3333333333,0.5]\r\n",
      "[\"george\",\"the\"]\t[0.2222222222,0.377964473]\r\n",
      "[\"george\",\"wales\"]\t[0.1428571429,0.25]\r\n",
      "[\"government\",\"a\"]\t[0.1071428571,0.2886751346]\r\n",
      "[\"government\",\"bill\"]\t[0.1428571429,0.25]\r\n",
      "[\"government\",\"biography\"]\t[0.1428571429,0.25]\r\n",
      "[\"government\",\"by\"]\t[0.1428571429,0.25]\r\n",
      "[\"government\",\"case\"]\t[0.375,0.5669467095]\r\n",
      "[\"government\",\"child's\"]\t[0.3333333333,0.5]\r\n",
      "[\"government\",\"christmas\"]\t[0.3333333333,0.5]\r\n",
      "[\"government\",\"circumstantial\"]\t[0.1428571429,0.25]\r\n",
      "[\"government\",\"city\"]\t[0.1428571429,0.25]\r\n",
      "[\"government\",\"collection\"]\t[0.125,0.2236067977]\r\n",
      "[\"government\",\"establishing\"]\t[0.1428571429,0.25]\r\n",
      "[\"government\",\"fairy\"]\t[0.1428571429,0.25]\r\n",
      "[\"government\",\"female\"]\t[0.6,0.75]\r\n",
      "[\"government\",\"for\"]\t[0.1428571429,0.25]\r\n",
      "[\"government\",\"forms\"]\t[0.1666666667,0.2886751346]\r\n",
      "[\"government\",\"general\"]\t[0.1428571429,0.25]\r\n",
      "[\"government\",\"george\"]\t[0.1428571429,0.25]\r\n",
      "[\"government\",\"in\"]\t[0.375,0.5669467095]\r\n",
      "[\"government\",\"limited\"]\t[0.6,0.75]\r\n",
      "[\"government\",\"narrative\"]\t[0.1428571429,0.25]\r\n",
      "[\"government\",\"of\"]\t[0.1875,0.3872983346]\r\n",
      "[\"government\",\"religious\"]\t[0.1428571429,0.25]\r\n",
      "[\"government\",\"sea\"]\t[0.1428571429,0.25]\r\n",
      "[\"government\",\"study\"]\t[0.375,0.5669467095]\r\n",
      "[\"government\",\"tales\"]\t[0.1428571429,0.25]\r\n",
      "[\"government\",\"the\"]\t[0.1,0.1889822365]\r\n",
      "[\"government\",\"wales\"]\t[0.3333333333,0.5]\r\n",
      "[\"in\",\"a\"]\t[0.2142857143,0.4364357805]\r\n",
      "[\"in\",\"bill\"]\t[0.1,0.1889822365]\r\n",
      "[\"in\",\"biography\"]\t[0.1,0.1889822365]\r\n",
      "[\"in\",\"by\"]\t[0.1,0.1889822365]\r\n",
      "[\"in\",\"case\"]\t[0.2727272727,0.4285714286]\r\n",
      "[\"in\",\"child's\"]\t[0.375,0.5669467095]\r\n",
      "[\"in\",\"christmas\"]\t[0.375,0.5669467095]\r\n",
      "[\"in\",\"circumstantial\"]\t[0.1,0.1889822365]\r\n",
      "[\"in\",\"city\"]\t[0.1,0.1889822365]\r\n",
      "[\"in\",\"collection\"]\t[0.0909090909,0.1690308509]\r\n",
      "[\"in\",\"establishing\"]\t[0.1,0.1889822365]\r\n",
      "[\"in\",\"fairy\"]\t[0.1,0.1889822365]\r\n",
      "[\"in\",\"female\"]\t[0.375,0.5669467095]\r\n",
      "[\"in\",\"for\"]\t[0.1,0.1889822365]\r\n",
      "[\"in\",\"forms\"]\t[0.1111111111,0.2182178902]\r\n",
      "[\"in\",\"general\"]\t[0.1,0.1889822365]\r\n",
      "[\"in\",\"george\"]\t[0.1,0.1889822365]\r\n",
      "[\"in\",\"government\"]\t[0.375,0.5669467095]\r\n",
      "[\"in\",\"limited\"]\t[0.375,0.5669467095]\r\n",
      "[\"in\",\"narrative\"]\t[0.1,0.1889822365]\r\n",
      "[\"in\",\"of\"]\t[0.1578947368,0.2927700219]\r\n",
      "[\"in\",\"religious\"]\t[0.1,0.1889822365]\r\n",
      "[\"in\",\"sea\"]\t[0.1,0.1889822365]\r\n",
      "[\"in\",\"study\"]\t[0.2727272727,0.4285714286]\r\n",
      "[\"in\",\"tales\"]\t[0.1,0.1889822365]\r\n",
      "[\"in\",\"the\"]\t[0.0769230769,0.1428571429]\r\n",
      "[\"in\",\"wales\"]\t[0.375,0.5669467095]\r\n",
      "[\"limited\",\"a\"]\t[0.1071428571,0.2886751346]\r\n",
      "[\"limited\",\"bill\"]\t[0.1428571429,0.25]\r\n",
      "[\"limited\",\"biography\"]\t[0.3333333333,0.5]\r\n",
      "[\"limited\",\"by\"]\t[0.1428571429,0.25]\r\n",
      "[\"limited\",\"case\"]\t[0.375,0.5669467095]\r\n",
      "[\"limited\",\"child's\"]\t[0.1428571429,0.25]\r\n",
      "[\"limited\",\"christmas\"]\t[0.1428571429,0.25]\r\n",
      "[\"limited\",\"circumstantial\"]\t[0.3333333333,0.5]\r\n",
      "[\"limited\",\"city\"]\t[0.1428571429,0.25]\r\n",
      "[\"limited\",\"collection\"]\t[0.2857142857,0.4472135955]\r\n",
      "[\"limited\",\"establishing\"]\t[0.1428571429,0.25]\r\n",
      "[\"limited\",\"fairy\"]\t[0.3333333333,0.5]\r\n",
      "[\"limited\",\"female\"]\t[1.0,1.0]\r\n",
      "[\"limited\",\"for\"]\t[0.1428571429,0.25]\r\n",
      "[\"limited\",\"forms\"]\t[0.4,0.5773502692]\r\n",
      "[\"limited\",\"general\"]\t[0.3333333333,0.5]\r\n",
      "[\"limited\",\"george\"]\t[0.3333333333,0.5]\r\n",
      "[\"limited\",\"government\"]\t[0.6,0.75]\r\n",
      "[\"limited\",\"in\"]\t[0.375,0.5669467095]\r\n",
      "[\"limited\",\"narrative\"]\t[0.3333333333,0.5]\r\n",
      "[\"limited\",\"of\"]\t[0.1875,0.3872983346]\r\n",
      "[\"limited\",\"religious\"]\t[0.1428571429,0.25]\r\n",
      "[\"limited\",\"sea\"]\t[0.1428571429,0.25]\r\n",
      "[\"limited\",\"study\"]\t[0.375,0.5669467095]\r\n",
      "[\"limited\",\"tales\"]\t[0.3333333333,0.5]\r\n",
      "[\"limited\",\"the\"]\t[0.2222222222,0.377964473]\r\n",
      "[\"limited\",\"wales\"]\t[0.1428571429,0.25]\r\n",
      "[\"narrative\",\"a\"]\t[0.1071428571,0.2886751346]\r\n",
      "[\"narrative\",\"bill\"]\t[0.1428571429,0.25]\r\n",
      "[\"narrative\",\"biography\"]\t[0.3333333333,0.5]\r\n",
      "[\"narrative\",\"by\"]\t[0.3333333333,0.5]\r\n",
      "[\"narrative\",\"case\"]\t[0.2222222222,0.377964473]\r\n",
      "[\"narrative\",\"child's\"]\t[0.1428571429,0.25]\r\n",
      "[\"narrative\",\"christmas\"]\t[0.1428571429,0.25]\r\n",
      "[\"narrative\",\"circumstantial\"]\t[0.6,0.75]\r\n",
      "[\"narrative\",\"city\"]\t[0.3333333333,0.5]\r\n",
      "[\"narrative\",\"collection\"]\t[0.2857142857,0.4472135955]\r\n",
      "[\"narrative\",\"establishing\"]\t[0.1428571429,0.25]\r\n",
      "[\"narrative\",\"fairy\"]\t[0.3333333333,0.5]\r\n",
      "[\"narrative\",\"female\"]\t[0.3333333333,0.5]\r\n",
      "[\"narrative\",\"for\"]\t[0.1428571429,0.25]\r\n",
      "[\"narrative\",\"forms\"]\t[0.4,0.5773502692]\r\n",
      "[\"narrative\",\"general\"]\t[0.3333333333,0.5]\r\n",
      "[\"narrative\",\"george\"]\t[0.3333333333,0.5]\r\n",
      "[\"narrative\",\"government\"]\t[0.1428571429,0.25]\r\n",
      "[\"narrative\",\"in\"]\t[0.1,0.1889822365]\r\n",
      "[\"narrative\",\"limited\"]\t[0.3333333333,0.5]\r\n",
      "[\"narrative\",\"of\"]\t[0.1875,0.3872983346]\r\n",
      "[\"narrative\",\"religious\"]\t[0.1428571429,0.25]\r\n",
      "[\"narrative\",\"sea\"]\t[0.3333333333,0.5]\r\n",
      "[\"narrative\",\"study\"]\t[0.2222222222,0.377964473]\r\n",
      "[\"narrative\",\"tales\"]\t[0.3333333333,0.5]\r\n",
      "[\"narrative\",\"the\"]\t[0.375,0.5669467095]\r\n",
      "[\"narrative\",\"wales\"]\t[0.1428571429,0.25]\r\n",
      "[\"of\",\"a\"]\t[0.5,0.695665593]\r\n",
      "[\"of\",\"bill\"]\t[0.0555555556,0.1290994449]\r\n",
      "[\"of\",\"biography\"]\t[0.1875,0.3872983346]\r\n",
      "[\"of\",\"by\"]\t[0.1176470588,0.2581988897]\r\n",
      "[\"of\",\"case\"]\t[0.2222222222,0.3903600292]\r\n",
      "[\"of\",\"child's\"]\t[0.0555555556,0.1290994449]\r\n",
      "[\"of\",\"christmas\"]\t[0.0555555556,0.1290994449]\r\n",
      "[\"of\",\"circumstantial\"]\t[0.1875,0.3872983346]\r\n",
      "[\"of\",\"city\"]\t[0.1176470588,0.2581988897]\r\n",
      "[\"of\",\"collection\"]\t[0.25,0.4618802154]\r\n",
      "[\"of\",\"establishing\"]\t[0.0555555556,0.1290994449]\r\n",
      "[\"of\",\"fairy\"]\t[0.1875,0.3872983346]\r\n",
      "[\"of\",\"female\"]\t[0.1875,0.3872983346]\r\n",
      "[\"of\",\"for\"]\t[0.0555555556,0.1290994449]\r\n",
      "[\"of\",\"forms\"]\t[0.125,0.298142397]\r\n",
      "[\"of\",\"general\"]\t[0.1875,0.3872983346]\r\n",
      "[\"of\",\"george\"]\t[0.1875,0.3872983346]\r\n",
      "[\"of\",\"government\"]\t[0.1875,0.3872983346]\r\n",
      "[\"of\",\"in\"]\t[0.1578947368,0.2927700219]\r\n",
      "[\"of\",\"limited\"]\t[0.1875,0.3872983346]\r\n",
      "[\"of\",\"narrative\"]\t[0.1875,0.3872983346]\r\n",
      "[\"of\",\"religious\"]\t[0.0555555556,0.1290994449]\r\n",
      "[\"of\",\"sea\"]\t[0.1176470588,0.2581988897]\r\n",
      "[\"of\",\"study\"]\t[0.2222222222,0.3903600292]\r\n",
      "[\"of\",\"tales\"]\t[0.1875,0.3872983346]\r\n",
      "[\"of\",\"the\"]\t[0.1578947368,0.2927700219]\r\n",
      "[\"of\",\"wales\"]\t[0.0555555556,0.1290994449]\r\n",
      "[\"religious\",\"a\"]\t[0.1071428571,0.2886751346]\r\n",
      "[\"religious\",\"bill\"]\t[0.6,0.75]\r\n",
      "[\"religious\",\"biography\"]\t[0.1428571429,0.25]\r\n",
      "[\"religious\",\"by\"]\t[0.1428571429,0.25]\r\n",
      "[\"religious\",\"case\"]\t[0.1,0.1889822365]\r\n",
      "[\"religious\",\"child's\"]\t[0.1428571429,0.25]\r\n",
      "[\"religious\",\"christmas\"]\t[0.1428571429,0.25]\r\n",
      "[\"religious\",\"circumstantial\"]\t[0.1428571429,0.25]\r\n",
      "[\"religious\",\"city\"]\t[0.1428571429,0.25]\r\n",
      "[\"religious\",\"collection\"]\t[0.125,0.2236067977]\r\n",
      "[\"religious\",\"establishing\"]\t[0.6,0.75]\r\n",
      "[\"religious\",\"fairy\"]\t[0.1428571429,0.25]\r\n",
      "[\"religious\",\"female\"]\t[0.1428571429,0.25]\r\n",
      "[\"religious\",\"for\"]\t[0.6,0.75]\r\n",
      "[\"religious\",\"forms\"]\t[0.1666666667,0.2886751346]\r\n",
      "[\"religious\",\"general\"]\t[0.1428571429,0.25]\r\n",
      "[\"religious\",\"george\"]\t[0.1428571429,0.25]\r\n",
      "[\"religious\",\"government\"]\t[0.1428571429,0.25]\r\n",
      "[\"religious\",\"in\"]\t[0.1,0.1889822365]\r\n",
      "[\"religious\",\"limited\"]\t[0.1428571429,0.25]\r\n",
      "[\"religious\",\"narrative\"]\t[0.1428571429,0.25]\r\n",
      "[\"religious\",\"of\"]\t[0.0555555556,0.1290994449]\r\n",
      "[\"religious\",\"sea\"]\t[0.1428571429,0.25]\r\n",
      "[\"religious\",\"study\"]\t[0.1,0.1889822365]\r\n",
      "[\"religious\",\"tales\"]\t[0.1428571429,0.25]\r\n",
      "[\"religious\",\"the\"]\t[0.1,0.1889822365]\r\n",
      "[\"religious\",\"wales\"]\t[0.1428571429,0.25]\r\n",
      "[\"sea\",\"a\"]\t[0.1071428571,0.2886751346]\r\n",
      "[\"sea\",\"bill\"]\t[0.1428571429,0.25]\r\n",
      "[\"sea\",\"biography\"]\t[0.1428571429,0.25]\r\n",
      "[\"sea\",\"by\"]\t[0.6,0.75]\r\n",
      "[\"sea\",\"case\"]\t[0.1,0.1889822365]\r\n",
      "[\"sea\",\"child's\"]\t[0.1428571429,0.25]\r\n",
      "[\"sea\",\"christmas\"]\t[0.1428571429,0.25]\r\n",
      "[\"sea\",\"circumstantial\"]\t[0.3333333333,0.5]\r\n",
      "[\"sea\",\"city\"]\t[0.6,0.75]\r\n",
      "[\"sea\",\"collection\"]\t[0.125,0.2236067977]\r\n",
      "[\"sea\",\"establishing\"]\t[0.1428571429,0.25]\r\n",
      "[\"sea\",\"fairy\"]\t[0.1428571429,0.25]\r\n",
      "[\"sea\",\"female\"]\t[0.1428571429,0.25]\r\n",
      "[\"sea\",\"for\"]\t[0.1428571429,0.25]\r\n",
      "[\"sea\",\"forms\"]\t[0.1666666667,0.2886751346]\r\n",
      "[\"sea\",\"general\"]\t[0.1428571429,0.25]\r\n",
      "[\"sea\",\"george\"]\t[0.1428571429,0.25]\r\n",
      "[\"sea\",\"government\"]\t[0.1428571429,0.25]\r\n",
      "[\"sea\",\"in\"]\t[0.1,0.1889822365]\r\n",
      "[\"sea\",\"limited\"]\t[0.1428571429,0.25]\r\n",
      "[\"sea\",\"narrative\"]\t[0.3333333333,0.5]\r\n",
      "[\"sea\",\"of\"]\t[0.1176470588,0.2581988897]\r\n",
      "[\"sea\",\"religious\"]\t[0.1428571429,0.25]\r\n",
      "[\"sea\",\"study\"]\t[0.1,0.1889822365]\r\n",
      "[\"sea\",\"tales\"]\t[0.1428571429,0.25]\r\n",
      "[\"sea\",\"the\"]\t[0.375,0.5669467095]\r\n",
      "[\"sea\",\"wales\"]\t[0.1428571429,0.25]\r\n",
      "[\"study\",\"a\"]\t[0.2142857143,0.4364357805]\r\n",
      "[\"study\",\"bill\"]\t[0.1,0.1889822365]\r\n",
      "[\"study\",\"biography\"]\t[0.2222222222,0.377964473]\r\n",
      "[\"study\",\"by\"]\t[0.1,0.1889822365]\r\n",
      "[\"study\",\"case\"]\t[0.75,0.8571428571]\r\n",
      "[\"study\",\"child's\"]\t[0.2222222222,0.377964473]\r\n",
      "[\"study\",\"christmas\"]\t[0.2222222222,0.377964473]\r\n",
      "[\"study\",\"circumstantial\"]\t[0.2222222222,0.377964473]\r\n",
      "[\"study\",\"city\"]\t[0.1,0.1889822365]\r\n",
      "[\"study\",\"collection\"]\t[0.2,0.3380617019]\r\n",
      "[\"study\",\"establishing\"]\t[0.1,0.1889822365]\r\n",
      "[\"study\",\"fairy\"]\t[0.2222222222,0.377964473]\r\n",
      "[\"study\",\"female\"]\t[0.375,0.5669467095]\r\n",
      "[\"study\",\"for\"]\t[0.1,0.1889822365]\r\n",
      "[\"study\",\"forms\"]\t[0.25,0.4364357805]\r\n",
      "[\"study\",\"general\"]\t[0.2222222222,0.377964473]\r\n",
      "[\"study\",\"george\"]\t[0.2222222222,0.377964473]\r\n",
      "[\"study\",\"government\"]\t[0.375,0.5669467095]\r\n",
      "[\"study\",\"in\"]\t[0.2727272727,0.4285714286]\r\n",
      "[\"study\",\"limited\"]\t[0.375,0.5669467095]\r\n",
      "[\"study\",\"narrative\"]\t[0.2222222222,0.377964473]\r\n",
      "[\"study\",\"of\"]\t[0.2222222222,0.3903600292]\r\n",
      "[\"study\",\"religious\"]\t[0.1,0.1889822365]\r\n",
      "[\"study\",\"sea\"]\t[0.1,0.1889822365]\r\n",
      "[\"study\",\"tales\"]\t[0.2222222222,0.377964473]\r\n",
      "[\"study\",\"the\"]\t[0.1666666667,0.2857142857]\r\n",
      "[\"study\",\"wales\"]\t[0.2222222222,0.377964473]\r\n",
      "[\"tales\",\"a\"]\t[0.1071428571,0.2886751346]\r\n",
      "[\"tales\",\"bill\"]\t[0.1428571429,0.25]\r\n",
      "[\"tales\",\"biography\"]\t[0.3333333333,0.5]\r\n",
      "[\"tales\",\"by\"]\t[0.1428571429,0.25]\r\n",
      "[\"tales\",\"case\"]\t[0.2222222222,0.377964473]\r\n",
      "[\"tales\",\"child's\"]\t[0.1428571429,0.25]\r\n",
      "[\"tales\",\"christmas\"]\t[0.1428571429,0.25]\r\n",
      "[\"tales\",\"circumstantial\"]\t[0.3333333333,0.5]\r\n",
      "[\"tales\",\"city\"]\t[0.1428571429,0.25]\r\n",
      "[\"tales\",\"collection\"]\t[0.5,0.6708203932]\r\n",
      "[\"tales\",\"establishing\"]\t[0.1428571429,0.25]\r\n",
      "[\"tales\",\"fairy\"]\t[0.6,0.75]\r\n",
      "[\"tales\",\"female\"]\t[0.3333333333,0.5]\r\n",
      "[\"tales\",\"for\"]\t[0.1428571429,0.25]\r\n",
      "[\"tales\",\"forms\"]\t[0.75,0.8660254038]\r\n",
      "[\"tales\",\"general\"]\t[0.3333333333,0.5]\r\n",
      "[\"tales\",\"george\"]\t[0.3333333333,0.5]\r\n",
      "[\"tales\",\"government\"]\t[0.1428571429,0.25]\r\n",
      "[\"tales\",\"in\"]\t[0.1,0.1889822365]\r\n",
      "[\"tales\",\"limited\"]\t[0.3333333333,0.5]\r\n",
      "[\"tales\",\"narrative\"]\t[0.3333333333,0.5]\r\n",
      "[\"tales\",\"of\"]\t[0.1875,0.3872983346]\r\n",
      "[\"tales\",\"religious\"]\t[0.1428571429,0.25]\r\n",
      "[\"tales\",\"sea\"]\t[0.1428571429,0.25]\r\n",
      "[\"tales\",\"study\"]\t[0.2222222222,0.377964473]\r\n",
      "[\"tales\",\"the\"]\t[0.2222222222,0.377964473]\r\n",
      "[\"tales\",\"wales\"]\t[0.1428571429,0.25]\r\n",
      "[\"the\",\"a\"]\t[0.2142857143,0.4364357805]\r\n",
      "[\"the\",\"bill\"]\t[0.1,0.1889822365]\r\n",
      "[\"the\",\"biography\"]\t[0.2222222222,0.377964473]\r\n",
      "[\"the\",\"by\"]\t[0.375,0.5669467095]\r\n",
      "[\"the\",\"case\"]\t[0.1666666667,0.2857142857]\r\n",
      "[\"the\",\"child's\"]\t[0.1,0.1889822365]\r\n",
      "[\"the\",\"christmas\"]\t[0.1,0.1889822365]\r\n",
      "[\"the\",\"circumstantial\"]\t[0.375,0.5669467095]\r\n",
      "[\"the\",\"city\"]\t[0.375,0.5669467095]\r\n",
      "[\"the\",\"collection\"]\t[0.2,0.3380617019]\r\n",
      "[\"the\",\"establishing\"]\t[0.1,0.1889822365]\r\n",
      "[\"the\",\"fairy\"]\t[0.2222222222,0.377964473]\r\n",
      "[\"the\",\"female\"]\t[0.2222222222,0.377964473]\r\n",
      "[\"the\",\"for\"]\t[0.1,0.1889822365]\r\n",
      "[\"the\",\"forms\"]\t[0.25,0.4364357805]\r\n",
      "[\"the\",\"general\"]\t[0.2222222222,0.377964473]\r\n",
      "[\"the\",\"george\"]\t[0.2222222222,0.377964473]\r\n",
      "[\"the\",\"government\"]\t[0.1,0.1889822365]\r\n",
      "[\"the\",\"in\"]\t[0.0769230769,0.1428571429]\r\n",
      "[\"the\",\"limited\"]\t[0.2222222222,0.377964473]\r\n",
      "[\"the\",\"narrative\"]\t[0.375,0.5669467095]\r\n",
      "[\"the\",\"of\"]\t[0.1578947368,0.2927700219]\r\n",
      "[\"the\",\"religious\"]\t[0.1,0.1889822365]\r\n",
      "[\"the\",\"sea\"]\t[0.375,0.5669467095]\r\n",
      "[\"the\",\"study\"]\t[0.1666666667,0.2857142857]\r\n",
      "[\"the\",\"tales\"]\t[0.2222222222,0.377964473]\r\n",
      "[\"the\",\"wales\"]\t[0.1,0.1889822365]\r\n",
      "[\"wales\",\"a\"]\t[0.1071428571,0.2886751346]\r\n",
      "[\"wales\",\"bill\"]\t[0.1428571429,0.25]\r\n",
      "[\"wales\",\"biography\"]\t[0.1428571429,0.25]\r\n",
      "[\"wales\",\"by\"]\t[0.1428571429,0.25]\r\n",
      "[\"wales\",\"case\"]\t[0.2222222222,0.377964473]\r\n",
      "[\"wales\",\"child's\"]\t[0.6,0.75]\r\n",
      "[\"wales\",\"christmas\"]\t[0.6,0.75]\r\n",
      "[\"wales\",\"circumstantial\"]\t[0.1428571429,0.25]\r\n",
      "[\"wales\",\"city\"]\t[0.1428571429,0.25]\r\n",
      "[\"wales\",\"collection\"]\t[0.125,0.2236067977]\r\n",
      "[\"wales\",\"establishing\"]\t[0.1428571429,0.25]\r\n",
      "[\"wales\",\"fairy\"]\t[0.1428571429,0.25]\r\n",
      "[\"wales\",\"female\"]\t[0.1428571429,0.25]\r\n",
      "[\"wales\",\"for\"]\t[0.1428571429,0.25]\r\n",
      "[\"wales\",\"forms\"]\t[0.1666666667,0.2886751346]\r\n",
      "[\"wales\",\"general\"]\t[0.1428571429,0.25]\r\n",
      "[\"wales\",\"george\"]\t[0.1428571429,0.25]\r\n",
      "[\"wales\",\"government\"]\t[0.3333333333,0.5]\r\n",
      "[\"wales\",\"in\"]\t[0.375,0.5669467095]\r\n",
      "[\"wales\",\"limited\"]\t[0.1428571429,0.25]\r\n",
      "[\"wales\",\"narrative\"]\t[0.1428571429,0.25]\r\n",
      "[\"wales\",\"of\"]\t[0.0555555556,0.1290994449]\r\n",
      "[\"wales\",\"religious\"]\t[0.1428571429,0.25]\r\n",
      "[\"wales\",\"sea\"]\t[0.1428571429,0.25]\r\n",
      "[\"wales\",\"study\"]\t[0.2222222222,0.377964473]\r\n",
      "[\"wales\",\"tales\"]\t[0.1428571429,0.25]\r\n",
      "[\"wales\",\"the\"]\t[0.1,0.1889822365]\r\n"
     ]
    }
   ],
   "source": [
    "!cat systems_test_doc_similarity_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `systems_test_doc_similarity_2': No such file or directory\n",
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/similarity.nwchen24.20170620.042152.511581\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/nwchen24/tmp/mrjob/similarity.nwchen24.20170620.042152.511581/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob2135687392576930032.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1497906899862_0324\n",
      "  Submitted application application_1497906899862_0324\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1497906899862_0324/\n",
      "  Running job: job_1497906899862_0324\n",
      "  Job job_1497906899862_0324 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497906899862_0324 completed successfully\n",
      "  Output directory: hdfs:///user/nwchen24/tmp/mrjob/similarity.nwchen24.20170620.042152.511581/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=206\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=428\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=227\n",
      "\t\tFILE: Number of bytes written=400528\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=578\n",
      "\t\tHDFS: Number of bytes written=428\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=20499456\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=17067520\n",
      "\t\tTotal time spent by all map tasks (ms)=13346\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=40038\n",
      "\t\tTotal time spent by all reduce tasks (ms)=6667\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=33335\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=13346\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=6667\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2600\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=161\n",
      "\t\tInput split bytes=372\n",
      "\t\tMap input records=4\n",
      "\t\tMap output bytes=704\n",
      "\t\tMap output materialized bytes=324\n",
      "\t\tMap output records=16\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1900789760\n",
      "\t\tReduce input groups=12\n",
      "\t\tReduce input records=16\n",
      "\t\tReduce output records=12\n",
      "\t\tReduce shuffle bytes=324\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=32\n",
      "\t\tTotal committed heap usage (bytes)=5218762752\n",
      "\t\tVirtual memory (bytes) snapshot=7747260416\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/nwchen24/tmp/mrjob/similarity.nwchen24.20170620.042152.511581/output...\n",
      "Removing HDFS temp directory hdfs:///user/nwchen24/tmp/mrjob/similarity.nwchen24.20170620.042152.511581...\n",
      "Removing temp directory /tmp/similarity.nwchen24.20170620.042152.511581...\n"
     ]
    }
   ],
   "source": [
    "###########################################################################\n",
    "# Document similarity from inverted index\n",
    "###########################################################################\n",
    "\n",
    "!hdfs dfs -rm -r systems_test_doc_similarity_2\n",
    "\n",
    "#Hadoop for altiscale\n",
    "!python similarity.py -r hadoop systems_test_inv_index_2 > systems_test_doc_similarity_2 \\\n",
    "    --python-bin=./my_env27_for_hadoop_upload/bin/python \\\n",
    "    --archive=hdfs:///user/nwchen24/virtualenv/my_env27_for_hadoop_upload.zip#my_env27_for_hadoop_upload\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"atlas\",\"boon\"]\t[0.25,0.4082482905]\r\n",
      "[\"atlas\",\"cava\"]\t[1.0,1.0]\r\n",
      "[\"atlas\",\"dipped\"]\t[0.25,0.4082482905]\r\n",
      "[\"boon\",\"atlas\"]\t[0.25,0.4082482905]\r\n",
      "[\"boon\",\"cava\"]\t[0.25,0.4082482905]\r\n",
      "[\"boon\",\"dipped\"]\t[0.5,0.6666666667]\r\n",
      "[\"cava\",\"atlas\"]\t[1.0,1.0]\r\n",
      "[\"cava\",\"boon\"]\t[0.25,0.4082482905]\r\n",
      "[\"cava\",\"dipped\"]\t[0.25,0.4082482905]\r\n",
      "[\"dipped\",\"atlas\"]\t[0.25,0.4082482905]\r\n",
      "[\"dipped\",\"boon\"]\t[0.5,0.6666666667]\r\n",
      "[\"dipped\",\"cava\"]\t[0.25,0.4082482905]\r\n"
     ]
    }
   ],
   "source": [
    "!cat systems_test_doc_similarity_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `systems_test_doc_similarity_3': No such file or directory\n",
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/similarity.nwchen24.20170620.042307.797058\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/nwchen24/tmp/mrjob/similarity.nwchen24.20170620.042307.797058/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob3392201196392387702.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1497906899862_0327\n",
      "  Submitted application application_1497906899862_0327\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1497906899862_0327/\n",
      "  Running job: job_1497906899862_0327\n",
      "  Job job_1497906899862_0327 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497906899862_0327 completed successfully\n",
      "  Output directory: hdfs:///user/nwchen24/tmp/mrjob/similarity.nwchen24.20170620.042307.797058/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=167\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=228\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=130\n",
      "\t\tFILE: Number of bytes written=400294\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=539\n",
      "\t\tHDFS: Number of bytes written=228\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=20362752\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=19829760\n",
      "\t\tTotal time spent by all map tasks (ms)=13257\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=39771\n",
      "\t\tTotal time spent by all reduce tasks (ms)=7746\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=38730\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=13257\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=7746\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2810\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=130\n",
      "\t\tInput split bytes=372\n",
      "\t\tMap input records=5\n",
      "\t\tMap output bytes=410\n",
      "\t\tMap output materialized bytes=187\n",
      "\t\tMap output records=10\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1895104512\n",
      "\t\tReduce input groups=6\n",
      "\t\tReduce input records=10\n",
      "\t\tReduce output records=6\n",
      "\t\tReduce shuffle bytes=187\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=20\n",
      "\t\tTotal committed heap usage (bytes)=5218762752\n",
      "\t\tVirtual memory (bytes) snapshot=7722893312\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/nwchen24/tmp/mrjob/similarity.nwchen24.20170620.042307.797058/output...\n",
      "Removing HDFS temp directory hdfs:///user/nwchen24/tmp/mrjob/similarity.nwchen24.20170620.042307.797058...\n",
      "Removing temp directory /tmp/similarity.nwchen24.20170620.042307.797058...\n"
     ]
    }
   ],
   "source": [
    "###########################################################################\n",
    "# Document similarity from inverted index\n",
    "###########################################################################\n",
    "\n",
    "!hdfs dfs -rm -r systems_test_doc_similarity_3\n",
    "\n",
    "#Hadoop for altiscale\n",
    "!python similarity.py -r hadoop systems_test_inv_index_3 > systems_test_doc_similarity_3 \\\n",
    "    --python-bin=./my_env27_for_hadoop_upload/bin/python \\\n",
    "    --archive=hdfs:///user/nwchen24/virtualenv/my_env27_for_hadoop_upload.zip#my_env27_for_hadoop_upload\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"DocA\",\"DocB\"]\t[0.6666666667,0.8164965809]\r\n",
      "[\"DocA\",\"DocC\"]\t[0.4,0.5773502692]\r\n",
      "[\"DocB\",\"DocA\"]\t[0.6666666667,0.8164965809]\r\n",
      "[\"DocB\",\"DocC\"]\t[0.2,0.3535533906]\r\n",
      "[\"DocC\",\"DocA\"]\t[0.4,0.5773502692]\r\n",
      "[\"DocC\",\"DocB\"]\t[0.2,0.3535533906]\r\n"
     ]
    }
   ],
   "source": [
    "!cat systems_test_doc_similarity_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.4.1 <a name=\"5.4.1\"></a>Full-scale experiment: EDA of Google n-grams dataset (PHASE 2)\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "Do some EDA on this dataset using mrjob, e.g., \n",
    "\n",
    "- A. Longest 5-gram (number of characters)\n",
    "- B. Top 10 most frequent words (please use the count information), i.e., unigrams\n",
    "- C. 20 Most/Least densely appearing words (count/pages_count) sorted in decreasing order of relative frequency \n",
    "- D. Distribution of 5-gram sizes (character length).  E.g., count (using the count field) up how many times a 5-gram of 50 characters shows up. Plot the data graphically using a histogram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW5.4.1 - A. Longest 5-gram (number of characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing longest5gram.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile longest5gram.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import re\n",
    "\n",
    "import mrjob\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import numpy as np\n",
    "\n",
    "class longest5gram(MRJob):\n",
    "    \n",
    "    # START STUDENT CODE 5.4.1.A\n",
    "         \n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper,\n",
    "                   reducer=self.reducer\n",
    "                  )\n",
    "        ]\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "\n",
    "        #get the list of tokens in the 5-gram\n",
    "        token_list = line.strip().lower().split()[:-3]\n",
    "\n",
    "        #instantiate counter to hold the five-gram length\n",
    "        five_gram_len = 0\n",
    "\n",
    "        #For each token, increment the five-gram length holder    \n",
    "        for token in token_list:\n",
    "            five_gram_len += len(token)\n",
    "\n",
    "        yield None, five_gram_len\n",
    "        \n",
    "    def reducer(self, key, five_gram_len):\n",
    "        yield None, max(five_gram_len)\n",
    "    # END STUDENT CODE 5.4.1.A\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    longest5gram.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__On test data set:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/longest5gram.nwchen24.20170620.042554.731645\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/nwchen24/tmp/mrjob/longest5gram.nwchen24.20170620.042554.731645/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob3537024265210486667.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1497906899862_0331\n",
      "  Submitted application application_1497906899862_0331\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1497906899862_0331/\n",
      "  Running job: job_1497906899862_0331\n",
      "  Job job_1497906899862_0331 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497906899862_0331 completed successfully\n",
      "  Output directory: hdfs:///user/nwchen24/tmp/mrjob/longest5gram.nwchen24.20170620.042554.731645/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=563\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=8\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=76\n",
      "\t\tFILE: Number of bytes written=398435\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1019\n",
      "\t\tHDFS: Number of bytes written=8\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=20958720\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=15946240\n",
      "\t\tTotal time spent by all map tasks (ms)=13645\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=40935\n",
      "\t\tTotal time spent by all reduce tasks (ms)=6229\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=31145\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=13645\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=6229\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=3160\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=105\n",
      "\t\tInput split bytes=456\n",
      "\t\tMap input records=10\n",
      "\t\tMap output bytes=80\n",
      "\t\tMap output materialized bytes=98\n",
      "\t\tMap output records=10\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1901559808\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce input records=10\n",
      "\t\tReduce output records=1\n",
      "\t\tReduce shuffle bytes=98\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=20\n",
      "\t\tTotal committed heap usage (bytes)=5218762752\n",
      "\t\tVirtual memory (bytes) snapshot=7735349248\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/nwchen24/tmp/mrjob/longest5gram.nwchen24.20170620.042554.731645/output...\n",
      "Removing HDFS temp directory hdfs:///user/nwchen24/tmp/mrjob/longest5gram.nwchen24.20170620.042554.731645...\n",
      "Removing temp directory /tmp/longest5gram.nwchen24.20170620.042554.731645...\n"
     ]
    }
   ],
   "source": [
    "#Hadoop works when virtual python environment loaded up to hadoop is specified\n",
    "!python longest5gram.py -r hadoop googlebooks-eng-all-5gram-20090715-0-filtered-first-10-lines.txt > longest_5gram_test \\\n",
    "    --python-bin=./my_env27_for_hadoop_upload/bin/python \\\n",
    "    --archive=hdfs:///user/nwchen24/virtualenv/my_env27_for_hadoop_upload.zip#my_env27_for_hadoop_upload\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "null\t29\r\n"
     ]
    }
   ],
   "source": [
    "!cat longest_5gram_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__On full data set:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Creating temp directory /tmp/longest5gram.nwchen24.20170620.042838.479759\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/nwchen24/tmp/mrjob/longest5gram.nwchen24.20170620.042838.479759/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob2439718490139734305.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 190\n",
      "  number of splits:190\n",
      "  Submitting tokens for job: job_1497906899862_0335\n",
      "  Submitted application application_1497906899862_0335\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1497906899862_0335/\n",
      "  Running job: job_1497906899862_0335\n",
      "  Job job_1497906899862_0335 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 1% reduce 0%\n",
      "   map 2% reduce 0%\n",
      "   map 3% reduce 0%\n",
      "   map 5% reduce 0%\n",
      "   map 7% reduce 0%\n",
      "   map 9% reduce 0%\n",
      "   map 10% reduce 0%\n",
      "   map 11% reduce 0%\n",
      "   map 12% reduce 0%\n",
      "   map 13% reduce 0%\n",
      "   map 17% reduce 0%\n",
      "   map 18% reduce 0%\n",
      "   map 21% reduce 0%\n",
      "   map 26% reduce 0%\n",
      "   map 30% reduce 0%\n",
      "   map 36% reduce 0%\n",
      "   map 39% reduce 0%\n",
      "   map 45% reduce 0%\n",
      "   map 48% reduce 0%\n",
      "   map 51% reduce 0%\n",
      "   map 55% reduce 0%\n",
      "   map 58% reduce 0%\n",
      "   map 62% reduce 0%\n",
      "   map 64% reduce 0%\n",
      "   map 65% reduce 0%\n",
      "   map 66% reduce 0%\n",
      "   map 67% reduce 0%\n",
      "   map 68% reduce 0%\n",
      "   map 70% reduce 0%\n",
      "   map 72% reduce 0%\n",
      "   map 76% reduce 0%\n",
      "   map 78% reduce 0%\n",
      "   map 80% reduce 0%\n",
      "   map 83% reduce 0%\n",
      "   map 85% reduce 0%\n",
      "   map 87% reduce 0%\n",
      "   map 88% reduce 0%\n",
      "   map 91% reduce 0%\n",
      "   map 93% reduce 0%\n",
      "   map 97% reduce 0%\n",
      "   map 98% reduce 0%\n",
      "   map 99% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 34%\n",
      "   map 100% reduce 35%\n",
      "   map 100% reduce 38%\n",
      "   map 100% reduce 39%\n",
      "   map 100% reduce 42%\n",
      "   map 100% reduce 44%\n",
      "   map 100% reduce 47%\n",
      "   map 100% reduce 49%\n",
      "   map 100% reduce 51%\n",
      "   map 100% reduce 52%\n",
      "   map 100% reduce 54%\n",
      "   map 100% reduce 56%\n",
      "   map 100% reduce 58%\n",
      "   map 100% reduce 61%\n",
      "   map 100% reduce 63%\n",
      "   map 100% reduce 65%\n",
      "   map 100% reduce 66%\n",
      "   map 100% reduce 67%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497906899862_0335 completed successfully\n",
      "  Output directory: hdfs:///user/nwchen24/tmp/mrjob/longest5gram.nwchen24.20170620.042838.479759/output\n",
      "Counters: 51\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2156069116\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=9\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=114126113\n",
      "\t\tFILE: Number of bytes written=253594291\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2156101116\n",
      "\t\tHDFS: Number of bytes written=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=573\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=2\n",
      "\t\tLaunched map tasks=191\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tOther local map tasks=2\n",
      "\t\tRack-local map tasks=189\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=9598559232\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1041528320\n",
      "\t\tTotal time spent by all map tasks (ms)=6249062\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=18747186\n",
      "\t\tTotal time spent by all reduce tasks (ms)=406847\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2034235\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=6249062\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=406847\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1650740\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=40327\n",
      "\t\tInput split bytes=32000\n",
      "\t\tMap input records=58682266\n",
      "\t\tMap output bytes=469453360\n",
      "\t\tMap output materialized bytes=114128761\n",
      "\t\tMap output records=58682266\n",
      "\t\tMerged Map outputs=190\n",
      "\t\tPhysical memory (bytes) snapshot=155067011072\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce input records=58682266\n",
      "\t\tReduce output records=1\n",
      "\t\tReduce shuffle bytes=114128761\n",
      "\t\tShuffled Maps =190\n",
      "\t\tSpilled Records=117364532\n",
      "\t\tTotal committed heap usage (bytes)=299923668992\n",
      "\t\tVirtual memory (bytes) snapshot=421355159552\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/nwchen24/tmp/mrjob/longest5gram.nwchen24.20170620.042838.479759/output...\n",
      "Removing HDFS temp directory hdfs:///user/nwchen24/tmp/mrjob/longest5gram.nwchen24.20170620.042838.479759...\n",
      "Removing temp directory /tmp/longest5gram.nwchen24.20170620.042838.479759...\n"
     ]
    }
   ],
   "source": [
    "#!python longest5gram.py -r local googlebooks-eng-all-5gram-20090715-0-filtered.txt > longest_5gram_test   \n",
    "\n",
    "#Hadoop mode for docker container\n",
    "#!python longest5gram.py -r hadoop googlebooks-eng-all-5gram-20090715-0-filtered.txt > longest_5gram_test   \n",
    "\n",
    "#Hadoop works when virtual python environment loaded up to hadoop is specified\n",
    "!python longest5gram.py -r hadoop hdfs:///user/cendylin/filtered-5Grams > longest_5gram_full_test \\\n",
    "    --python-bin=./my_env27_for_hadoop_upload/bin/python \\\n",
    "    --archive=hdfs:///user/nwchen24/virtualenv/my_env27_for_hadoop_upload.zip#my_env27_for_hadoop_upload\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "null\t155\r\n"
     ]
    }
   ],
   "source": [
    "!cat longest_5gram_full_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report Stats\n",
    "\n",
    "## Longest 5grams MR stats\n",
    "\n",
    "    altiscale cluster with 176 nodes and 476 GB of memory.\n",
    "\n",
    "__Step 1:__  \n",
    "\n",
    "    RUNNING for 8 minutes 13 seconds  \n",
    "    Map tasks = 190\n",
    "    Reduce tasks = 1 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW5.4.1 - B. Top 10 most frequent words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mostFrequentWords_combined.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mostFrequentWords_combined.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import re\n",
    "\n",
    "import mrjob\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from mrjob.protocol import RawValueProtocol\n",
    "from mrjob.protocol import RawProtocol\n",
    "from operator import itemgetter\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class mostFrequentWords(MRJob):\n",
    "        \n",
    "    #****************************************************\n",
    "    # Allows values to be treated as keys\n",
    "    MRJob.SORT_VALUES = True \n",
    "    \n",
    "    # The protocols are critical. It will not work without these:\n",
    "    INPUT_PROTOCOL = RawValueProtocol\n",
    "    INTERNAL_PROTOCOL = RawProtocol\n",
    "    OUTPUT_PROTOCOL = RawProtocol\n",
    "     \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(mostFrequentWords, self).__init__(*args, **kwargs)\n",
    "        self.NUM_REDUCERS = 16\n",
    "    #****************************************************\n",
    "\n",
    "\n",
    "    # START STUDENT CODE 5.4.1.B\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "\n",
    "        #get the list of tokens in the 5-gram\n",
    "        token_list = line.strip().lower().split()[:-3]\n",
    "\n",
    "        #emit each token and a 1   \n",
    "        for token in token_list:\n",
    "            yield token, str(1)\n",
    "\n",
    "    def combiner(self, token, count):\n",
    "        #just do local sums before passing to reducer\n",
    "        count_sum = 0\n",
    "        for item in count:\n",
    "            count_sum += int(item)\n",
    "        yield token, str(count_sum)            \n",
    "            \n",
    "    def reducer(self, token, count):\n",
    "        count_sum = 0\n",
    "        for item in count:\n",
    "            count_sum += int(item)\n",
    "        yield token, str(count_sum)\n",
    "    \n",
    "    #Implement total sort with ordered partitions\n",
    "    def mapper_partitioner_init(self):\n",
    "        \n",
    "        #Function to hash keys\n",
    "        def makeKeyHash(key, num_reducers):\n",
    "            byteof = lambda char: int(format(ord(char), 'b'), 2)\n",
    "            current_hash = 0\n",
    "            for c in key:\n",
    "                current_hash = (current_hash * 31 + byteof(c))\n",
    "            return current_hash % num_reducers\n",
    "        \n",
    "        # get the keys: printable ascii characters, starting with 'A'\n",
    "        keys = [str(unichr(i)) for i in range(65,65+self.NUM_REDUCERS)]\n",
    "        partitions = []\n",
    "        \n",
    "        #Hash each key and link the key to its hashed int value\n",
    "        for key in keys:\n",
    "            partitions.append([key, makeKeyHash(key, self.NUM_REDUCERS)])\n",
    "\n",
    "        #This step re-sorts so that the keys are 'out of order', but the partitions are 'ordered'\n",
    "        parts = sorted(partitions,key=itemgetter(1))\n",
    "        self.partition_keys = list(np.array(parts)[:,0])\n",
    "        \n",
    "        #NC Note - need to write a separate partition file that mirrors the distribution of your data.\n",
    "        #Want to end up with roughly equi-sized partitions.\n",
    "        #The example in the total sort code just assumes a uniform distribution.\n",
    "        #self.partition_file = np.arange(0,self.N,self.N/(self.NUM_REDUCERS))[::-1]\n",
    "        #You can mess with the breakpoints here to get evenly sized partitions.\n",
    "        #Right now, I'm just doing these breakpoints by trial and error to try and get closer to evenly sized partitions.\n",
    "        #A big improvement would be to make these breakpoints in a more sophisticated way to get evenly sized partitions.\n",
    "        #Action item 1\n",
    "        self.partition_file = np.array([100000, 50000, 30000, 25000, 20000, 15000, 12000, 10000, 5000, 4000, 3000, 2000, 1000, 500, 250, 0])\n",
    "        \n",
    "    #Read line and assign partition key to each line based on the value\n",
    "    def mapper_partition(self, key, value):\n",
    "\n",
    "        # Prepend the approriate key by finding the bucket, and using the index to fetch the key.\n",
    "        #Loop over number of reducers\n",
    "        for idx in xrange(self.NUM_REDUCERS):\n",
    "            if float(value) > self.partition_file[idx]:\n",
    "                #Need to emit the partition key here to leverage the hadoop shuffle phase.\n",
    "                yield str(self.partition_keys[idx]),key+\"\\t\"+value\n",
    "                break\n",
    "       \n",
    "            \n",
    "    def reducer_sort(self,key,value):\n",
    "        for v in value:\n",
    "            #This emits the partition key for illustration\n",
    "            #yield key,v\n",
    "            #This omits the partition key from the output\n",
    "            yield None, v\n",
    "    \n",
    "    \n",
    "    def steps(self):\n",
    "\n",
    "        JOBCONF_STEP1 = {'mapred.reduce.tasks': self.NUM_REDUCERS\n",
    "            \n",
    "        }\n",
    "        \n",
    "        JOBCONF_STEP2 = {\n",
    "            'stream.num.map.output.key.fields':3,\n",
    "            'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'stream.map.output.field.separator':\"\\t\",\n",
    "            'mapreduce.partition.keypartitioner.options':'-k1,1',\n",
    "            'mapreduce.partition.keycomparator.options':'-k3,3nr -k2,2',\n",
    "            'mapred.reduce.tasks': self.NUM_REDUCERS,\n",
    "            'partitioner':'org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner'\n",
    "        }\n",
    "        \n",
    "        return [\n",
    "            MRStep(jobconf=JOBCONF_STEP1,\n",
    "                   mapper=self.mapper,\n",
    "                   combiner=self.combiner,\n",
    "                   reducer=self.reducer),\n",
    "            MRStep(jobconf=JOBCONF_STEP2,\n",
    "                    mapper_init=self.mapper_partitioner_init,\n",
    "                    mapper=self.mapper_partition,\n",
    "                    reducer=self.reducer_sort\n",
    "                  )\n",
    "        ]\n",
    "     \n",
    "        \n",
    "    # END STUDENT CODE 5.4.1.B\n",
    "        \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    mostFrequentWords.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__On the full data set:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/06/20 05:05:46 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 5760 minutes, Emptier interval = 360 minutes.\n",
      "Moved: 'hdfs://nn-ia.s3s.altiscale.com:8020/user/nwchen24/HW5_Phase2' to trash at: hdfs://nn-ia.s3s.altiscale.com:8020/user/nwchen24/.Trash/Current\n",
      "No configs found; falling back on auto-configuration\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Creating temp directory /tmp/mostFrequentWords_combined.nwchen24.20170620.050547.015519\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/nwchen24/tmp/mrjob/mostFrequentWords_combined.nwchen24.20170620.050547.015519/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.reduce.tasks: mapreduce.job.reduces\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob1921351216468164218.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 190\n",
      "  number of splits:190\n",
      "  Submitting tokens for job: job_1497906899862_0358\n",
      "  Submitted application application_1497906899862_0358\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1497906899862_0358/\n",
      "  Running job: job_1497906899862_0358\n",
      "  Job job_1497906899862_0358 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 1% reduce 0%\n",
      "   map 2% reduce 0%\n",
      "   map 3% reduce 0%\n",
      "   map 4% reduce 0%\n",
      "   map 5% reduce 0%\n",
      "   map 6% reduce 0%\n",
      "   map 7% reduce 0%\n",
      "   map 8% reduce 0%\n",
      "   map 10% reduce 0%\n",
      "   map 11% reduce 0%\n",
      "   map 12% reduce 0%\n",
      "   map 13% reduce 0%\n",
      "   map 14% reduce 0%\n",
      "   map 15% reduce 0%\n",
      "   map 16% reduce 0%\n",
      "   map 17% reduce 0%\n",
      "   map 18% reduce 0%\n",
      "   map 19% reduce 0%\n",
      "   map 20% reduce 0%\n",
      "   map 21% reduce 0%\n",
      "   map 22% reduce 0%\n",
      "   map 23% reduce 0%\n",
      "   map 24% reduce 0%\n",
      "   map 25% reduce 0%\n",
      "   map 26% reduce 0%\n",
      "   map 27% reduce 0%\n",
      "   map 28% reduce 0%\n",
      "   map 29% reduce 0%\n",
      "   map 30% reduce 0%\n",
      "   map 31% reduce 0%\n",
      "   map 33% reduce 0%\n",
      "   map 34% reduce 0%\n",
      "   map 35% reduce 0%\n",
      "   map 36% reduce 0%\n",
      "   map 37% reduce 0%\n",
      "   map 38% reduce 0%\n",
      "   map 39% reduce 0%\n",
      "   map 40% reduce 0%\n",
      "   map 41% reduce 0%\n",
      "   map 42% reduce 0%\n",
      "   map 43% reduce 0%\n",
      "   map 44% reduce 0%\n",
      "   map 45% reduce 0%\n",
      "   map 46% reduce 0%\n",
      "   map 47% reduce 0%\n",
      "   map 48% reduce 0%\n",
      "   map 49% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 51% reduce 0%\n",
      "   map 52% reduce 0%\n",
      "   map 53% reduce 0%\n",
      "   map 54% reduce 0%\n",
      "   map 55% reduce 0%\n",
      "   map 56% reduce 0%\n",
      "   map 57% reduce 0%\n",
      "   map 58% reduce 0%\n",
      "   map 59% reduce 0%\n",
      "   map 60% reduce 0%\n",
      "   map 61% reduce 0%\n",
      "   map 62% reduce 0%\n",
      "   map 63% reduce 0%\n",
      "   map 64% reduce 0%\n",
      "   map 65% reduce 0%\n",
      "   map 66% reduce 0%\n",
      "   map 67% reduce 0%\n",
      "   map 69% reduce 0%\n",
      "   map 70% reduce 0%\n",
      "   map 72% reduce 0%\n",
      "   map 73% reduce 0%\n",
      "   map 74% reduce 0%\n",
      "   map 75% reduce 0%\n",
      "   map 77% reduce 0%\n",
      "   map 78% reduce 0%\n",
      "   map 80% reduce 0%\n",
      "   map 81% reduce 0%\n",
      "   map 82% reduce 0%\n",
      "   map 84% reduce 0%\n",
      "   map 86% reduce 0%\n",
      "   map 87% reduce 0%\n",
      "   map 88% reduce 0%\n",
      "   map 89% reduce 0%\n",
      "   map 90% reduce 0%\n",
      "   map 91% reduce 0%\n",
      "   map 92% reduce 0%\n",
      "   map 93% reduce 0%\n",
      "   map 94% reduce 0%\n",
      "   map 96% reduce 0%\n",
      "   map 97% reduce 0%\n",
      "   map 98% reduce 0%\n",
      "   map 99% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 25%\n",
      "   map 100% reduce 50%\n",
      "   map 100% reduce 88%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497906899862_0358 completed successfully\n",
      "  Output directory: hdfs:///user/nwchen24/tmp/mrjob/mostFrequentWords_combined.nwchen24.20170620.050547.015519/step-output/0000\n",
      "Counters: 52\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2156069116\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=3169513\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=16398344\n",
      "\t\tFILE: Number of bytes written=111531302\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2156101116\n",
      "\t\tHDFS: Number of bytes written=3169513\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=618\n",
      "\t\tHDFS: Number of write operations=32\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=2\n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=192\n",
      "\t\tLaunched reduce tasks=16\n",
      "\t\tOther local map tasks=2\n",
      "\t\tRack-local map tasks=190\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=43041905664\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=265950720\n",
      "\t\tTotal time spent by all map tasks (ms)=28022074\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=84066222\n",
      "\t\tTotal time spent by all reduce tasks (ms)=103887\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=519435\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=28022074\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=103887\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=6064530\n",
      "\t\tCombine input records=293411330\n",
      "\t\tCombine output records=6822745\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=131490\n",
      "\t\tInput split bytes=32000\n",
      "\t\tMap input records=58682266\n",
      "\t\tMap output bytes=2449095140\n",
      "\t\tMap output materialized bytes=67504834\n",
      "\t\tMap output records=293411330\n",
      "\t\tMerged Map outputs=3040\n",
      "\t\tPhysical memory (bytes) snapshot=161809346560\n",
      "\t\tReduce input groups=269339\n",
      "\t\tReduce input records=6822745\n",
      "\t\tReduce output records=269339\n",
      "\t\tReduce shuffle bytes=67504834\n",
      "\t\tShuffled Maps =3040\n",
      "\t\tSpilled Records=13645490\n",
      "\t\tTotal committed heap usage (bytes)=331391959040\n",
      "\t\tVirtual memory (bytes) snapshot=471528550400\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.reduce.tasks: mapreduce.job.reduces\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob201422004352695287.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 16\n",
      "  number of splits:16\n",
      "  Submitting tokens for job: job_1497906899862_0363\n",
      "  Submitted application application_1497906899862_0363\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1497906899862_0363/\n",
      "  Running job: job_1497906899862_0363\n",
      "  Job job_1497906899862_0363 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 14% reduce 0%\n",
      "   map 28% reduce 0%\n",
      "   map 41% reduce 0%\n",
      "   map 74% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 6%\n",
      "   map 100% reduce 13%\n",
      "   map 100% reduce 19%\n",
      "   map 100% reduce 31%\n",
      "   map 100% reduce 38%\n",
      "   map 100% reduce 44%\n",
      "   map 100% reduce 56%\n",
      "   map 100% reduce 69%\n",
      "   map 100% reduce 75%\n",
      "   map 100% reduce 94%\n",
      "   map 100% reduce 98%\n",
      "   map 100% reduce 99%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497906899862_0363 completed successfully\n",
      "  Output directory: hdfs:///user/nwchen24/HW5_Phase2\n",
      "Counters: 51\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3169513\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=3438852\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=2296111\n",
      "\t\tFILE: Number of bytes written=9296313\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3172697\n",
      "\t\tHDFS: Number of bytes written=3438852\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=96\n",
      "\t\tHDFS: Number of write operations=32\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tKilled reduce tasks=2\n",
      "\t\tLaunched map tasks=16\n",
      "\t\tLaunched reduce tasks=18\n",
      "\t\tRack-local map tasks=16\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=454130688\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=832880640\n",
      "\t\tTotal time spent by all map tasks (ms)=295658\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=886974\n",
      "\t\tTotal time spent by all reduce tasks (ms)=325344\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1626720\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=295658\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=325344\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=122580\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=4131\n",
      "\t\tInput split bytes=3184\n",
      "\t\tMap input records=269339\n",
      "\t\tMap output bytes=3977530\n",
      "\t\tMap output materialized bytes=2706382\n",
      "\t\tMap output records=269339\n",
      "\t\tMerged Map outputs=256\n",
      "\t\tPhysical memory (bytes) snapshot=18159570944\n",
      "\t\tReduce input groups=269339\n",
      "\t\tReduce input records=269339\n",
      "\t\tReduce output records=269339\n",
      "\t\tReduce shuffle bytes=2706382\n",
      "\t\tShuffled Maps =256\n",
      "\t\tSpilled Records=538678\n",
      "\t\tTotal committed heap usage (bytes)=58479607808\n",
      "\t\tVirtual memory (bytes) snapshot=88809062400\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Removing HDFS temp directory hdfs:///user/nwchen24/tmp/mrjob/mostFrequentWords_combined.nwchen24.20170620.050547.015519...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing temp directory /tmp/mostFrequentWords_combined.nwchen24.20170620.050547.015519...\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/nwchen24/HW5_Phase2\n",
    "  \n",
    "#Hadoop mode for altiscale cluster\n",
    "!python mostFrequentWords_combined.py -r hadoop hdfs:///user/cendylin/filtered-5Grams \\\n",
    "    --output-dir='/user/nwchen24/HW5_Phase2'\\\n",
    "    --no-output\\\n",
    "    --python-bin=./my_env27_for_hadoop_upload/bin/python \\\n",
    "    --archive=hdfs:///user/nwchen24/virtualenv/my_env27_for_hadoop_upload.zip#my_env27_for_hadoop_upload\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17 items\n",
      "-rw-r--r--   3 nwchen24 users          0 2017-06-20 05:14 /user/nwchen24/HW5_Phase2/_SUCCESS\n",
      "-rw-r--r--   3 nwchen24 users       3576 2017-06-20 05:14 /user/nwchen24/HW5_Phase2/part-00000\n",
      "-rw-r--r--   3 nwchen24 users       4091 2017-06-20 05:13 /user/nwchen24/HW5_Phase2/part-00001\n",
      "-rw-r--r--   3 nwchen24 users       6081 2017-06-20 05:14 /user/nwchen24/HW5_Phase2/part-00002\n",
      "-rw-r--r--   3 nwchen24 users       2730 2017-06-20 05:14 /user/nwchen24/HW5_Phase2/part-00003\n",
      "-rw-r--r--   3 nwchen24 users       3839 2017-06-20 05:14 /user/nwchen24/HW5_Phase2/part-00004\n",
      "-rw-r--r--   3 nwchen24 users       6118 2017-06-20 05:14 /user/nwchen24/HW5_Phase2/part-00005\n",
      "-rw-r--r--   3 nwchen24 users       5858 2017-06-20 05:14 /user/nwchen24/HW5_Phase2/part-00006\n",
      "-rw-r--r--   3 nwchen24 users       4967 2017-06-20 05:14 /user/nwchen24/HW5_Phase2/part-00007\n",
      "-rw-r--r--   3 nwchen24 users      23890 2017-06-20 05:14 /user/nwchen24/HW5_Phase2/part-00008\n",
      "-rw-r--r--   3 nwchen24 users       9931 2017-06-20 05:14 /user/nwchen24/HW5_Phase2/part-00009\n",
      "-rw-r--r--   3 nwchen24 users      14993 2017-06-20 05:14 /user/nwchen24/HW5_Phase2/part-00010\n",
      "-rw-r--r--   3 nwchen24 users      23521 2017-06-20 05:14 /user/nwchen24/HW5_Phase2/part-00011\n",
      "-rw-r--r--   3 nwchen24 users      49743 2017-06-20 05:14 /user/nwchen24/HW5_Phase2/part-00012\n",
      "-rw-r--r--   3 nwchen24 users      66260 2017-06-20 05:14 /user/nwchen24/HW5_Phase2/part-00013\n",
      "-rw-r--r--   3 nwchen24 users      85302 2017-06-20 05:14 /user/nwchen24/HW5_Phase2/part-00014\n",
      "-rw-r--r--   3 nwchen24 users    3127952 2017-06-20 05:14 /user/nwchen24/HW5_Phase2/part-00015\n",
      "The ten most frequent words in the google 5-gram dataset are:\n",
      "the\t27502442\t\n",
      "of\t18191779\t\n",
      "to\t12075971\t\n",
      "in\t7881239\t\n",
      "a\t7853465\t\n",
      "and\t7767900\t\n",
      "that\t4316884\t\n",
      "is\t3847383\t\n",
      "be\t3288731\t\n",
      "for\t2763613\t\n",
      "263\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "====================================================================================================\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "The first five words in the remaining partitions are:\n",
      "act\t99974\t\n",
      "young\t99561\t\n",
      "home\t99392\t\n",
      "year\t98901\t\n",
      "process\t98764\t\n",
      "295\n",
      "====================================================================================================\n",
      "taking\t49985\t\n",
      "lord\t49839\t\n",
      "allowed\t49813\t\n",
      "latter\t49726\t\n",
      "lead\t49688\t\n",
      "419\n",
      "====================================================================================================\n",
      "responsibility\t29949\t\n",
      "principal\t29919\t\n",
      "levels\t29850\t\n",
      "sight\t29823\t\n",
      "bear\t29789\t\n",
      "187\n",
      "====================================================================================================\n",
      "scene\t24996\t\n",
      "labour\t24960\t\n",
      "memory\t24958\t\n",
      "feelings\t24879\t\n",
      "bill\t24875\t\n",
      "260\n",
      "====================================================================================================\n",
      "daughter\t19993\t\n",
      "style\t19986\t\n",
      "credit\t19963\t\n",
      "politics\t19958\t\n",
      "finding\t19939\t\n",
      "411\n",
      "====================================================================================================\n",
      "mixture\t14989\t\n",
      "sex\t14979\t\n",
      "requires\t14972\t\n",
      "republic\t14957\t\n",
      "identity\t14950\t\n",
      "390\n",
      "====================================================================================================\n",
      "degrees\t11990\t\n",
      "layer\t11987\t\n",
      "criticism\t11983\t\n",
      "intensity\t11971\t\n",
      "muscle\t11967\t\n",
      "330\n",
      "====================================================================================================\n",
      "reply\t9996\t\n",
      "entry\t9987\t\n",
      "validity\t9975\t\n",
      "buried\t9971\t\n",
      "windows\t9959\t\n",
      "1680\n",
      "====================================================================================================\n",
      "decree\t4995\t\n",
      "planted\t4995\t\n",
      "respiratory\t4994\t\n",
      "sisters\t4991\t\n",
      "theology\t4991\t\n",
      "684\n",
      "====================================================================================================\n",
      "publications\t4000\t\n",
      "pulse\t4000\t\n",
      "pile\t3998\t\n",
      "designated\t3997\t\n",
      "inclusion\t3997\t\n",
      "1022\n",
      "====================================================================================================\n",
      "enhanced\t3000\t\n",
      "heap\t2999\t\n",
      "introducing\t2999\t\n",
      "magistrate\t2998\t\n",
      "rats\t2998\t\n",
      "1608\n",
      "====================================================================================================\n",
      "blanket\t2000\t\n",
      "court's\t2000\t\n",
      "neighboring\t2000\t\n",
      "poetical\t2000\t\n",
      "conveniently\t1999\t\n",
      "3356\n",
      "====================================================================================================\n",
      "clinically\t1000\t\n",
      "convened\t1000\t\n",
      "coordinated\t1000\t\n",
      "cords\t1000\t\n",
      "drill\t1000\t\n",
      "4786\n",
      "====================================================================================================\n",
      "antagonists\t500\t\n",
      "brazen\t500\t\n",
      "conservatism\t500\t\n",
      "consort\t500\t\n",
      "devil's\t500\t\n",
      "cat: Unable to write to output stream.\n",
      "6111\n",
      "====================================================================================================\n",
      "arteriosus\t250\t\n",
      "association's\t250\t\n",
      "attrition\t250\t\n",
      "blinking\t250\t\n",
      "brag\t250\t\n",
      "cat: Unable to write to output stream.\n",
      "247537\n",
      "====================================================================================================\n",
      "cat: `/user/nwchen24/HW5_Phase2/part-00016': No such file or directory\n",
      "cat: `/user/nwchen24/HW5_Phase2/part-00016': No such file or directory\n",
      "0\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/nwchen24/HW5_Phase2\n",
    "\n",
    "print \"The ten most frequent words in the google 5-gram dataset are:\"\n",
    "!hdfs dfs -cat /user/nwchen24/HW5_Phase2/part-00000 | head -n 10\n",
    "!hdfs dfs -cat /user/nwchen24/HW5_Phase2/part-00000 | wc -l\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report Stats\n",
    "\n",
    "## Most frequent words MR stats\n",
    "    \n",
    "    altiscale cluster with 176 nodes and 476 GB of memory\n",
    "    \n",
    "__Step 1:__   \n",
    "\n",
    "    RUNNING for 4 minutets 46 seconds\n",
    "    Launched map tasks=190  \n",
    "    Launched reduce tasks=16   \n",
    "\n",
    "__Step 2:__  \n",
    "\n",
    "    RUNNING for 3 minutes 7 seconds   \n",
    "    Launched map tasks=16\n",
    "    Launched reduce tasks=16  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW5.4.1 - C. 20 Most/Least densely appearing words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mostLeastDenseWords.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mostLeastDenseWords.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import division\n",
    "import re\n",
    "import mrjob\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from mrjob.protocol import RawValueProtocol\n",
    "from mrjob.protocol import RawProtocol\n",
    "from operator import itemgetter\n",
    "import numpy as np\n",
    "\n",
    "class mostLeastDenseWords(MRJob):\n",
    "    \n",
    "    # START STUDENT CODE 5.4.1.C\n",
    "    \n",
    "            \n",
    "    #****************************************************\n",
    "    # Allows values to be treated as keys\n",
    "    MRJob.SORT_VALUES = True \n",
    "    \n",
    "    # The protocols are critical. Total sort will not work without these:\n",
    "    #Raw protocols require emitting strings from each phase\n",
    "    INPUT_PROTOCOL = RawValueProtocol\n",
    "    INTERNAL_PROTOCOL = RawProtocol\n",
    "    OUTPUT_PROTOCOL = RawProtocol\n",
    "     \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(mostLeastDenseWords, self).__init__(*args, **kwargs)\n",
    "        self.NUM_REDUCERS = 16\n",
    "    #****************************************************\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        token_list = line.strip().lower().split()[:-3]\n",
    "        pages = line.strip().lower().split()[-2]\n",
    "        count = line.strip().lower().split()[-3]\n",
    "        \n",
    "        for token in token_list:\n",
    "            #yield token, pages+\"\\t\"+count\n",
    "            yield token, pages+\"\\t\"+str(1)\n",
    "\n",
    "    #increment page and length sums from the mapper before sending to reducer\n",
    "    def combiner(self, token, count):\n",
    "        page_sum = 0\n",
    "        length_sum = 0\n",
    "\n",
    "        for item in count:\n",
    "            item_list = item.split(\"\\t\")\n",
    "            page_sum = page_sum + int(item_list[0])\n",
    "            length_sum = length_sum + int(item_list[1])\n",
    "        yield token, str(page_sum)+\"\\t\"+str(length_sum)\n",
    "\n",
    "    def reducer(self, token, count):\n",
    "        page_sum = 0\n",
    "        length_sum = 0\n",
    "        \n",
    "        #increment page and length sums\n",
    "        for item in count:\n",
    "            item_list = item.split(\"\\t\")\n",
    "            page_sum = page_sum + int(item_list[0])\n",
    "            length_sum = length_sum + int(item_list[1])\n",
    "        yield token, str(length_sum / page_sum)\n",
    "      \n",
    "           \n",
    "    #Implement total sort with ordered partitions\n",
    "    def mapper_partitioner_init(self):\n",
    "        \n",
    "        #Function to hash keys\n",
    "        def makeKeyHash(key, num_reducers):\n",
    "            byteof = lambda char: int(format(ord(char), 'b'), 2)\n",
    "            current_hash = 0\n",
    "            for c in key:\n",
    "                current_hash = (current_hash * 31 + byteof(c))\n",
    "            return current_hash % num_reducers\n",
    "        \n",
    "        # get the keys: printable ascii characters, starting with 'A'\n",
    "        keys = [str(unichr(i)) for i in range(65,65+self.NUM_REDUCERS)]\n",
    "        partitions = []\n",
    "        \n",
    "        #Hash each key and link the key to its hashed int value\n",
    "        for key in keys:\n",
    "            partitions.append([key, makeKeyHash(key, self.NUM_REDUCERS)])\n",
    "\n",
    "        #This step re-sorts so that the keys are 'out of order', but the partitions are 'ordered'\n",
    "        parts = sorted(partitions,key=itemgetter(1))\n",
    "        self.partition_keys = list(np.array(parts)[:,0])\n",
    "        \n",
    "        #Create partition file\n",
    "        self.partition_file = np.array([.0225, .02, .018, .0175, .016, .015, .0135, .0125, .01, .008, .0075, .005, .0025, .002, .001, 0])\n",
    "        \n",
    "    #Read line and assign partition key to each line based on the value\n",
    "    def mapper_partition(self, key, value):\n",
    "\n",
    "        # Prepend the approriate key by finding the bucket, and using the index to fetch the key.\n",
    "        #Loop over number of reducers\n",
    "        for idx in xrange(self.NUM_REDUCERS):\n",
    "            if float(value) > self.partition_file[idx]:\n",
    "                #Need to emit the partition key here to leverage the hadoop shuffle phase.\n",
    "                yield str(self.partition_keys[idx]),key+\"\\t\"+value\n",
    "                break\n",
    "       \n",
    "            \n",
    "    def reducer_sort(self,key,value):\n",
    "        for v in value:\n",
    "            #This emits the partition key for illustration\n",
    "            #yield key,v\n",
    "            #This omits the partition key from the output\n",
    "            yield None, v\n",
    "        \n",
    "    def steps(self):\n",
    "        \n",
    "        JOBCONF_STEP1 = {\n",
    "            'stream.num.map.output.key.fields':2,\n",
    "            'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'stream.map.output.field.separator':\"\\t\",\n",
    "            'mapreduce.partition.keypartitioner.options':'-k1,1',\n",
    "            'mapred.reduce.tasks': self.NUM_REDUCERS,\n",
    "            'partitioner':'org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner'\n",
    "        }\n",
    "        \n",
    "        JOBCONF_STEP2 = {\n",
    "            'stream.num.map.output.key.fields':3,\n",
    "            'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'stream.map.output.field.separator':\"\\t\",\n",
    "            'mapreduce.partition.keypartitioner.options':'-k1,1',\n",
    "            'mapreduce.partition.keycomparator.options':'-k3,3nr -k2,2',\n",
    "            'mapred.reduce.tasks': self.NUM_REDUCERS,\n",
    "            'partitioner':'org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner'\n",
    "        }\n",
    "        \n",
    "        return [\n",
    "            MRStep(jobconf=JOBCONF_STEP1,\n",
    "                   mapper=self.mapper,\n",
    "                   combiner=self.combiner,\n",
    "                   reducer=self.reducer),\n",
    "            MRStep(jobconf=JOBCONF_STEP2,\n",
    "                    mapper_init=self.mapper_partitioner_init,\n",
    "                    mapper=self.mapper_partition,\n",
    "                    reducer=self.reducer_sort\n",
    "                  )\n",
    "        ]\n",
    "          \n",
    "    # END STUDENT CODE 5.4.1.C\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    mostLeastDenseWords.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__On the test data set:__   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/nwchen24/HW5_Phase2': No such file or directory\n",
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/mostLeastDenseWords.nwchen24.20170621.212801.034698\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/nwchen24/tmp/mrjob/mostLeastDenseWords.nwchen24.20170621.212801.034698/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.reduce.tasks: mapreduce.job.reduces\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob2087475590080263371.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1497906899862_1318\n",
      "  Submitted application application_1497906899862_1318\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1497906899862_1318/\n",
      "  Running job: job_1497906899862_1318\n",
      "  Job job_1497906899862_1318 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 33% reduce 0%\n",
      "   map 67% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 13%\n",
      "   map 100% reduce 25%\n",
      "   map 100% reduce 31%\n",
      "   map 100% reduce 81%\n",
      "   map 100% reduce 94%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497906899862_1318 completed successfully\n",
      "  Output directory: hdfs:///user/nwchen24/tmp/mrjob/mostLeastDenseWords.nwchen24.20170621.212801.034698/step-output/0000\n",
      "Counters: 50\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=11489475\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=897205\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=643532\n",
      "\t\tFILE: Number of bytes written=3804711\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=11489863\n",
      "\t\tHDFS: Number of bytes written=897205\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=54\n",
      "\t\tHDFS: Number of write operations=32\n",
      "\tJob Counters \n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=16\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=97890816\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=279188480\n",
      "\t\tTotal time spent by all map tasks (ms)=63731\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=191193\n",
      "\t\tTotal time spent by all reduce tasks (ms)=109058\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=545290\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=63731\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=109058\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=48900\n",
      "\t\tCombine input records=1558070\n",
      "\t\tCombine output records=54102\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=872\n",
      "\t\tInput split bytes=388\n",
      "\t\tMap input records=311614\n",
      "\t\tMap output bytes=16647060\n",
      "\t\tMap output materialized bytes=744401\n",
      "\t\tMap output records=1558070\n",
      "\t\tMerged Map outputs=32\n",
      "\t\tPhysical memory (bytes) snapshot=6656167936\n",
      "\t\tReduce input groups=36353\n",
      "\t\tReduce input records=54102\n",
      "\t\tReduce output records=36353\n",
      "\t\tReduce shuffle bytes=744401\n",
      "\t\tShuffled Maps =32\n",
      "\t\tSpilled Records=108204\n",
      "\t\tTotal committed heap usage (bytes)=36471570432\n",
      "\t\tVirtual memory (bytes) snapshot=57914392576\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.reduce.tasks: mapreduce.job.reduces\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob1392612959889318197.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 16\n",
      "  number of splits:16\n",
      "  Submitting tokens for job: job_1497906899862_1321\n",
      "  Submitted application application_1497906899862_1321\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1497906899862_1321/\n",
      "  Running job: job_1497906899862_1321\n",
      "  Job job_1497906899862_1321 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 13% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 6%\n",
      "   map 100% reduce 13%\n",
      "   map 100% reduce 25%\n",
      "   map 100% reduce 81%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497906899862_1321 completed successfully\n",
      "  Output directory: hdfs:///user/nwchen24/HW5_Phase2\n",
      "Counters: 51\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=897205\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=933558\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=495722\n",
      "\t\tFILE: Number of bytes written=5408191\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=900277\n",
      "\t\tHDFS: Number of bytes written=933558\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=96\n",
      "\t\tHDFS: Number of write operations=32\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=16\n",
      "\t\tLaunched reduce tasks=16\n",
      "\t\tRack-local map tasks=16\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=263030784\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=319815680\n",
      "\t\tTotal time spent by all map tasks (ms)=171244\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=513732\n",
      "\t\tTotal time spent by all reduce tasks (ms)=124928\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=624640\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=171244\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=124928\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=50940\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=2121\n",
      "\t\tInput split bytes=3072\n",
      "\t\tMap input records=36353\n",
      "\t\tMap output bytes=1006264\n",
      "\t\tMap output materialized bytes=621753\n",
      "\t\tMap output records=36353\n",
      "\t\tMerged Map outputs=256\n",
      "\t\tPhysical memory (bytes) snapshot=17688186880\n",
      "\t\tReduce input groups=36353\n",
      "\t\tReduce input records=36353\n",
      "\t\tReduce output records=36353\n",
      "\t\tReduce shuffle bytes=621753\n",
      "\t\tShuffled Maps =256\n",
      "\t\tSpilled Records=72706\n",
      "\t\tTotal committed heap usage (bytes)=58418266112\n",
      "\t\tVirtual memory (bytes) snapshot=88673898496\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Removing HDFS temp directory hdfs:///user/nwchen24/tmp/mrjob/mostLeastDenseWords.nwchen24.20170621.212801.034698...\n",
      "Removing temp directory /tmp/mostLeastDenseWords.nwchen24.20170621.212801.034698...\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/nwchen24/HW5_Phase2\n",
    "\n",
    "#Hadoop mode for altiscale cluster\n",
    "!python mostLeastDenseWords.py -r hadoop google-5gram-sample.txt \\\n",
    "    --output-dir='/user/nwchen24/HW5_Phase2'\\\n",
    "    --no-output\\\n",
    "    --python-bin=./my_env27_for_hadoop_upload/bin/python \\\n",
    "    --archive=hdfs:///user/nwchen24/virtualenv/my_env27_for_hadoop_upload.zip#my_env27_for_hadoop_upload\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17 items\n",
      "-rw-r--r--   3 nwchen24 users          0 2017-06-21 21:30 /user/nwchen24/HW5_Phase2/_SUCCESS\n",
      "-rw-r--r--   3 nwchen24 users          0 2017-06-21 21:30 /user/nwchen24/HW5_Phase2/part-00000\n",
      "-rw-r--r--   3 nwchen24 users          0 2017-06-21 21:30 /user/nwchen24/HW5_Phase2/part-00001\n",
      "-rw-r--r--   3 nwchen24 users          0 2017-06-21 21:30 /user/nwchen24/HW5_Phase2/part-00002\n",
      "-rw-r--r--   3 nwchen24 users      53505 2017-06-21 21:30 /user/nwchen24/HW5_Phase2/part-00003\n",
      "-rw-r--r--   3 nwchen24 users      70485 2017-06-21 21:30 /user/nwchen24/HW5_Phase2/part-00004\n",
      "-rw-r--r--   3 nwchen24 users      75630 2017-06-21 21:30 /user/nwchen24/HW5_Phase2/part-00005\n",
      "-rw-r--r--   3 nwchen24 users      23261 2017-06-21 21:30 /user/nwchen24/HW5_Phase2/part-00006\n",
      "-rw-r--r--   3 nwchen24 users     107519 2017-06-21 21:30 /user/nwchen24/HW5_Phase2/part-00007\n",
      "-rw-r--r--   3 nwchen24 users     129337 2017-06-21 21:30 /user/nwchen24/HW5_Phase2/part-00008\n",
      "-rw-r--r--   3 nwchen24 users     141054 2017-06-21 21:30 /user/nwchen24/HW5_Phase2/part-00009\n",
      "-rw-r--r--   3 nwchen24 users     151526 2017-06-21 21:30 /user/nwchen24/HW5_Phase2/part-00010\n",
      "-rw-r--r--   3 nwchen24 users     112840 2017-06-21 21:30 /user/nwchen24/HW5_Phase2/part-00011\n",
      "-rw-r--r--   3 nwchen24 users      51528 2017-06-21 21:30 /user/nwchen24/HW5_Phase2/part-00012\n",
      "-rw-r--r--   3 nwchen24 users       5669 2017-06-21 21:30 /user/nwchen24/HW5_Phase2/part-00013\n",
      "-rw-r--r--   3 nwchen24 users       8275 2017-06-21 21:30 /user/nwchen24/HW5_Phase2/part-00014\n",
      "-rw-r--r--   3 nwchen24 users       2929 2017-06-21 21:30 /user/nwchen24/HW5_Phase2/part-00015\n",
      "The twenty most densely occurring words in the google 5-gram dataset are:\n",
      "0\n",
      "The ten least densely occurring words in the google 5-gram dataset are:\n",
      "cat: `/user/nwchen24/HW5_Phase2/part-00016': No such file or directory\n",
      "cat: `/user/nwchen24/HW5_Phase2/part-00016': No such file or directory\n",
      "0\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "====================================================================================================\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "The first five words in the remaining partitions are:\n",
      "0\n",
      "====================================================================================================\n",
      "0\n",
      "====================================================================================================\n",
      "abbe\t0.025\t\n",
      "abeyance\t0.025\t\n",
      "accomac\t0.025\t\n",
      "addicts\t0.025\t\n",
      "admittedly\t0.025\t\n",
      "2181\n",
      "====================================================================================================\n",
      "arabians\t0.0224719101124\t\n",
      "beneficence\t0.0224719101124\t\n",
      "cascade\t0.0224719101124\t\n",
      "caustic\t0.0224719101124\t\n",
      "clips\t0.0224719101124\t\n",
      "2710\n",
      "====================================================================================================\n",
      "aback\t0.02\t\n",
      "abiteboul\t0.02\t\n",
      "abortions\t0.02\t\n",
      "abscissas\t0.02\t\n",
      "absolution\t0.02\t\n",
      "cat: Unable to write to output stream.\n",
      "3100\n",
      "====================================================================================================\n",
      "wasted\t0.0179910044978\t\n",
      "alluring\t0.0179640718563\t\n",
      "avowal\t0.0179640718563\t\n",
      "calabria\t0.0179640718563\t\n",
      "carl\t0.0179640718563\t\n",
      "898\n",
      "====================================================================================================\n",
      "coloured\t0.0174927113703\t\n",
      "dissenting\t0.0174927113703\t\n",
      "faintly\t0.0174927113703\t\n",
      "ship's\t0.0174927113703\t\n",
      "maidens\t0.0174825174825\t\n",
      "cat: Unable to write to output stream.\n",
      "4272\n",
      "====================================================================================================\n",
      "annoyance\t0.015\t\n",
      "capillaries\t0.015\t\n",
      "charging\t0.015\t\n",
      "confronts\t0.015\t\n",
      "debris\t0.015\t\n",
      "cat: Unable to write to output stream.\n",
      "5009\n",
      "====================================================================================================\n",
      "alten\t0.0125\t\n",
      "applauded\t0.0125\t\n",
      "appropriately\t0.0125\t\n",
      "appropriating\t0.0125\t\n",
      "apron\t0.0125\t\n",
      "cat: Unable to write to output stream.\n",
      "5533\n",
      "====================================================================================================\n",
      "ac\t0.01\t\n",
      "aigina\t0.01\t\n",
      "atm\t0.01\t\n",
      "attempered\t0.01\t\n",
      "barricades\t0.01\t\n",
      "cat: Unable to write to output stream.\n",
      "5787\n",
      "====================================================================================================\n",
      "comte\t0.0075\t\n",
      "dwindled\t0.0075\t\n",
      "reigns\t0.0075\t\n",
      "vogue\t0.0075\t\n",
      "frequently\t0.00749765698219\t\n",
      "cat: Unable to write to output stream.\n",
      "4288\n",
      "====================================================================================================\n",
      "catholicity\t0.005\t\n",
      "cloves\t0.005\t\n",
      "consonance\t0.005\t\n",
      "dorsum\t0.005\t\n",
      "drives\t0.005\t\n",
      "1943\n",
      "====================================================================================================\n",
      "amorous\t0.00249376558603\t\n",
      "reveled\t0.00249376558603\t\n",
      "sproul\t0.00249376558603\t\n",
      "rs\t0.00249066002491\t\n",
      "approximation\t0.00248194945848\t\n",
      "214\n",
      "====================================================================================================\n",
      "magma\t0.002\t\n",
      "thanking\t0.00199866755496\t\n",
      "tribunes\t0.00199866755496\t\n",
      "reproduced\t0.00199763915373\t\n",
      "palatal\t0.00199401794616\t\n",
      "311\n",
      "====================================================================================================\n",
      "gyration\t8.28363154407e-05\t\n",
      "slap\t6.04302634759e-05\t\n",
      "keepers\t0.000993048659384\t\n",
      "marche\t0.000991080277502\t\n",
      "scrannel\t0.000983284169125\t\n",
      "107\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/nwchen24/HW5_Phase2/\n",
    "\n",
    "print \"The twenty most densely occurring words in the google 5-gram dataset are:\"\n",
    "!hdfs dfs -cat /user/nwchen24/HW5_Phase2/part-00000 | head -n 20\n",
    "!hdfs dfs -cat /user/nwchen24/HW5_Phase2/part-00000 | wc -l\n",
    "\n",
    "print \"The ten least densely occurring words in the google 5-gram dataset are:\"\n",
    "!hdfs dfs -cat /user/nwchen24/HW5_Phase2/part-00015 | tail -n 10\n",
    "!hdfs dfs -cat /user/nwchen24/HW5_Phase2/part-00015 | wc -l\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__On the full data set:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/06/21 22:02:06 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 5760 minutes, Emptier interval = 360 minutes.\n",
      "Moved: 'hdfs://nn-ia.s3s.altiscale.com:8020/user/nwchen24/HW5_Phase2' to trash at: hdfs://nn-ia.s3s.altiscale.com:8020/user/nwchen24/.Trash/Current\n",
      "No configs found; falling back on auto-configuration\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Creating temp directory /tmp/mostLeastDenseWords.nwchen24.20170621.220207.741570\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/nwchen24/tmp/mrjob/mostLeastDenseWords.nwchen24.20170621.220207.741570/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.reduce.tasks: mapreduce.job.reduces\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob489636043396763224.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 190\n",
      "  number of splits:190\n",
      "  Submitting tokens for job: job_1497906899862_1333\n",
      "  Submitted application application_1497906899862_1333\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1497906899862_1333/\n",
      "  Running job: job_1497906899862_1333\n",
      "  Job job_1497906899862_1333 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 1% reduce 0%\n",
      "   map 2% reduce 0%\n",
      "   map 3% reduce 0%\n",
      "   map 4% reduce 0%\n",
      "   map 5% reduce 0%\n",
      "   map 6% reduce 0%\n",
      "   map 7% reduce 0%\n",
      "   map 8% reduce 0%\n",
      "   map 9% reduce 0%\n",
      "   map 10% reduce 0%\n",
      "   map 11% reduce 0%\n",
      "   map 13% reduce 0%\n",
      "   map 14% reduce 0%\n",
      "   map 16% reduce 0%\n",
      "   map 17% reduce 0%\n",
      "   map 18% reduce 0%\n",
      "   map 20% reduce 0%\n",
      "   map 21% reduce 0%\n",
      "   map 22% reduce 0%\n",
      "   map 24% reduce 0%\n",
      "   map 25% reduce 0%\n",
      "   map 27% reduce 0%\n",
      "   map 28% reduce 0%\n",
      "   map 29% reduce 0%\n",
      "   map 30% reduce 0%\n",
      "   map 32% reduce 0%\n",
      "   map 33% reduce 0%\n",
      "   map 34% reduce 0%\n",
      "   map 36% reduce 0%\n",
      "   map 37% reduce 0%\n",
      "   map 38% reduce 0%\n",
      "   map 39% reduce 0%\n",
      "   map 40% reduce 0%\n",
      "   map 42% reduce 0%\n",
      "   map 43% reduce 0%\n",
      "   map 44% reduce 0%\n",
      "   map 45% reduce 0%\n",
      "   map 46% reduce 0%\n",
      "   map 47% reduce 0%\n",
      "   map 48% reduce 0%\n",
      "   map 49% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 51% reduce 0%\n",
      "   map 52% reduce 0%\n",
      "   map 53% reduce 0%\n",
      "   map 54% reduce 0%\n",
      "   map 55% reduce 0%\n",
      "   map 56% reduce 0%\n",
      "   map 57% reduce 0%\n",
      "   map 58% reduce 0%\n",
      "   map 59% reduce 0%\n",
      "   map 60% reduce 0%\n",
      "   map 61% reduce 0%\n",
      "   map 62% reduce 0%\n",
      "   map 63% reduce 0%\n",
      "   map 64% reduce 0%\n",
      "   map 65% reduce 0%\n",
      "   map 66% reduce 0%\n",
      "   map 67% reduce 0%\n",
      "   map 68% reduce 0%\n",
      "   map 69% reduce 0%\n",
      "   map 70% reduce 0%\n",
      "   map 71% reduce 0%\n",
      "   map 72% reduce 0%\n",
      "   map 74% reduce 0%\n",
      "   map 75% reduce 0%\n",
      "   map 76% reduce 0%\n",
      "   map 79% reduce 0%\n",
      "   map 81% reduce 0%\n",
      "   map 83% reduce 0%\n",
      "   map 84% reduce 0%\n",
      "   map 85% reduce 0%\n",
      "   map 87% reduce 0%\n",
      "   map 92% reduce 0%\n",
      "   map 93% reduce 0%\n",
      "   map 94% reduce 0%\n",
      "   map 95% reduce 0%\n",
      "   map 96% reduce 0%\n",
      "   map 97% reduce 0%\n",
      "   map 98% reduce 0%\n",
      "   map 99% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 25%\n",
      "   map 100% reduce 44%\n",
      "   map 100% reduce 75%\n",
      "   map 100% reduce 94%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497906899862_1333 completed successfully\n",
      "  Output directory: hdfs:///user/nwchen24/tmp/mrjob/mostLeastDenseWords.nwchen24.20170621.220207.741570/step-output/0000\n",
      "Counters: 52\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2156069116\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=6743011\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=44079040\n",
      "\t\tFILE: Number of bytes written=166174926\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2156101116\n",
      "\t\tHDFS: Number of bytes written=6743011\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=618\n",
      "\t\tHDFS: Number of write operations=32\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=191\n",
      "\t\tLaunched reduce tasks=16\n",
      "\t\tOther local map tasks=2\n",
      "\t\tRack-local map tasks=189\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=47751129600\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=289431040\n",
      "\t\tTotal time spent by all map tasks (ms)=31087975\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=93263925\n",
      "\t\tTotal time spent by all reduce tasks (ms)=113059\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=565295\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=31087975\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=113059\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=6920230\n",
      "\t\tCombine input records=293411330\n",
      "\t\tCombine output records=6822745\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=127640\n",
      "\t\tInput split bytes=32000\n",
      "\t\tMap input records=58682266\n",
      "\t\tMap output bytes=3135816560\n",
      "\t\tMap output materialized bytes=94383508\n",
      "\t\tMap output records=293411330\n",
      "\t\tMerged Map outputs=3040\n",
      "\t\tPhysical memory (bytes) snapshot=161001975808\n",
      "\t\tReduce input groups=269339\n",
      "\t\tReduce input records=6822745\n",
      "\t\tReduce output records=269339\n",
      "\t\tReduce shuffle bytes=94383508\n",
      "\t\tShuffled Maps =3040\n",
      "\t\tSpilled Records=13645490\n",
      "\t\tTotal committed heap usage (bytes)=329839017984\n",
      "\t\tVirtual memory (bytes) snapshot=471333273600\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.reduce.tasks: mapreduce.job.reduces\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob3254104798207132430.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 16\n",
      "  number of splits:16\n",
      "  Submitting tokens for job: job_1497906899862_1337\n",
      "  Submitted application application_1497906899862_1337\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1497906899862_1337/\n",
      "  Running job: job_1497906899862_1337\n",
      "  Job job_1497906899862_1337 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 13% reduce 0%\n",
      "   map 38% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 13%\n",
      "   map 100% reduce 25%\n",
      "   map 100% reduce 50%\n",
      "   map 100% reduce 94%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497906899862_1337 completed successfully\n",
      "  Output directory: hdfs:///user/nwchen24/HW5_Phase2\n",
      "Counters: 50\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=6743011\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=7012350\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=3556800\n",
      "\t\tFILE: Number of bytes written=12064849\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=6746083\n",
      "\t\tHDFS: Number of bytes written=7012350\n",
      "\t\tHDFS: Number of large read operations=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of read operations=96\n",
      "\t\tHDFS: Number of write operations=32\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=16\n",
      "\t\tLaunched reduce tasks=16\n",
      "\t\tRack-local map tasks=16\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=393942528\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=232573440\n",
      "\t\tTotal time spent by all map tasks (ms)=256473\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=769419\n",
      "\t\tTotal time spent by all reduce tasks (ms)=90849\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=454245\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=256473\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=90849\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=78940\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=2716\n",
      "\t\tInput split bytes=3072\n",
      "\t\tMap input records=269339\n",
      "\t\tMap output bytes=7551028\n",
      "\t\tMap output materialized bytes=4217333\n",
      "\t\tMap output records=269339\n",
      "\t\tMerged Map outputs=256\n",
      "\t\tPhysical memory (bytes) snapshot=17988259840\n",
      "\t\tReduce input groups=269339\n",
      "\t\tReduce input records=269339\n",
      "\t\tReduce output records=269339\n",
      "\t\tReduce shuffle bytes=4217333\n",
      "\t\tShuffled Maps =256\n",
      "\t\tSpilled Records=538678\n",
      "\t\tTotal committed heap usage (bytes)=58540949504\n",
      "\t\tVirtual memory (bytes) snapshot=88679972864\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Removing HDFS temp directory hdfs:///user/nwchen24/tmp/mrjob/mostLeastDenseWords.nwchen24.20170621.220207.741570...\n",
      "Removing temp directory /tmp/mostLeastDenseWords.nwchen24.20170621.220207.741570...\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/nwchen24/HW5_Phase2\n",
    "\n",
    "#Hadoop mode for altiscale cluster\n",
    "!python mostLeastDenseWords.py -r hadoop hdfs:///user/cendylin/filtered-5Grams \\\n",
    "    --output-dir='/user/nwchen24/HW5_Phase2'\\\n",
    "    --no-output\\\n",
    "    --python-bin=./my_env27_for_hadoop_upload/bin/python \\\n",
    "    --archive=hdfs:///user/nwchen24/virtualenv/my_env27_for_hadoop_upload.zip#my_env27_for_hadoop_upload\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17 items\n",
      "-rw-r--r--   3 nwchen24 users          0 2017-06-21 22:08 /user/nwchen24/HW5_Phase2/_SUCCESS\n",
      "-rw-r--r--   3 nwchen24 users     504226 2017-06-21 22:07 /user/nwchen24/HW5_Phase2/part-00000\n",
      "-rw-r--r--   3 nwchen24 users     629455 2017-06-21 22:07 /user/nwchen24/HW5_Phase2/part-00001\n",
      "-rw-r--r--   3 nwchen24 users     640449 2017-06-21 22:07 /user/nwchen24/HW5_Phase2/part-00002\n",
      "-rw-r--r--   3 nwchen24 users     193941 2017-06-21 22:07 /user/nwchen24/HW5_Phase2/part-00003\n",
      "-rw-r--r--   3 nwchen24 users     551442 2017-06-21 22:07 /user/nwchen24/HW5_Phase2/part-00004\n",
      "-rw-r--r--   3 nwchen24 users     392429 2017-06-21 22:07 /user/nwchen24/HW5_Phase2/part-00005\n",
      "-rw-r--r--   3 nwchen24 users     688067 2017-06-21 22:07 /user/nwchen24/HW5_Phase2/part-00006\n",
      "-rw-r--r--   3 nwchen24 users     457387 2017-06-21 22:07 /user/nwchen24/HW5_Phase2/part-00007\n",
      "-rw-r--r--   3 nwchen24 users    1218704 2017-06-21 22:07 /user/nwchen24/HW5_Phase2/part-00008\n",
      "-rw-r--r--   3 nwchen24 users     837528 2017-06-21 22:07 /user/nwchen24/HW5_Phase2/part-00009\n",
      "-rw-r--r--   3 nwchen24 users     164176 2017-06-21 22:07 /user/nwchen24/HW5_Phase2/part-00010\n",
      "-rw-r--r--   3 nwchen24 users     513866 2017-06-21 22:07 /user/nwchen24/HW5_Phase2/part-00011\n",
      "-rw-r--r--   3 nwchen24 users     181461 2017-06-21 22:07 /user/nwchen24/HW5_Phase2/part-00012\n",
      "-rw-r--r--   3 nwchen24 users      16298 2017-06-21 22:07 /user/nwchen24/HW5_Phase2/part-00013\n",
      "-rw-r--r--   3 nwchen24 users      17806 2017-06-21 22:07 /user/nwchen24/HW5_Phase2/part-00014\n",
      "-rw-r--r--   3 nwchen24 users       5115 2017-06-21 22:07 /user/nwchen24/HW5_Phase2/part-00015\n",
      "The twenty most densely occurring words in the google 5-gram dataset are:\n",
      "dddd\t0.0416666666667\t\n",
      "abandonne\t0.025\t\n",
      "abditis\t0.025\t\n",
      "abit\t0.025\t\n",
      "abracadabras\t0.025\t\n",
      "abrol\t0.025\t\n",
      "absentee's\t0.025\t\n",
      "absolutio\t0.025\t\n",
      "abundanti\t0.025\t\n",
      "accablent\t0.025\t\n",
      "accessorium\t0.025\t\n",
      "accipe\t0.025\t\n",
      "accipiunt\t0.025\t\n",
      "acclimating\t0.025\t\n",
      "accouries\t0.025\t\n",
      "aceras\t0.025\t\n",
      "acerrae\t0.025\t\n",
      "acidifies\t0.025\t\n",
      "aciduric\t0.025\t\n",
      "ackney\t0.025\t\n",
      "cat: Unable to write to output stream.\n",
      "20241\n",
      "The ten least densely occurring words in the google 5-gram dataset are:\n",
      "sci\t0.00032228896787\t\n",
      "endod\t0.000308451573103\t\n",
      "ifthis\t0.000298418382572\t\n",
      "proc\t0.000194103864053\t\n",
      "elibron\t0.000181389443134\t\n",
      "adolesc\t0.000163305299257\t\n",
      "umi's\t0.000158503724838\t\n",
      "superlarge\t0.00015295197308\t\n",
      "acad\t0.000146105621082\t\n",
      "nrlf\t0.000131216375804\t\n",
      "185\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "====================================================================================================\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "The first five words in the remaining partitions are:\n",
      "betteredge\t0.022491349481\t\n",
      "pryer\t0.0224887556222\t\n",
      "siouxes\t0.0224877020379\t\n",
      "dunwoodie\t0.0224825839139\t\n",
      "abateth\t0.0224719101124\t\n",
      "cat: Unable to write to output stream.\n",
      "23904\n",
      "====================================================================================================\n",
      "abatting\t0.02\t\n",
      "abcdefg\t0.02\t\n",
      "abdominus\t0.02\t\n",
      "abeokuta\t0.02\t\n",
      "aberfan\t0.02\t\n",
      "cat: Unable to write to output stream.\n",
      "25617\n",
      "====================================================================================================\n",
      "amethystine\t0.018\t\n",
      "ammar\t0.018\t\n",
      "charteris\t0.018\t\n",
      "cutlers\t0.018\t\n",
      "inedible\t0.018\t\n",
      "cat: Unable to write to output stream.\n",
      "7335\n",
      "====================================================================================================\n",
      "amputating\t0.0175\t\n",
      "auspice\t0.0175\t\n",
      "bourgeoise\t0.0175\t\n",
      "cadenus\t0.0175\t\n",
      "carronade\t0.0175\t\n",
      "cat: Unable to write to output stream.\n",
      "21017\n",
      "====================================================================================================\n",
      "acardiac\t0.016\t\n",
      "acca\t0.016\t\n",
      "achray\t0.016\t\n",
      "acrolein\t0.016\t\n",
      "activization\t0.016\t\n",
      "cat: Unable to write to output stream.\n",
      "15660\n",
      "====================================================================================================\n",
      "abyss's\t0.015\t\n",
      "antiestrogen\t0.015\t\n",
      "anzeiger\t0.015\t\n",
      "aphorismic\t0.015\t\n",
      "archenteron\t0.015\t\n",
      "cat: Unable to write to output stream.\n",
      "26258\n",
      "====================================================================================================\n",
      "argives\t0.0134994807892\t\n",
      "bypasses\t0.0134994807892\t\n",
      "caracci\t0.0134994807892\t\n",
      "fourier's\t0.0134994807892\t\n",
      "neon\t0.0134994807892\t\n",
      "cat: Unable to write to output stream.\n",
      "17422\n",
      "====================================================================================================\n",
      "abdulaziz\t0.0125\t\n",
      "abiquiu\t0.0125\t\n",
      "accomplies\t0.0125\t\n",
      "acidulating\t0.0125\t\n",
      "actualizes\t0.0125\t\n",
      "cat: Unable to write to output stream.\n",
      "47049\n",
      "====================================================================================================\n",
      "aagje\t0.01\t\n",
      "achor\t0.01\t\n",
      "adventive\t0.01\t\n",
      "ahhing\t0.01\t\n",
      "aigina\t0.01\t\n",
      "cat: Unable to write to output stream.\n",
      "31211\n",
      "====================================================================================================\n",
      "abbreviature\t0.008\t\n",
      "aerea\t0.008\t\n",
      "afylum\t0.008\t\n",
      "aile\t0.008\t\n",
      "allpervading\t0.008\t\n",
      "cat: Unable to write to output stream.\n",
      "6236\n",
      "====================================================================================================\n",
      "abbat\t0.0075\t\n",
      "autocatalytic\t0.0075\t\n",
      "borisov\t0.0075\t\n",
      "brundle\t0.0075\t\n",
      "eptifibatide\t0.0075\t\n",
      "cat: Unable to write to output stream.\n",
      "19147\n",
      "====================================================================================================\n",
      "academicus\t0.005\t\n",
      "aftermark\t0.005\t\n",
      "bankimchandra\t0.005\t\n",
      "baronin\t0.005\t\n",
      "birlas\t0.005\t\n",
      "cat: Unable to write to output stream.\n",
      "6788\n",
      "====================================================================================================\n",
      "jayaswal\t0.0025\t\n",
      "lnterior\t0.0025\t\n",
      "metalogicon\t0.0025\t\n",
      "puttenham's\t0.0025\t\n",
      "varela\t0.0025\t\n",
      "606\n",
      "====================================================================================================\n",
      "everytime\t0.002\t\n",
      "collectio\t0.00199900049975\t\n",
      "digastricus\t0.00199600798403\t\n",
      "gesenius's\t0.00199203187251\t\n",
      "kennt\t0.00199203187251\t\n",
      "663\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/nwchen24/HW5_Phase2/\n",
    "\n",
    "print \"The twenty most densely occurring words in the google 5-gram dataset are:\"\n",
    "!hdfs dfs -cat /user/nwchen24/HW5_Phase2/part-00000 | head -n 20\n",
    "!hdfs dfs -cat /user/nwchen24/HW5_Phase2/part-00000 | wc -l\n",
    "\n",
    "print \"The ten least densely occurring words in the google 5-gram dataset are:\"\n",
    "!hdfs dfs -cat /user/nwchen24/HW5_Phase2/part-00015 | tail -n 10\n",
    "!hdfs dfs -cat /user/nwchen24/HW5_Phase2/part-00015 | wc -l\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word density MR stats\n",
    "\n",
    "    altiscale cluster with 176 CPUs and 476 GB of memory\n",
    "    \n",
    "__Step 1:__ \n",
    "\n",
    "    RUNNING for 4 minutes 36 seconds      \n",
    "    Launched map tasks=190   \n",
    "    Launched reduce tasks=16     \n",
    "\n",
    "__Step 2:__  \n",
    "\n",
    "    RUNNING for 2 minutes 43 seconds    \n",
    "    Launched map tasks=16   \n",
    "    Launched reduce tasks=16   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW5.4.1 - D. Distribution of 5-gram sizes (character length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing distribution.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile distribution.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import mrjob\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class distribution(MRJob):\n",
    "    \n",
    "    # START STUDENT CODE 5.4.1.D\n",
    "    \n",
    "    def steps(self):\n",
    "        JOBCONF_STEP1 = {\n",
    "            'mapred.reduce.tasks': 20\n",
    "        }\n",
    "        \n",
    "        return [\n",
    "            MRStep(jobconf=JOBCONF_STEP1,\n",
    "                   mapper=self.mapper,\n",
    "                   reducer=self.reducer\n",
    "                  )\n",
    "        ]\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "\n",
    "        #get the list of tokens in the 5-gram\n",
    "        token_list = line.strip().lower().split()[:-3]\n",
    "\n",
    "        #instantiate counter to hold the five-gram length\n",
    "        five_gram_len = 0\n",
    "\n",
    "        #For each token, increment the five-gram length holder    \n",
    "        for token in token_list:\n",
    "            five_gram_len += len(token)\n",
    "\n",
    "        yield five_gram_len, 1\n",
    "        \n",
    "    def reducer(self, key, five_gram_len_count):\n",
    "        yield key, sum(five_gram_len_count)\n",
    "        \n",
    "    # END STUDENT CODE 5.4.1.D\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    distribution.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__On the test data set:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22, 25991)\n",
      "(23, 23405)\n",
      "(24, 20587)\n",
      "(25, 17257)\n",
      "(26, 14428)\n",
      "(27, 11340)\n",
      "(28, 9061)\n",
      "(29, 6950)\n",
      "(30, 5152)\n",
      "(31, 3871)\n",
      "(32, 2868)\n",
      "(33, 2027)\n",
      "(34, 1516)\n",
      "(35, 1027)\n",
      "(36, 756)\n",
      "(37, 476)\n",
      "(38, 337)\n",
      "(39, 263)\n",
      "(40, 195)\n",
      "(41, 113)\n",
      "(42, 87)\n",
      "(43, 51)\n",
      "(44, 33)\n",
      "(45, 31)\n",
      "(46, 13)\n",
      "(47, 10)\n",
      "(48, 9)\n",
      "(49, 8)\n",
      "(5, 1)\n",
      "(50, 2)\n",
      "(51, 4)\n",
      "(53, 2)\n",
      "(54, 2)\n",
      "(6, 1)\n",
      "(7, 1)\n",
      "(8, 3)\n",
      "(9, 22)\n",
      "(10, 94)\n",
      "(11, 309)\n",
      "(12, 1112)\n",
      "(13, 2859)\n",
      "(14, 5853)\n",
      "(15, 10200)\n",
      "(16, 15479)\n",
      "(17, 20108)\n",
      "(18, 24625)\n",
      "(19, 27333)\n",
      "(20, 28052)\n",
      "(21, 27690)\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "my_file = open('distribution.txt', 'w')\n",
    "from distribution import distribution\n",
    "mr_job = distribution(args=['googlebooks-eng-all-5gram-20090715-0-filtered.txt'])\n",
    "with mr_job.make_runner() as runner:\n",
    "    runner.run()\n",
    "    for line in runner.stream_output():\n",
    "        my_file.write(str(mr_job.parse_output_line(line)))\n",
    "        my_file.write(\"\\n\")\n",
    "        #print mr_job.parse_output_line(line)\n",
    "\n",
    "my_file.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__On the full data set:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Creating temp directory /tmp/distribution.nwchen24.20170622.225255.026127\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/nwchen24/tmp/mrjob/distribution.nwchen24.20170622.225255.026127/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.reduce.tasks: mapreduce.job.reduces\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob6369344251077656418.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 190\n",
      "  number of splits:190\n",
      "  Submitting tokens for job: job_1497906899862_2209\n",
      "  Submitted application application_1497906899862_2209\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1497906899862_2209/\n",
      "  Running job: job_1497906899862_2209\n",
      "  Job job_1497906899862_2209 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 1% reduce 0%\n",
      "   map 2% reduce 0%\n",
      "   map 3% reduce 0%\n",
      "   map 4% reduce 0%\n",
      "   map 5% reduce 0%\n",
      "   map 6% reduce 0%\n",
      "   map 7% reduce 0%\n",
      "   map 8% reduce 0%\n",
      "   map 9% reduce 0%\n",
      "   map 11% reduce 0%\n",
      "   map 12% reduce 0%\n",
      "   map 13% reduce 0%\n",
      "   map 14% reduce 0%\n",
      "   map 17% reduce 0%\n",
      "   map 18% reduce 0%\n",
      "   map 20% reduce 0%\n",
      "   map 21% reduce 0%\n",
      "   map 22% reduce 0%\n",
      "   map 23% reduce 0%\n",
      "   map 24% reduce 0%\n",
      "   map 26% reduce 0%\n",
      "   map 28% reduce 0%\n",
      "   map 30% reduce 0%\n",
      "   map 31% reduce 0%\n",
      "   map 32% reduce 0%\n",
      "   map 33% reduce 0%\n",
      "   map 35% reduce 0%\n",
      "   map 36% reduce 0%\n",
      "   map 38% reduce 0%\n",
      "   map 40% reduce 0%\n",
      "   map 41% reduce 0%\n",
      "   map 42% reduce 0%\n",
      "   map 43% reduce 0%\n",
      "   map 46% reduce 0%\n",
      "   map 47% reduce 0%\n",
      "   map 49% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 51% reduce 0%\n",
      "   map 52% reduce 0%\n",
      "   map 54% reduce 0%\n",
      "   map 56% reduce 0%\n",
      "   map 57% reduce 0%\n",
      "   map 58% reduce 0%\n",
      "   map 59% reduce 0%\n",
      "   map 60% reduce 0%\n",
      "   map 61% reduce 0%\n",
      "   map 62% reduce 0%\n",
      "   map 63% reduce 0%\n",
      "   map 64% reduce 0%\n",
      "   map 65% reduce 0%\n",
      "   map 67% reduce 0%\n",
      "   map 69% reduce 0%\n",
      "   map 70% reduce 0%\n",
      "   map 72% reduce 0%\n",
      "   map 74% reduce 0%\n",
      "   map 75% reduce 0%\n",
      "   map 77% reduce 0%\n",
      "   map 78% reduce 0%\n",
      "   map 79% reduce 0%\n",
      "   map 80% reduce 0%\n",
      "   map 82% reduce 0%\n",
      "   map 83% reduce 0%\n",
      "   map 85% reduce 0%\n",
      "   map 86% reduce 0%\n",
      "   map 88% reduce 0%\n",
      "   map 89% reduce 0%\n",
      "   map 90% reduce 0%\n",
      "   map 91% reduce 0%\n",
      "   map 92% reduce 0%\n",
      "   map 93% reduce 0%\n",
      "   map 95% reduce 0%\n",
      "   map 97% reduce 0%\n",
      "   map 98% reduce 0%\n",
      "   map 99% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 10%\n",
      "   map 100% reduce 17%\n",
      "   map 100% reduce 23%\n",
      "   map 100% reduce 25%\n",
      "   map 100% reduce 27%\n",
      "   map 100% reduce 28%\n",
      "   map 100% reduce 32%\n",
      "   map 100% reduce 33%\n",
      "   map 100% reduce 38%\n",
      "   map 100% reduce 45%\n",
      "   map 100% reduce 48%\n",
      "   map 100% reduce 50%\n",
      "   map 100% reduce 54%\n",
      "   map 100% reduce 55%\n",
      "   map 100% reduce 59%\n",
      "   map 100% reduce 66%\n",
      "   map 100% reduce 78%\n",
      "   map 100% reduce 80%\n",
      "   map 100% reduce 83%\n",
      "   map 100% reduce 88%\n",
      "   map 100% reduce 90%\n",
      "   map 100% reduce 93%\n",
      "   map 100% reduce 95%\n",
      "   map 100% reduce 97%\n",
      "   map 100% reduce 98%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497906899862_2209 completed successfully\n",
      "  Output directory: hdfs:///user/nwchen24/tmp/mrjob/distribution.nwchen24.20170622.225255.026127/output\n",
      "Counters: 52\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2156069116\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=619\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=19337541\n",
      "\t\tFILE: Number of bytes written=66712229\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2156101116\n",
      "\t\tHDFS: Number of bytes written=619\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=630\n",
      "\t\tHDFS: Number of write operations=40\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=190\n",
      "\t\tLaunched reduce tasks=21\n",
      "\t\tOther local map tasks=2\n",
      "\t\tRack-local map tasks=188\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=4235882496\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1364679680\n",
      "\t\tTotal time spent by all map tasks (ms)=2757736\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=8273208\n",
      "\t\tTotal time spent by all reduce tasks (ms)=533078\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2665390\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=2757736\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=533078\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1581890\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=28743\n",
      "\t\tInput split bytes=32000\n",
      "\t\tMap input records=58682266\n",
      "\t\tMap output bytes=293406562\n",
      "\t\tMap output materialized bytes=19428878\n",
      "\t\tMap output records=58682266\n",
      "\t\tMerged Map outputs=3800\n",
      "\t\tPhysical memory (bytes) snapshot=160335761408\n",
      "\t\tReduce input groups=80\n",
      "\t\tReduce input records=58682266\n",
      "\t\tReduce output records=80\n",
      "\t\tReduce shuffle bytes=19428878\n",
      "\t\tShuffled Maps =3800\n",
      "\t\tSpilled Records=117364532\n",
      "\t\tTotal committed heap usage (bytes)=340315865088\n",
      "\t\tVirtual memory (bytes) snapshot=484758368256\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/nwchen24/tmp/mrjob/distribution.nwchen24.20170622.225255.026127/output...\n",
      "Removing HDFS temp directory hdfs:///user/nwchen24/tmp/mrjob/distribution.nwchen24.20170622.225255.026127...\n",
      "Removing temp directory /tmp/distribution.nwchen24.20170622.225255.026127...\n"
     ]
    }
   ],
   "source": [
    "#Hadoop mode for altiscale cluster\n",
    "!python distribution.py -r hadoop hdfs:///user/cendylin/filtered-5Grams > distribution.txt \\\n",
    "    --python-bin=./my_env27_for_hadoop_upload/bin/python \\\n",
    "    --archive=hdfs:///user/nwchen24/virtualenv/my_env27_for_hadoop_upload.zip#my_env27_for_hadoop_upload\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f78c8ce2610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEXCAYAAAD4LtBgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f78c8923090>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot a histogram\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker\n",
    "\n",
    "#create dict and DF that will hold the counts of the number of fivegrams of each length\n",
    "fivegram_lengths_list = []\n",
    "fivegram_counts_list = []\n",
    "\n",
    "\n",
    "#Add the fivegram lengths and the number of fivegrams of that length to a dict\n",
    "with open('distribution.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        line_list = line.strip(\"\\n\").strip(\"(\").strip(\")\").split(\"\\t\")\n",
    "        fivegram_lengths_list.append(int(line_list[0]))\n",
    "        fivegram_counts_list.append(int(line_list[1]))\n",
    "\n",
    "#Create a dataframe\n",
    "fivegram_hist_df = pd.DataFrame({'Fivegram_len':fivegram_lengths_list, 'Count_of_Fivegrams':fivegram_counts_list})\n",
    "\n",
    "#Sort the dataframe by fivegram length\n",
    "fivegram_hist_df = fivegram_hist_df.sort_values('Fivegram_len')\n",
    "\n",
    "#Plot density plot\n",
    "plt.figure()\n",
    "fivegram_hist_df.plot(x = 'Fivegram_len', y = 'Count_of_Fivegrams')\n",
    "plt.title(\"Distribution of Fivegram Character Lenghts\")\n",
    "plt.ylabel(\"Number of Fivegrams\")\n",
    "ax = plt.axes()\n",
    "ax.get_yaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution MRJob stats\n",
    "\n",
    "    altiscale cluster with 176 nodes and 476 GB of memory\n",
    "\n",
    "    RUNNING for 5 min 13 sec \n",
    "    Launched map tasks=190  \n",
    "    Launched reduce tasks=20   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "\n",
    "results_A = []\n",
    "for line in open(\"5.3distributions/part-00000\").readlines():\n",
    "    line = line.strip()\n",
    "    X,Y = line.split(\",\")\n",
    "    results_A.append([int(X),int(Y)])\n",
    "\n",
    "items = (np.array(results_A)[::-1].T)\n",
    "fig = pl.figure(figsize=(17,7))\n",
    "ax = pl.subplot(111)\n",
    "width=0.8\n",
    "ax.bar(range(len(items[0])), items[1], width=width)\n",
    "ax.set_xticks(np.arange(len(items[0])) + width/2)\n",
    "ax.set_xticklabels(items[0], rotation=90)\n",
    "\n",
    "\n",
    "\n",
    "pl.title(\"Distributions of 5 Gram lengths\")\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.4.2 <a name=\"5.4.2\"></a>OPTIONAL Question: log-log plots (PHASE 2)\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "Plot the log-log plot of the frequency distributuion of unigrams. Does it follow power law distribution?\n",
    "\n",
    "For more background see:\n",
    "- https://en.wikipedia.org/wiki/Log%E2%80%93log_plot\n",
    "- https://en.wikipedia.org/wiki/Power_law"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.5  <a name=\"5.5\"></a> Synonym detection over 2Gig of Data with extra Preprocessing steps (HW5.3 plus some preprocessing)   (Phase 2)\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "For the remainder of this assignment please feel free to eliminate stop words from your analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">There is also a corpus of stopwords, that is, high-frequency words like \"the\", \"to\" and \"also\" that we sometimes want to filter out of a document before further processing. Stopwords usually have little lexical content, and their presence in a text fails to distinguish it from other texts. Python's nltk comes with a prebuilt list of stopwords (see below). Using this stopword list filter out these tokens from your analysis and rerun the experiments in 5.5 and disucuss the results of using a stopword list and without using a stopword list.\n",
    "\n",
    "> from nltk.corpus import stopwords\n",
    " stopwords.words('english')\n",
    "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n",
    "'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
    "'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
    "'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
    "'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
    "'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
    "'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n",
    "'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
    "'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here',\n",
    "'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
    "'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so',\n",
    "'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: A large subset of the Google n-grams dataset as was described above\n",
    "\n",
    "For each HW 5.4 -5.5.1 Please unit test and system test your code with respect \n",
    "to SYSTEMS TEST DATASET and show the results. \n",
    "Please compute the expected answer by hand and show your hand calculations for the \n",
    "SYSTEMS TEST DATASET. Then show the results you get with your system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the assignment we will focus on developing methods for detecting synonyms, using the Google 5-grams dataset. At a high level:\n",
    "\n",
    "\n",
    "1. remove stopwords\n",
    "2. get 10,0000 most frequent\n",
    "3. get 1000 (9001-10000) features\n",
    "3. build stripes\n",
    "\n",
    "To accomplish this you must script two main tasks using MRJob:\n",
    "\n",
    "\n",
    "__TASK (1)__ Build stripes for the most frequent 10,000 words using cooccurence information based on\n",
    "the words ranked from 9001,-10,000 as a basis/vocabulary (drop stopword-like terms),\n",
    "and output to a file in your bucket on s3 (bigram analysis, though the words are non-contiguous).\n",
    "\n",
    "\n",
    "__TASK (2)__ Using two (symmetric) comparison methods of your choice \n",
    "(e.g., correlations, distances, similarities), pairwise compare \n",
    "all stripes (vectors), and output to a file in your bucket on s3.\n",
    "\n",
    "#### Design notes for TASK (1)\n",
    "For this task you will be able to modify the pattern we used in HW 3.2\n",
    "(feel free to use the solution as reference). To total the word counts \n",
    "across the 5-grams, output the support from the mappers using the total \n",
    "order inversion pattern:\n",
    "\n",
    "<*word,count>\n",
    "\n",
    "to ensure that the support arrives before the cooccurrences.\n",
    "\n",
    "In addition to ensuring the determination of the total word counts,\n",
    "the mapper must also output co-occurrence counts for the pairs of\n",
    "words inside of each 5-gram. Treat these words as a basket,\n",
    "as we have in HW 3, but count all stripes or pairs in both orders,\n",
    "i.e., count both orderings: (word1,word2), and (word2,word1), to preserve\n",
    "symmetry in our output for TASK (2).\n",
    "\n",
    "#### Design notes for _TASK (2)_\n",
    "For this task you will have to determine a method of comparison.\n",
    "Here are a few that you might consider:\n",
    "\n",
    "- Jaccard\n",
    "- Cosine similarity\n",
    "- Spearman correlation\n",
    "- Euclidean distance\n",
    "- Taxicab (Manhattan) distance\n",
    "- Shortest path graph distance (a graph, because our data is symmetric!)\n",
    "- Pearson correlation\n",
    "- Kendall correlation\n",
    "\n",
    "However, be cautioned that some comparison methods are more difficult to\n",
    "parallelize than others, and do not perform more associations than is necessary, \n",
    "since your choice of association will be symmetric.\n",
    "\n",
    "Please use the inverted index (discussed in live session #5) based pattern to compute the pairwise (term-by-term) similarity matrix. \n",
    "\n",
    "Please report the size of the cluster used and the amount of time it takes to run for the index construction task and for the synonym calculation task. How many pairs need to be processed (HINT: use the posting list length to calculate directly)? Report your  Cluster configuration!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example MR stats: (report times!)\n",
    "    took ~11 minutes on 5 m3.xlarge nodes\n",
    "    Data-local map tasks=188\n",
    "\tLaunched map tasks=190\n",
    "\tLaunched reduce tasks=15\n",
    "\tOther local map tasks=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# START STUDENT CODE 5.5\n",
    "# ADD OR REMOVE CELLS AS NEEDED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Get the top 10,000 Words  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/nwchen24/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure you have the stopwords corpus\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mostFrequentWords_rank.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mostFrequentWords_rank.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import re\n",
    "\n",
    "import mrjob\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.protocol import RawValueProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import nltk\n",
    "from nltk.corpus import stopwords as sw\n",
    "from operator import itemgetter\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class mostFrequentWords(MRJob):\n",
    "    \n",
    "    #****************************************************\n",
    "    # Allows values to be treated as keys\n",
    "    MRJob.SORT_VALUES = True \n",
    "    \n",
    "    # The protocols are critical. It will not work without these:\n",
    "    INPUT_PROTOCOL = RawValueProtocol\n",
    "    INTERNAL_PROTOCOL = RawProtocol\n",
    "    OUTPUT_PROTOCOL = RawProtocol\n",
    "     \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(mostFrequentWords, self).__init__(*args, **kwargs)\n",
    "        self.NUM_REDUCERS = 21\n",
    "    #****************************************************\n",
    "    #************************************************    \n",
    "    #Get word counts\n",
    "\n",
    "    def mapper_init(self):\n",
    "        self.stopwords = []\n",
    "\n",
    "        with open(\"english\") as f:\n",
    "            for line in f:\n",
    "                self.stopwords.append(line.strip(\"\\n\"))\n",
    "        \n",
    "    def mapper_wc(self, _, line):\n",
    "        \n",
    "        #get the list of tokens in the 5-gram\n",
    "        token_list = line.strip().lower().split()[:-3]\n",
    "\n",
    "        #emit each token and a 1   \n",
    "        for token in token_list:\n",
    "            #if token not in sw.words('english'):\n",
    "            if token not in self.stopwords:\n",
    "                yield token, str(1)\n",
    "\n",
    "    def combiner_wc(self, token, count):\n",
    "        #just do local sums before passing to reducer\n",
    "        count_sum = 0\n",
    "        for item in count:\n",
    "            count_sum += int(item)\n",
    "        yield token, str(count_sum)            \n",
    "            \n",
    "    def reducer_wc(self, token, count):\n",
    "        count_sum = 0\n",
    "        for item in count:\n",
    "            count_sum += int(item)\n",
    "        yield token, str(count_sum)\n",
    "\n",
    "    #********************************************************\n",
    "    #Rank by word count and only emit top 10k\n",
    "    \n",
    "    def mapper_rank(self, word, count):\n",
    "        yield word, count\n",
    "\n",
    "    def reducer_rank_init(self):\n",
    "        self.rank_counter = 0\n",
    "        \n",
    "    def reducer_rank(self, word, count):\n",
    "        self.rank_counter += 1\n",
    "        \n",
    "        if self.rank_counter <= 10000:\n",
    "            for count_out in count:\n",
    "                yield word, str(count_out)\n",
    "        else:\n",
    "            pass\n",
    "       \n",
    "    #********************************************************\n",
    "    #Steps\n",
    "    def steps(self):\n",
    "\n",
    "        JOBCONF_STEP1 = {\n",
    "            'stream.num.map.output.key.fields':3,\n",
    "            'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'stream.map.output.field.separator':\"\\t\",\n",
    "            'mapreduce.partition.keycomparator.options':'-k2,2nr',\n",
    "            'mapred.reduce.tasks': 1,\n",
    "            'partitioner':'org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner'\n",
    "        }\n",
    "        \n",
    "        JOBCONF_STEP3 = {\n",
    "            'mapred.map.tasks': 1,\n",
    "            'mapred.reduce.tasks': 1\n",
    "        }\n",
    "        \n",
    "        return [\n",
    "            MRStep(mapper_init=self.mapper_init,\n",
    "                   mapper=self.mapper_wc,\n",
    "                   combiner=self.combiner_wc,\n",
    "                   reducer=self.reducer_wc),\n",
    "            MRStep(jobconf=JOBCONF_STEP1,\n",
    "                   mapper=self.mapper_rank,\n",
    "                   reducer_init=self.reducer_rank_init,\n",
    "                   reducer=self.reducer_rank)\n",
    "        ]\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    mostFrequentWords.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/06/22 03:12:30 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 5760 minutes, Emptier interval = 360 minutes.\n",
      "Moved: 'hdfs://nn-ia.s3s.altiscale.com:8020/user/nwchen24/HW5_Phase2' to trash at: hdfs://nn-ia.s3s.altiscale.com:8020/user/nwchen24/.Trash/Current\n",
      "No configs found; falling back on auto-configuration\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Creating temp directory /tmp/mostFrequentWords_rank.nwchen24.20170622.031231.492177\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/nwchen24/tmp/mrjob/mostFrequentWords_rank.nwchen24.20170622.031231.492177/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob8203743565404364122.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 190\n",
      "  number of splits:190\n",
      "  Submitting tokens for job: job_1497906899862_1615\n",
      "  Submitted application application_1497906899862_1615\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1497906899862_1615/\n",
      "  Running job: job_1497906899862_1615\n",
      "  Job job_1497906899862_1615 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 1% reduce 0%\n",
      "   map 2% reduce 0%\n",
      "   map 3% reduce 0%\n",
      "   map 4% reduce 0%\n",
      "   map 5% reduce 0%\n",
      "   map 6% reduce 0%\n",
      "   map 7% reduce 0%\n",
      "   map 8% reduce 0%\n",
      "   map 9% reduce 0%\n",
      "   map 10% reduce 0%\n",
      "   map 11% reduce 0%\n",
      "   map 12% reduce 0%\n",
      "   map 13% reduce 0%\n",
      "   map 14% reduce 0%\n",
      "   map 15% reduce 0%\n",
      "   map 16% reduce 0%\n",
      "   map 17% reduce 0%\n",
      "   map 18% reduce 0%\n",
      "   map 19% reduce 0%\n",
      "   map 20% reduce 0%\n",
      "   map 21% reduce 0%\n",
      "   map 22% reduce 0%\n",
      "   map 23% reduce 0%\n",
      "   map 24% reduce 0%\n",
      "   map 25% reduce 0%\n",
      "   map 26% reduce 0%\n",
      "   map 27% reduce 0%\n",
      "   map 28% reduce 0%\n",
      "   map 29% reduce 0%\n",
      "   map 30% reduce 0%\n",
      "   map 31% reduce 0%\n",
      "   map 32% reduce 0%\n",
      "   map 33% reduce 0%\n",
      "   map 34% reduce 0%\n",
      "   map 35% reduce 0%\n",
      "   map 36% reduce 0%\n",
      "   map 37% reduce 0%\n",
      "   map 38% reduce 0%\n",
      "   map 39% reduce 0%\n",
      "   map 40% reduce 0%\n",
      "   map 41% reduce 0%\n",
      "   map 42% reduce 0%\n",
      "   map 43% reduce 0%\n",
      "   map 44% reduce 0%\n",
      "   map 45% reduce 0%\n",
      "   map 46% reduce 0%\n",
      "   map 47% reduce 0%\n",
      "   map 48% reduce 0%\n",
      "   map 49% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 51% reduce 0%\n",
      "   map 52% reduce 0%\n",
      "   map 53% reduce 0%\n",
      "   map 54% reduce 0%\n",
      "   map 55% reduce 0%\n",
      "   map 56% reduce 0%\n",
      "   map 57% reduce 0%\n",
      "   map 58% reduce 0%\n",
      "   map 59% reduce 0%\n",
      "   map 60% reduce 0%\n",
      "   map 61% reduce 0%\n",
      "   map 62% reduce 0%\n",
      "   map 63% reduce 0%\n",
      "   map 64% reduce 0%\n",
      "   map 65% reduce 0%\n",
      "   map 66% reduce 0%\n",
      "   map 67% reduce 0%\n",
      "   map 68% reduce 0%\n",
      "   map 69% reduce 0%\n",
      "   map 70% reduce 0%\n",
      "   map 71% reduce 0%\n",
      "   map 72% reduce 0%\n",
      "   map 73% reduce 0%\n",
      "   map 74% reduce 0%\n",
      "   map 75% reduce 0%\n",
      "   map 76% reduce 0%\n",
      "   map 77% reduce 0%\n",
      "   map 78% reduce 0%\n",
      "   map 79% reduce 0%\n",
      "   map 80% reduce 0%\n",
      "   map 81% reduce 0%\n",
      "   map 82% reduce 0%\n",
      "   map 83% reduce 0%\n",
      "   map 84% reduce 0%\n",
      "   map 85% reduce 0%\n",
      "   map 86% reduce 0%\n",
      "   map 87% reduce 0%\n",
      "   map 88% reduce 0%\n",
      "   map 89% reduce 0%\n",
      "   map 90% reduce 0%\n",
      "   map 91% reduce 0%\n",
      "   map 92% reduce 0%\n",
      "   map 93% reduce 0%\n",
      "   map 94% reduce 0%\n",
      "   map 95% reduce 0%\n",
      "   map 96% reduce 0%\n",
      "   map 97% reduce 0%\n",
      "   map 98% reduce 0%\n",
      "   map 99% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 48%\n",
      "   map 100% reduce 58%\n",
      "   map 100% reduce 67%\n",
      "   map 100% reduce 68%\n",
      "   map 100% reduce 70%\n",
      "   map 100% reduce 71%\n",
      "   map 100% reduce 73%\n",
      "   map 100% reduce 74%\n",
      "   map 100% reduce 76%\n",
      "   map 100% reduce 78%\n",
      "   map 100% reduce 79%\n",
      "   map 100% reduce 81%\n",
      "   map 100% reduce 82%\n",
      "   map 100% reduce 84%\n",
      "   map 100% reduce 85%\n",
      "   map 100% reduce 87%\n",
      "   map 100% reduce 89%\n",
      "   map 100% reduce 90%\n",
      "   map 100% reduce 92%\n",
      "   map 100% reduce 93%\n",
      "   map 100% reduce 95%\n",
      "   map 100% reduce 96%\n",
      "   map 100% reduce 98%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497906899862_1615 completed successfully\n",
      "  Output directory: hdfs:///user/nwchen24/tmp/mrjob/mostFrequentWords_rank.nwchen24.20170622.031231.492177/step-output/0000\n",
      "Counters: 51\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2156069116\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=3167755\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=15792903\n",
      "\t\tFILE: Number of bytes written=94987526\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2156101116\n",
      "\t\tHDFS: Number of bytes written=3167755\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=573\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=2\n",
      "\t\tLaunched map tasks=192\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tOther local map tasks=2\n",
      "\t\tRack-local map tasks=190\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=8904794112\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=241873920\n",
      "\t\tTotal time spent by all map tasks (ms)=5797392\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=17392176\n",
      "\t\tTotal time spent by all reduce tasks (ms)=94482\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=472410\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=5797392\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=94482\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=4104560\n",
      "\t\tCombine input records=128113253\n",
      "\t\tCombine output records=6794902\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=37634\n",
      "\t\tInput split bytes=32000\n",
      "\t\tMap input records=58682266\n",
      "\t\tMap output bytes=1328397506\n",
      "\t\tMap output materialized bytes=53583409\n",
      "\t\tMap output records=128113253\n",
      "\t\tMerged Map outputs=190\n",
      "\t\tPhysical memory (bytes) snapshot=154325381120\n",
      "\t\tReduce input groups=269186\n",
      "\t\tReduce input records=6794902\n",
      "\t\tReduce output records=269186\n",
      "\t\tReduce shuffle bytes=53583409\n",
      "\t\tShuffled Maps =190\n",
      "\t\tSpilled Records=13589804\n",
      "\t\tTotal committed heap usage (bytes)=299992875008\n",
      "\t\tVirtual memory (bytes) snapshot=421091549184\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.reduce.tasks: mapreduce.job.reduces\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob2190380604092620009.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1497906899862_1627\n",
      "  Submitted application application_1497906899862_1627\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1497906899862_1627/\n",
      "  Running job: job_1497906899862_1627\n",
      "  Job job_1497906899862_1627 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497906899862_1627 completed successfully\n",
      "  Output directory: hdfs:///user/nwchen24/tmp/mrjob/mostFrequentWords_rank.nwchen24.20170622.031231.492177/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3287814\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=146201\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=2363354\n",
      "\t\tFILE: Number of bytes written=5129653\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3288204\n",
      "\t\tHDFS: Number of bytes written=146201\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=38372352\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=23769600\n",
      "\t\tTotal time spent by all map tasks (ms)=24982\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=74946\n",
      "\t\tTotal time spent by all reduce tasks (ms)=9285\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=46425\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=24982\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=9285\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=8870\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=379\n",
      "\t\tInput split bytes=390\n",
      "\t\tMap input records=269186\n",
      "\t\tMap output bytes=3436941\n",
      "\t\tMap output materialized bytes=2363154\n",
      "\t\tMap output records=269186\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1952825344\n",
      "\t\tReduce input groups=269186\n",
      "\t\tReduce input records=269186\n",
      "\t\tReduce output records=10000\n",
      "\t\tReduce shuffle bytes=2363154\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=538372\n",
      "\t\tTotal committed heap usage (bytes)=5280104448\n",
      "\t\tVirtual memory (bytes) snapshot=7740182528\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/nwchen24/tmp/mrjob/mostFrequentWords_rank.nwchen24.20170622.031231.492177/output...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing HDFS temp directory hdfs:///user/nwchen24/tmp/mrjob/mostFrequentWords_rank.nwchen24.20170622.031231.492177...\n",
      "Removing temp directory /tmp/mostFrequentWords_rank.nwchen24.20170622.031231.492177...\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/nwchen24/HW5_Phase2\n",
    " \n",
    "#Hadoop mode for altiscale cluster - ran 4min57sec\n",
    "!python mostFrequentWords_rank.py -r hadoop hdfs:///user/cendylin/filtered-5Grams > google_5grams_top10k_words \\\n",
    "    --file=/home/nwchen24/nltk_data/corpora/stopwords/english \\\n",
    "    --python-bin=./myenv_for_hadoop2/bin/python \\\n",
    "    --archive=hdfs:///user/nwchen24/virtualenv/myenv_for_hadoop2.zip#myenv_for_hadoop2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Build Stripes  \n",
    "\n",
    "Build stripes for all words in n-grams, but the only entries in the stripes are the 9k - 10k words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting buildStripes.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile buildStripes.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __future__ import division\n",
    "import re\n",
    "import mrjob\n",
    "import json\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import itertools\n",
    "import collections as cl\n",
    "import ast\n",
    "\n",
    "class MRbuildStripes(MRJob):\n",
    "  \n",
    "    #START SUDENT CODE531_STRIPES\n",
    "    \n",
    "    #OUTPUT_PROTOCOL = JSONValueProtocol\n",
    "    \n",
    "    def steps(self):\n",
    "        \n",
    "        JOBCONF_STEP = {\n",
    "            'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'stream.map.output.field.separator':'\\t',    \n",
    "            'mapreduce.partition.keycomparator.options': '-k1,1',\n",
    "            'mapreduce.job.reduces': '30'\n",
    "        }\n",
    "        \n",
    "        return [\n",
    "            MRStep(jobconf=JOBCONF_STEP,\n",
    "                   mapper_init=self.mapper_init,\n",
    "                   mapper=self.mapper,\n",
    "                   reducer=self.reducer\n",
    "                  )\n",
    "        ]\n",
    "    \n",
    "    def mapper_init(self):\n",
    "        self.increment_counter('group', 'Num_mapperInit_calls', 1)\n",
    "             \n",
    "        counter = 0\n",
    "        \n",
    "        self.words_10k = open(\"google_5grams_top10k_words\").readlines()\n",
    "        self.words_10k_list = []\n",
    "        self.words_9k_10k_list = []\n",
    "        \n",
    "        for line in self.words_10k:\n",
    "            #Add each word to the list\n",
    "            self.words_10k_list.append(line.split()[0].strip(\"\\\"\"))\n",
    " \n",
    "            if counter > 9000:\n",
    "                self.words_9k_10k_list.append(line.split()[0].strip(\"\\\"\"))\n",
    "            \n",
    "            counter += 1\n",
    "        \n",
    "        #convert the 10k words to set\n",
    "        self.words_10k_set = set(self.words_10k_list)\n",
    "        \n",
    "        #yield self.words_9k_10k_list,None\n",
    "    def mapper(self, _, line):\n",
    "        \n",
    "        #instantiate dict to hold stripes\n",
    "        stripes_dict = {}\n",
    "        \n",
    "        #parse the co-occurrence count from the line\n",
    "        co_occur_count = line.strip().split()[-3]\n",
    "\n",
    "        #Get list of words to build stripes for\n",
    "        build_stripe_list = set(line.strip().lower().split()[:-3]) & self.words_10k_set\n",
    "        \n",
    "        if len(build_stripe_list) < 2:\n",
    "            pass\n",
    "        \n",
    "        else:        \n",
    "            for subset in itertools.permutations(sorted(build_stripe_list), 2):\n",
    "                if subset[1].strip(\"\\\"\") not in self.words_9k_10k_list:\n",
    "                    pass #could also use \"continue\"... not much difference in this particular scenario though\n",
    "\n",
    "                else:\n",
    "                    if subset[0].strip(\"\\\"\") not in stripes_dict.keys():\n",
    "                        stripes_dict[subset[0].strip(\"\\\"\")] = {}\n",
    "                        stripes_dict[subset[0].strip(\"\\\"\")][subset[1].strip(\"\\\"\")] = int(co_occur_count)\n",
    "\n",
    "                    elif subset[1].strip(\"\\\"\") not in stripes_dict[subset[0].strip(\"\\\"\")]:\n",
    "                        stripes_dict[subset[0].strip(\"\\\"\")][subset[1].strip(\"\\\"\")] = int(co_occur_count)\n",
    "\n",
    "                    else:\n",
    "                        stripes_dict[subset[0].strip(\"\\\"\")][subset[1].strip(\"\\\"\")] += int(co_occur_count)\n",
    "\n",
    "            for key in stripes_dict:\n",
    "                yield key, stripes_dict[key]\n",
    "\n",
    "    def reducer(self,key,stripe):\n",
    "        \n",
    "        #instantiate counter to hold combined stripes\n",
    "        cur_counter = cl.Counter()\n",
    "                \n",
    "        for stripe_dict in stripe:\n",
    "            cur_key = key\n",
    "            \n",
    "            #load the stripe into a dict\n",
    "            stripe_dict_to_add = stripe_dict\n",
    "            \n",
    "            #update with each stripe\n",
    "            cur_counter.update(stripe_dict_to_add)\n",
    "\n",
    "        #output the key and the counter with the sum of all co-occurrences for that key\n",
    "        yield key, cur_counter\n",
    "\n",
    "    #END SUDENT CODE531_STRIPES\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRbuildStripes.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/06/22 04:23:25 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 5760 minutes, Emptier interval = 360 minutes.\n",
      "Moved: 'hdfs://nn-ia.s3s.altiscale.com:8020/user/nwchen24/HW5_Phase2/stripes' to trash at: hdfs://nn-ia.s3s.altiscale.com:8020/user/nwchen24/.Trash/Current\n",
      "No configs found; falling back on auto-configuration\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Creating temp directory /tmp/buildStripes.nwchen24.20170622.042326.831048\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/nwchen24/tmp/mrjob/buildStripes.nwchen24.20170622.042326.831048/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob8378315993788984844.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 190\n",
      "  number of splits:190\n",
      "  Submitting tokens for job: job_1497906899862_1707\n",
      "  Submitted application application_1497906899862_1707\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1497906899862_1707/\n",
      "  Running job: job_1497906899862_1707\n",
      "  Job job_1497906899862_1707 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 1% reduce 0%\n",
      "   map 2% reduce 0%\n",
      "   map 3% reduce 0%\n",
      "   map 4% reduce 0%\n",
      "   map 5% reduce 0%\n",
      "   map 6% reduce 0%\n",
      "   map 7% reduce 0%\n",
      "   map 8% reduce 0%\n",
      "   map 9% reduce 0%\n",
      "   map 10% reduce 0%\n",
      "   map 11% reduce 0%\n",
      "   map 12% reduce 0%\n",
      "   map 13% reduce 0%\n",
      "   map 15% reduce 0%\n",
      "   map 16% reduce 0%\n",
      "   map 20% reduce 0%\n",
      "   map 21% reduce 0%\n",
      "   map 22% reduce 0%\n",
      "   map 23% reduce 0%\n",
      "   map 24% reduce 0%\n",
      "   map 25% reduce 0%\n",
      "   map 26% reduce 0%\n",
      "   map 27% reduce 0%\n",
      "   map 28% reduce 0%\n",
      "   map 29% reduce 0%\n",
      "   map 30% reduce 0%\n",
      "   map 31% reduce 0%\n",
      "   map 32% reduce 0%\n",
      "   map 33% reduce 0%\n",
      "   map 34% reduce 0%\n",
      "   map 35% reduce 0%\n",
      "   map 36% reduce 0%\n",
      "   map 37% reduce 0%\n",
      "   map 39% reduce 0%\n",
      "   map 40% reduce 0%\n",
      "   map 42% reduce 0%\n",
      "   map 43% reduce 0%\n",
      "   map 45% reduce 0%\n",
      "   map 46% reduce 0%\n",
      "   map 48% reduce 0%\n",
      "   map 49% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 51% reduce 0%\n",
      "   map 53% reduce 0%\n",
      "   map 55% reduce 0%\n",
      "   map 57% reduce 0%\n",
      "   map 59% reduce 0%\n",
      "   map 61% reduce 0%\n",
      "   map 62% reduce 0%\n",
      "   map 63% reduce 0%\n",
      "   map 64% reduce 0%\n",
      "   map 65% reduce 0%\n",
      "   map 66% reduce 0%\n",
      "   map 68% reduce 0%\n",
      "   map 69% reduce 0%\n",
      "   map 70% reduce 0%\n",
      "   map 71% reduce 0%\n",
      "   map 73% reduce 0%\n",
      "   map 74% reduce 0%\n",
      "   map 75% reduce 0%\n",
      "   map 76% reduce 0%\n",
      "   map 77% reduce 0%\n",
      "   map 78% reduce 0%\n",
      "   map 79% reduce 0%\n",
      "   map 80% reduce 0%\n",
      "   map 81% reduce 0%\n",
      "   map 82% reduce 0%\n",
      "   map 84% reduce 0%\n",
      "   map 85% reduce 0%\n",
      "   map 86% reduce 0%\n",
      "   map 87% reduce 0%\n",
      "   map 88% reduce 0%\n",
      "   map 89% reduce 0%\n",
      "   map 90% reduce 0%\n",
      "   map 91% reduce 0%\n",
      "   map 92% reduce 0%\n",
      "   map 93% reduce 0%\n",
      "   map 94% reduce 0%\n",
      "   map 95% reduce 0%\n",
      "   map 96% reduce 0%\n",
      "   map 97% reduce 0%\n",
      "   map 98% reduce 0%\n",
      "   map 99% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 7%\n",
      "   map 100% reduce 10%\n",
      "   map 100% reduce 23%\n",
      "   map 100% reduce 43%\n",
      "   map 100% reduce 63%\n",
      "   map 100% reduce 87%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497906899862_1707 completed successfully\n",
      "  Output directory: hdfs:///user/nwchen24/HW5_Phase2/stripes\n",
      "Counters: 53\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2156069116\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=8141963\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=15222665\n",
      "\t\tFILE: Number of bytes written=74035512\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2156101116\n",
      "\t\tHDFS: Number of bytes written=8141963\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=660\n",
      "\t\tHDFS: Number of write operations=60\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=4\n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=194\n",
      "\t\tLaunched reduce tasks=30\n",
      "\t\tOther local map tasks=2\n",
      "\t\tRack-local map tasks=192\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=16689185280\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=536739840\n",
      "\t\tTotal time spent by all map tasks (ms)=10865355\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=32596065\n",
      "\t\tTotal time spent by all reduce tasks (ms)=209664\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1048320\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=10865355\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=209664\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=5213870\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=56595\n",
      "\t\tInput split bytes=32000\n",
      "\t\tMap input records=58682266\n",
      "\t\tMap output bytes=40216280\n",
      "\t\tMap output materialized bytes=29282047\n",
      "\t\tMap output records=1583251\n",
      "\t\tMerged Map outputs=5700\n",
      "\t\tPhysical memory (bytes) snapshot=163795316736\n",
      "\t\tReduce input groups=9998\n",
      "\t\tReduce input records=1583251\n",
      "\t\tReduce output records=9998\n",
      "\t\tReduce shuffle bytes=29282047\n",
      "\t\tShuffled Maps =5700\n",
      "\t\tSpilled Records=3166502\n",
      "\t\tTotal committed heap usage (bytes)=361748234240\n",
      "\t\tVirtual memory (bytes) snapshot=517998616576\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tgroup\n",
      "\t\tNum_mapperInit_calls=190\n",
      "Removing HDFS temp directory hdfs:///user/nwchen24/tmp/mrjob/buildStripes.nwchen24.20170622.042326.831048...\n",
      "Removing temp directory /tmp/buildStripes.nwchen24.20170622.042326.831048...\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/nwchen24/HW5_Phase2/stripes\n",
    "\n",
    "#Hadoop to try in altiscale. Ran 2min53sec.\n",
    "!python buildStripes.py -r hadoop --file=google_5grams_top10k_words hdfs:///user/cendylin/filtered-5Grams \\\n",
    "    --output-dir='/user/nwchen24/HW5_Phase2/stripes'\\\n",
    "    --no-output\\\n",
    "    --python-bin=./my_env27_for_hadoop_upload/bin/python \\\n",
    "    --archive=hdfs:///user/nwchen24/virtualenv/my_env27_for_hadoop_upload.zip#my_env27_for_hadoop_upload    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create Inverted Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting invertedIndex.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile invertedIndex.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __future__ import division\n",
    "import collections as cl\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import itertools\n",
    "import mrjob\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import sys\n",
    "\n",
    "class MRinvertedIndex(MRJob):\n",
    "    #START SUDENT CODE531_INV_INDEX\n",
    "    \n",
    "    #OUTPUT_PROTOCOL = JSONValueProtocol\n",
    "\n",
    "    \n",
    "    def steps(self):\n",
    "        \n",
    "        JOBCONF_STEP = {\n",
    "            'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'stream.map.output.field.separator':'\\t',    \n",
    "            'mapreduce.partition.keycomparator.options': '-k1,1',\n",
    "            'mapreduce.job.reduces': '30'\n",
    "        }\n",
    "        \n",
    "        return [\n",
    "            MRStep(jobconf=JOBCONF_STEP,\n",
    "                   mapper=self.mapper,\n",
    "                   reducer=self.reducer\n",
    "                  )\n",
    "        ]\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        \n",
    "        #Read the key and the stripe from the input\n",
    "        key_stripe_list = line.strip(\"\\n\").split(\"\\t\")\n",
    "        \n",
    "        key = json.loads(key_stripe_list[0])\n",
    "        stripe = json.loads(key_stripe_list[1])\n",
    "        \n",
    "        #get length of the stripe\n",
    "        stripe_len = len(stripe)\n",
    "        \n",
    "        #create dict that will hold doc title and the stripe length\n",
    "        stripe_len_dict = {key:stripe_len}\n",
    "        \n",
    "        for term in stripe:\n",
    "            yield term, stripe_len_dict\n",
    "  \n",
    "    def reducer(self, key, stripe):\n",
    "\n",
    "        #instantiate a dict where we will combine stripe_len_dicts emitted from mapper\n",
    "        stripe_len_dict_combined = {}\n",
    "        \n",
    "        #Read the incremental stripe length dit from the mapper output\n",
    "        for stripe_len_dict in stripe:\n",
    "            \n",
    "            #update the dict above with each incremental stripe length dict\n",
    "            stripe_len_dict_combined.update(stripe_len_dict)\n",
    "                    \n",
    "        yield key, stripe_len_dict_combined        \n",
    "        \n",
    "        #END SUDENT CODE531_INV_INDEX\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    MRinvertedIndex.run() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/nwchen24/HW5_Phase2/inv_index': No such file or directory\n",
      "No configs found; falling back on auto-configuration\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Creating temp directory /tmp/invertedIndex.nwchen24.20170622.191725.861309\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/nwchen24/tmp/mrjob/invertedIndex.nwchen24.20170622.191725.861309/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob4844616952546228502.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 30\n",
      "  number of splits:30\n",
      "  Submitting tokens for job: job_1497906899862_2120\n",
      "  Submitted application application_1497906899862_2120\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1497906899862_2120/\n",
      "  Running job: job_1497906899862_2120\n",
      "  Job job_1497906899862_2120 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 7% reduce 0%\n",
      "   map 20% reduce 0%\n",
      "   map 27% reduce 0%\n",
      "   map 28% reduce 0%\n",
      "   map 30% reduce 0%\n",
      "   map 34% reduce 0%\n",
      "   map 41% reduce 0%\n",
      "   map 42% reduce 0%\n",
      "   map 45% reduce 0%\n",
      "   map 65% reduce 0%\n",
      "   map 73% reduce 0%\n",
      "   map 77% reduce 0%\n",
      "   map 79% reduce 0%\n",
      "   map 82% reduce 0%\n",
      "   map 86% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 2%\n",
      "   map 100% reduce 7%\n",
      "   map 100% reduce 14%\n",
      "   map 100% reduce 17%\n",
      "   map 100% reduce 23%\n",
      "   map 100% reduce 38%\n",
      "   map 100% reduce 42%\n",
      "   map 100% reduce 44%\n",
      "   map 100% reduce 46%\n",
      "   map 100% reduce 47%\n",
      "   map 100% reduce 54%\n",
      "   map 100% reduce 62%\n",
      "   map 100% reduce 64%\n",
      "   map 100% reduce 71%\n",
      "   map 100% reduce 74%\n",
      "   map 100% reduce 81%\n",
      "   map 100% reduce 89%\n",
      "   map 100% reduce 96%\n",
      "   map 100% reduce 97%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497906899862_2120 completed successfully\n",
      "  Output directory: hdfs:///user/nwchen24/HW5_Phase2/inv_index\n",
      "Counters: 51\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=8141963\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=7252572\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=6288887\n",
      "\t\tFILE: Number of bytes written=20232842\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=8145893\n",
      "\t\tHDFS: Number of bytes written=7252572\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=180\n",
      "\t\tHDFS: Number of write operations=60\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=2\n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=30\n",
      "\t\tLaunched reduce tasks=31\n",
      "\t\tRack-local map tasks=30\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=1699424256\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1210577920\n",
      "\t\tTotal time spent by all map tasks (ms)=1106396\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=3319188\n",
      "\t\tTotal time spent by all reduce tasks (ms)=472882\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2364410\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=1106396\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=472882\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=212010\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=8274\n",
      "\t\tInput split bytes=3930\n",
      "\t\tMap input records=9998\n",
      "\t\tMap output bytes=14405740\n",
      "\t\tMap output materialized bytes=5936465\n",
      "\t\tMap output records=554034\n",
      "\t\tMerged Map outputs=900\n",
      "\t\tPhysical memory (bytes) snapshot=33978441728\n",
      "\t\tReduce input groups=999\n",
      "\t\tReduce input records=554034\n",
      "\t\tReduce output records=999\n",
      "\t\tReduce shuffle bytes=5936465\n",
      "\t\tShuffled Maps =900\n",
      "\t\tSpilled Records=1108068\n",
      "\t\tTotal committed heap usage (bytes)=109534248960\n",
      "\t\tVirtual memory (bytes) snapshot=166659899392\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Removing HDFS temp directory hdfs:///user/nwchen24/tmp/mrjob/invertedIndex.nwchen24.20170622.191725.861309...\n",
      "Removing temp directory /tmp/invertedIndex.nwchen24.20170622.191725.861309...\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/nwchen24/HW5_Phase2/inv_index\n",
    "\n",
    "#Hadoop to try in altiscale. Ran 1min49sec\n",
    "!python invertedIndex.py -r hadoop hdfs:///user/nwchen24/HW5_Phase2/stripes \\\n",
    "    --output-dir='/user/nwchen24/HW5_Phase2/inv_index'\\\n",
    "    --no-output\\\n",
    "    --python-bin=./my_env27_for_hadoop_upload/bin/python \\\n",
    "    --archive=hdfs:///user/nwchen24/virtualenv/my_env27_for_hadoop_upload.zip#my_env27_for_hadoop_upload\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Calculate Similarities  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting similarity.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile similarity.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __future__ import division\n",
    "import re\n",
    "import mrjob\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from mrjob.protocol import RawValueProtocol\n",
    "from mrjob.protocol import RawProtocol\n",
    "from operator import itemgetter\n",
    "import numpy as np\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "import collections\n",
    "import ast\n",
    "\n",
    "\n",
    "class MRsimilarity(MRJob):\n",
    "             \n",
    "    #****************************************************\n",
    "    # Allows values to be treated as keys\n",
    "    MRJob.SORT_VALUES = True \n",
    "    \n",
    "    #The protocols are critical. Total sort will not work without these:\n",
    "    #Raw protocols require emitting strings from each phase\n",
    "    INPUT_PROTOCOL = RawValueProtocol\n",
    "    INTERNAL_PROTOCOL = RawProtocol\n",
    "    OUTPUT_PROTOCOL = RawProtocol\n",
    "     \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(MRsimilarity, self).__init__(*args, **kwargs)\n",
    "        self.NUM_REDUCERS = 50\n",
    "    #****************************************************\n",
    "\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        \n",
    "        #Read the key and the stripe from the input\n",
    "        key_index_list = line.translate(None,\"\\n\").split(\"\\t\")\n",
    "        \n",
    "        key = json.loads(key_index_list[0])\n",
    "        inv_index = json.loads(key_index_list[1])\n",
    "        \n",
    "        for subset in itertools.permutations(sorted(set(inv_index.keys())), 2):\n",
    "            \n",
    "            #get the lengths of each item that we want to output\n",
    "            item_1_len = inv_index[subset[0]]\n",
    "            item_2_len = inv_index[subset[1]]\n",
    "            \n",
    "            #create a two item dict to hold the item names and their lengths\n",
    "            item_len_dict = {}\n",
    "            item_len_dict[subset[0]] = item_1_len\n",
    "            item_len_dict[subset[1]] = item_2_len\n",
    "            \n",
    "            yield subset[0]+\"|\"+subset[1], str(1)+\"\\t\"+str(item_len_dict)\n",
    "  \n",
    "    def reducer(self, key, stripe):\n",
    "\n",
    "        #instantiate object to hold the number of times the terms / documents intersect\n",
    "        items_intersect = 0\n",
    "        cosine_sim_product = 0\n",
    "        \n",
    "        #Split the compound key to get each word individually\n",
    "        key_list = key.split(\"|\")\n",
    "        \n",
    "        #the reducer gets output from the mapper in chunks grouped by key\n",
    "        for mapper_out in stripe:\n",
    "            \n",
    "            #Parse the mapper_out which is a string of count and the length dict separated by a tab\n",
    "            mapper_out_list = mapper_out.split(\"\\t\")\n",
    "            \n",
    "            #interpret the dict output from the mapper as a string\n",
    "            item_len_dict = ast.literal_eval(mapper_out_list[1])\n",
    "            #get the counter output from the mapper\n",
    "            n = int(mapper_out_list[0])\n",
    "            \n",
    "            #get the length of item 1 and item 2\n",
    "            item_1_len = item_len_dict[key_list[0]]\n",
    "            item_2_len = item_len_dict[key_list[1]]\n",
    "            \n",
    "            #increment the intersect count\n",
    "            items_intersect = items_intersect + n\n",
    "            \n",
    "            cosine_sim_product = cosine_sim_product + (1/np.sqrt(item_1_len)) * (1/np.sqrt(item_2_len))\n",
    "            \n",
    "        #calculate jacard similarity score\n",
    "        jacard_sim_score = float(items_intersect)/ float(item_1_len + item_2_len - items_intersect)\n",
    "        \n",
    "        #Record cosine similarity score before moving on to the next chunk\n",
    "        cosine_sim_score = cosine_sim_product\n",
    "        \n",
    "        #average sim score\n",
    "        avg_sim_score = (jacard_sim_score + cosine_sim_score) / 2\n",
    "        \n",
    "        yield key, str(avg_sim_score)+\"\\t\"+str(jacard_sim_score)+\"\\t\"+str(cosine_sim_score)   \n",
    "\n",
    "    \n",
    "    #*********************************************************************\n",
    "    #Sort the partitions in descending order of average sim score\n",
    "    #Implement total sort with ordered partitions\n",
    "    def mapper_partitioner_init(self):\n",
    "        \n",
    "        #Function to hash keys\n",
    "        def makeKeyHash(key, num_reducers):\n",
    "            byteof = lambda char: int(format(ord(char), 'b'), 2)\n",
    "            current_hash = 0\n",
    "            for c in key:\n",
    "                current_hash = (current_hash * 31 + byteof(c))\n",
    "            return current_hash % num_reducers\n",
    "        \n",
    "        # get the keys: printable ascii characters, starting with 'A'\n",
    "        keys = [str(unichr(i)) for i in range(65,65+self.NUM_REDUCERS)]\n",
    "        partitions = []\n",
    "        \n",
    "        #Hash each key and link the key to its hashed int value\n",
    "        for key in keys:\n",
    "            partitions.append([key, makeKeyHash(key, self.NUM_REDUCERS)])\n",
    "\n",
    "        #This step re-sorts so that the keys are 'out of order', but the partitions are 'ordered'\n",
    "        parts = sorted(partitions,key=itemgetter(1))\n",
    "        self.partition_keys = list(np.array(parts)[:,0])\n",
    "        \n",
    "        #generate breakpoints that will better match a power law distribution to get equally sized buckets.\n",
    "        break_list = []\n",
    "\n",
    "        first_break = 0.75\n",
    "\n",
    "        for i in range(self.NUM_REDUCERS):\n",
    "            break_list.append(first_break * 0.91** (i))\n",
    "        \n",
    "        #Create partition file from the break list.\n",
    "        self.partition_file = np.array(break_list)\n",
    "        \n",
    "    \n",
    "    #Read line and assign partition key to each line based on the value\n",
    "    def mapper_partition(self, key, value):\n",
    "\n",
    "        value_list = value.split(\"\\t\")\n",
    "        avg_sim_score = value_list[0]\n",
    "        # Prepend the approriate key by finding the bucket, and using the index to fetch the key.\n",
    "        #Loop over number of reducers\n",
    "        for idx in xrange(self.NUM_REDUCERS):\n",
    "            if float(avg_sim_score) > self.partition_file[idx]:\n",
    "                #Need to emit the partition key here to leverage the hadoop shuffle phase.\n",
    "                yield str(self.partition_keys[idx]),key+\"\\t\"+value\n",
    "                break\n",
    "            \n",
    "    def reducer_sort(self,key,value):\n",
    "        for v in value:\n",
    "            #This emits the partition key for illustration\n",
    "            #yield key,v\n",
    "            #This omits the partition key from the output\n",
    "            yield None, v\n",
    "        \n",
    "    def steps(self):\n",
    "\n",
    "\n",
    "        JOBCONF_STEP = {\n",
    "            'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'stream.map.output.field.separator':'\\t',    \n",
    "            'stream.num.map.output.key.fields': '2',\n",
    "            'mapreduce.partition.keycomparator.options': '-k1,1',\n",
    "            'mapreduce.job.reduces': self.NUM_REDUCERS\n",
    "        }\n",
    "        \n",
    "        JOBCONF_STEP2 = {\n",
    "            'stream.num.map.output.key.fields':4,\n",
    "            'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'stream.map.output.field.separator':\"\\t\",\n",
    "            'mapreduce.partition.keypartitioner.options':'-k1,1',\n",
    "            'mapreduce.partition.keycomparator.options':'-k3,3nr -k2,2',\n",
    "            'mapred.reduce.tasks': self.NUM_REDUCERS,\n",
    "            'partitioner':'org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner'\n",
    "        }\n",
    "        \n",
    "        return [\n",
    "            MRStep(jobconf=JOBCONF_STEP,\n",
    "                   mapper=self.mapper,\n",
    "                   reducer=self.reducer),\n",
    "            MRStep(jobconf=JOBCONF_STEP2,\n",
    "                    mapper_init=self.mapper_partitioner_init,\n",
    "                    mapper=self.mapper_partition,\n",
    "                    reducer=self.reducer_sort\n",
    "                  )\n",
    "        ]\n",
    "                  \n",
    "        \n",
    "        \n",
    "    #END SUDENT CODE531_SIMILARITY\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRsimilarity.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/nwchen24/HW5_Phase2/similarities': No such file or directory\n",
      "No configs found; falling back on auto-configuration\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Creating temp directory /tmp/similarity.nwchen24.20170622.195714.041304\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/nwchen24/tmp/mrjob/similarity.nwchen24.20170622.195714.041304/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob3414837540192038067.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 30\n",
      "  number of splits:30\n",
      "  Submitting tokens for job: job_1497906899862_2137\n",
      "  Submitted application application_1497906899862_2137\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1497906899862_2137/\n",
      "  Running job: job_1497906899862_2137\n",
      "  Job job_1497906899862_2137 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 13% reduce 0%\n",
      "   map 29% reduce 0%\n",
      "   map 31% reduce 0%\n",
      "   map 32% reduce 0%\n",
      "   map 33% reduce 0%\n",
      "   map 38% reduce 0%\n",
      "   map 40% reduce 0%\n",
      "   map 42% reduce 0%\n",
      "   map 43% reduce 0%\n",
      "   map 47% reduce 0%\n",
      "   map 49% reduce 0%\n",
      "   map 51% reduce 0%\n",
      "   map 53% reduce 0%\n",
      "   map 55% reduce 0%\n",
      "   map 56% reduce 0%\n",
      "   map 57% reduce 0%\n",
      "   map 59% reduce 0%\n",
      "   map 60% reduce 0%\n",
      "   map 61% reduce 0%\n",
      "   map 62% reduce 0%\n",
      "   map 63% reduce 0%\n",
      "   map 65% reduce 0%\n",
      "   map 66% reduce 0%\n",
      "   map 67% reduce 0%\n",
      "   map 68% reduce 0%\n",
      "   map 69% reduce 0%\n",
      "   map 70% reduce 0%\n",
      "   map 71% reduce 0%\n",
      "   map 72% reduce 0%\n",
      "   map 73% reduce 0%\n",
      "   map 74% reduce 0%\n",
      "   map 75% reduce 0%\n",
      "   map 76% reduce 0%\n",
      "   map 77% reduce 0%\n",
      "   map 78% reduce 0%\n",
      "   map 79% reduce 0%\n",
      "   map 80% reduce 0%\n",
      "   map 81% reduce 0%\n",
      "   map 82% reduce 0%\n",
      "   map 83% reduce 0%\n",
      "   map 84% reduce 0%\n",
      "   map 85% reduce 0%\n",
      "   map 86% reduce 0%\n",
      "   map 87% reduce 0%\n",
      "   map 88% reduce 0%\n",
      "   map 89% reduce 0%\n",
      "   map 90% reduce 0%\n",
      "   map 91% reduce 0%\n",
      "   map 92% reduce 0%\n",
      "   map 93% reduce 0%\n",
      "   map 94% reduce 0%\n",
      "   map 95% reduce 0%\n",
      "   map 96% reduce 0%\n",
      "   map 97% reduce 0%\n",
      "   map 98% reduce 0%\n",
      "   map 99% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 2%\n",
      "   map 100% reduce 4%\n",
      "   map 100% reduce 6%\n",
      "   map 100% reduce 8%\n",
      "   map 100% reduce 10%\n",
      "   map 100% reduce 11%\n",
      "   map 100% reduce 14%\n",
      "   map 100% reduce 16%\n",
      "   map 100% reduce 19%\n",
      "   map 100% reduce 21%\n",
      "   map 100% reduce 25%\n",
      "   map 100% reduce 29%\n",
      "   map 100% reduce 32%\n",
      "   map 100% reduce 34%\n",
      "   map 100% reduce 37%\n",
      "   map 100% reduce 41%\n",
      "   map 100% reduce 43%\n",
      "   map 100% reduce 45%\n",
      "   map 100% reduce 48%\n",
      "   map 100% reduce 51%\n",
      "   map 100% reduce 52%\n",
      "   map 100% reduce 53%\n",
      "   map 100% reduce 54%\n",
      "   map 100% reduce 55%\n",
      "   map 100% reduce 56%\n",
      "   map 100% reduce 57%\n",
      "   map 100% reduce 58%\n",
      "   map 100% reduce 59%\n",
      "   map 100% reduce 60%\n",
      "   map 100% reduce 61%\n",
      "   map 100% reduce 62%\n",
      "   map 100% reduce 63%\n",
      "   map 100% reduce 64%\n",
      "   map 100% reduce 65%\n",
      "   map 100% reduce 66%\n",
      "   map 100% reduce 67%\n",
      "   map 100% reduce 68%\n",
      "   map 100% reduce 69%\n",
      "   map 100% reduce 70%\n",
      "   map 100% reduce 71%\n",
      "   map 100% reduce 72%\n",
      "   map 100% reduce 73%\n",
      "   map 100% reduce 74%\n",
      "   map 100% reduce 75%\n",
      "   map 100% reduce 76%\n",
      "   map 100% reduce 77%\n",
      "   map 100% reduce 78%\n",
      "   map 100% reduce 79%\n",
      "   map 100% reduce 80%\n",
      "   map 100% reduce 81%\n",
      "   map 100% reduce 82%\n",
      "   map 100% reduce 83%\n",
      "   map 100% reduce 84%\n",
      "   map 100% reduce 85%\n",
      "   map 100% reduce 86%\n",
      "   map 100% reduce 87%\n",
      "   map 100% reduce 88%\n",
      "   map 100% reduce 89%\n",
      "   map 100% reduce 90%\n",
      "   map 100% reduce 91%\n",
      "   map 100% reduce 92%\n",
      "   map 100% reduce 93%\n",
      "   map 100% reduce 94%\n",
      "   map 100% reduce 95%\n",
      "   map 100% reduce 96%\n",
      "   map 100% reduce 97%\n",
      "   map 100% reduce 98%\n",
      "   map 100% reduce 99%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497906899862_2137 completed successfully\n",
      "  Output directory: hdfs:///user/nwchen24/tmp/mrjob/similarity.nwchen24.20170622.195714.041304/step-output/0000\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=7252572\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=3631499328\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=8282112667\n",
      "\t\tFILE: Number of bytes written=13668341852\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=7256562\n",
      "\t\tHDFS: Number of bytes written=3631499328\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=240\n",
      "\t\tHDFS: Number of write operations=100\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=30\n",
      "\t\tLaunched reduce tasks=50\n",
      "\t\tRack-local map tasks=30\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=17050263552\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=103405281280\n",
      "\t\tTotal time spent by all map tasks (ms)=11100432\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=33301296\n",
      "\t\tTotal time spent by all reduce tasks (ms)=40392688\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=201963440\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=11100432\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=40392688\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=37120570\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=459012\n",
      "\t\tInput split bytes=3990\n",
      "\t\tMap input records=999\n",
      "\t\tMap output bytes=15940065524\n",
      "\t\tMap output materialized bytes=5536915332\n",
      "\t\tMap output records=317127176\n",
      "\t\tMerged Map outputs=1500\n",
      "\t\tPhysical memory (bytes) snapshot=77766893568\n",
      "\t\tReduce input groups=57583882\n",
      "\t\tReduce input records=317127176\n",
      "\t\tReduce output records=57583882\n",
      "\t\tReduce shuffle bytes=5536915332\n",
      "\t\tShuffled Maps =1500\n",
      "\t\tSpilled Records=951381528\n",
      "\t\tTotal committed heap usage (bytes)=152667947008\n",
      "\t\tVirtual memory (bytes) snapshot=233330065408\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.reduce.tasks: mapreduce.job.reduces\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob5458148623691705560.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 50\n",
      "  number of splits:50\n",
      "  Submitting tokens for job: job_1497906899862_2159\n",
      "  Submitted application application_1497906899862_2159\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1497906899862_2159/\n",
      "  Running job: job_1497906899862_2159\n",
      "  Job job_1497906899862_2159 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 1% reduce 0%\n",
      "   map 2% reduce 0%\n",
      "   map 3% reduce 0%\n",
      "   map 4% reduce 0%\n",
      "   map 5% reduce 0%\n",
      "   map 6% reduce 0%\n",
      "   map 7% reduce 0%\n",
      "   map 8% reduce 0%\n",
      "   map 9% reduce 0%\n",
      "   map 10% reduce 0%\n",
      "   map 11% reduce 0%\n",
      "   map 12% reduce 0%\n",
      "   map 13% reduce 0%\n",
      "   map 14% reduce 0%\n",
      "   map 15% reduce 0%\n",
      "   map 16% reduce 0%\n",
      "   map 17% reduce 0%\n",
      "   map 18% reduce 0%\n",
      "   map 19% reduce 0%\n",
      "   map 20% reduce 0%\n",
      "   map 21% reduce 0%\n",
      "   map 22% reduce 0%\n",
      "   map 23% reduce 0%\n",
      "   map 24% reduce 0%\n",
      "   map 25% reduce 0%\n",
      "   map 26% reduce 0%\n",
      "   map 27% reduce 0%\n",
      "   map 28% reduce 0%\n",
      "   map 29% reduce 0%\n",
      "   map 30% reduce 0%\n",
      "   map 31% reduce 0%\n",
      "   map 32% reduce 0%\n",
      "   map 33% reduce 0%\n",
      "   map 34% reduce 0%\n",
      "   map 35% reduce 0%\n",
      "   map 36% reduce 0%\n",
      "   map 37% reduce 0%\n",
      "   map 38% reduce 0%\n",
      "   map 39% reduce 0%\n",
      "   map 40% reduce 0%\n",
      "   map 41% reduce 0%\n",
      "   map 42% reduce 0%\n",
      "   map 43% reduce 0%\n",
      "   map 44% reduce 0%\n",
      "   map 45% reduce 0%\n",
      "   map 46% reduce 0%\n",
      "   map 47% reduce 0%\n",
      "   map 48% reduce 0%\n",
      "   map 49% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 51% reduce 0%\n",
      "   map 52% reduce 0%\n",
      "   map 53% reduce 0%\n",
      "   map 54% reduce 0%\n",
      "   map 55% reduce 0%\n",
      "   map 56% reduce 0%\n",
      "   map 57% reduce 0%\n",
      "   map 58% reduce 0%\n",
      "   map 59% reduce 0%\n",
      "   map 60% reduce 0%\n",
      "   map 61% reduce 0%\n",
      "   map 62% reduce 0%\n",
      "   map 63% reduce 0%\n",
      "   map 64% reduce 0%\n",
      "   map 65% reduce 0%\n",
      "   map 66% reduce 0%\n",
      "   map 67% reduce 0%\n",
      "   map 69% reduce 0%\n",
      "   map 70% reduce 0%\n",
      "   map 71% reduce 0%\n",
      "   map 72% reduce 0%\n",
      "   map 73% reduce 0%\n",
      "   map 75% reduce 0%\n",
      "   map 76% reduce 0%\n",
      "   map 77% reduce 0%\n",
      "   map 78% reduce 0%\n",
      "   map 79% reduce 0%\n",
      "   map 80% reduce 0%\n",
      "   map 81% reduce 0%\n",
      "   map 82% reduce 0%\n",
      "   map 83% reduce 0%\n",
      "   map 84% reduce 0%\n",
      "   map 85% reduce 0%\n",
      "   map 87% reduce 0%\n",
      "   map 88% reduce 0%\n",
      "   map 89% reduce 0%\n",
      "   map 91% reduce 0%\n",
      "   map 93% reduce 0%\n",
      "   map 95% reduce 0%\n",
      "   map 96% reduce 0%\n",
      "   map 97% reduce 0%\n",
      "   map 99% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 6%\n",
      "   map 100% reduce 9%\n",
      "   map 100% reduce 17%\n",
      "   map 100% reduce 26%\n",
      "   map 100% reduce 31%\n",
      "   map 100% reduce 34%\n",
      "   map 100% reduce 37%\n",
      "   map 100% reduce 41%\n",
      "   map 100% reduce 42%\n",
      "   map 100% reduce 44%\n",
      "   map 100% reduce 48%\n",
      "   map 100% reduce 50%\n",
      "   map 100% reduce 52%\n",
      "   map 100% reduce 54%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   map 100% reduce 56%\n",
      "   map 100% reduce 57%\n",
      "   map 100% reduce 59%\n",
      "   map 100% reduce 61%\n",
      "   map 100% reduce 62%\n",
      "   map 100% reduce 63%\n",
      "   map 100% reduce 65%\n",
      "   map 100% reduce 66%\n",
      "   map 100% reduce 71%\n",
      "   map 100% reduce 73%\n",
      "   map 100% reduce 74%\n",
      "   map 100% reduce 75%\n",
      "   map 100% reduce 76%\n",
      "   map 100% reduce 77%\n",
      "   map 100% reduce 78%\n",
      "   map 100% reduce 79%\n",
      "   map 100% reduce 81%\n",
      "   map 100% reduce 82%\n",
      "   map 100% reduce 83%\n",
      "   map 100% reduce 84%\n",
      "   map 100% reduce 85%\n",
      "   map 100% reduce 86%\n",
      "   map 100% reduce 87%\n",
      "   map 100% reduce 88%\n",
      "   map 100% reduce 89%\n",
      "   map 100% reduce 90%\n",
      "   map 100% reduce 91%\n",
      "   map 100% reduce 92%\n",
      "   map 100% reduce 93%\n",
      "   map 100% reduce 94%\n",
      "   map 100% reduce 95%\n",
      "   map 100% reduce 96%\n",
      "   map 100% reduce 97%\n",
      "   map 100% reduce 98%\n",
      "   map 100% reduce 99%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497906899862_2159 completed successfully\n",
      "  Output directory: hdfs:///user/nwchen24/HW5_Phase2/similarities\n",
      "Counters: 50\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3631499328\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=3631009414\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=618015881\n",
      "\t\tFILE: Number of bytes written=1890207846\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3631508478\n",
      "\t\tHDFS: Number of bytes written=3631009414\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=300\n",
      "\t\tHDFS: Number of write operations=100\n",
      "\tJob Counters \n",
      "\t\tKilled reduce tasks=6\n",
      "\t\tLaunched map tasks=50\n",
      "\t\tLaunched reduce tasks=56\n",
      "\t\tRack-local map tasks=50\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=16162444800\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=9533332480\n",
      "\t\tTotal time spent by all map tasks (ms)=10522425\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=31567275\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3723958\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=18619790\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=10522425\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3723958\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=7349280\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=161546\n",
      "\t\tInput split bytes=9150\n",
      "\t\tMap input records=57583882\n",
      "\t\tMap output bytes=3746162054\n",
      "\t\tMap output materialized bytes=1258753935\n",
      "\t\tMap output records=57576320\n",
      "\t\tMerged Map outputs=2500\n",
      "\t\tPhysical memory (bytes) snapshot=66192171008\n",
      "\t\tReduce input groups=57576320\n",
      "\t\tReduce input records=57576320\n",
      "\t\tReduce output records=57576320\n",
      "\t\tReduce shuffle bytes=1258753935\n",
      "\t\tShuffled Maps =2500\n",
      "\t\tSpilled Records=115152640\n",
      "\t\tTotal committed heap usage (bytes)=182992764928\n",
      "\t\tVirtual memory (bytes) snapshot=277579530240\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Removing HDFS temp directory hdfs:///user/nwchen24/tmp/mrjob/similarity.nwchen24.20170622.195714.041304...\n",
      "Removing temp directory /tmp/similarity.nwchen24.20170622.195714.041304...\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/nwchen24/HW5_Phase2/similarities\n",
    "\n",
    "#Hadoop to try in altiscale. Ran in ~36 min with 50 reducers\n",
    "!python similarity.py -r hadoop hdfs:///user/nwchen24/HW5_Phase2/inv_index \\\n",
    "    --output-dir='/user/nwchen24/HW5_Phase2/similarities' \\\n",
    "    --no-output \\\n",
    "    --python-bin=./my_env27_for_hadoop_upload/bin/python \\\n",
    "    --archive=hdfs:///user/nwchen24/virtualenv/my_env27_for_hadoop_upload.zip#my_env27_for_hadoop_upload\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copyToLocal: `part00000': File exists\n",
      "copyToLocal: `part00001': File exists\n",
      "copyToLocal: `part00002': File exists\n",
      "copyToLocal: `part00003': File exists\n"
     ]
    }
   ],
   "source": [
    "#copy the top 1000 results to local\n",
    "!hdfs dfs -copyToLocal /user/nwchen24/HW5_Phase2/similarities/part-00000 part00000\n",
    "!hdfs dfs -copyToLocal /user/nwchen24/HW5_Phase2/similarities/part-00001 part00001\n",
    "!hdfs dfs -copyToLocal /user/nwchen24/HW5_Phase2/similarities/part-00002 part00002\n",
    "!hdfs dfs -copyToLocal /user/nwchen24/HW5_Phase2/similarities/part-00003 part00003\n",
    "\n",
    "#concatenate the files\n",
    "!cat part00000 part00001 part00002 part00003 > top_sim_results\n",
    "\n",
    "#remove every other line to remove duplicates e.g. con|pro and pro|con\n",
    "!sed '1~2d' top_sim_results > unique_top_sim_results\n",
    "\n",
    "#Get the top 1000 results\n",
    "!head -1000 unique_top_sim_results > top_1k_sim_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top/Bottom 20 results - Similarity measures - sorted by average\n",
      "(From the entire data set)\n",
      "\n",
      "                          pair |        average |         cosine |        jaccard\n",
      "=====================================================================================================================\n",
      "Top 20\n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "                       one|may |       0.936823 |       0.916667 |       0.956979\n",
      "                      time|one |       0.929034 |       0.906439 |       0.951630\n",
      "                      well|one |       0.903754 |       0.873621 |       0.933887\n",
      "                   los|angeles |       0.895371 |       0.863636 |       0.927105\n",
      "                     would|one |       0.895159 |       0.862450 |       0.927869\n",
      "                     one|first |       0.888629 |       0.853831 |       0.923427\n",
      "                      time|may |       0.880687 |       0.845226 |       0.916149\n",
      "                      well|may |       0.879847 |       0.844037 |       0.915658\n",
      "                     would|may |       0.870875 |       0.832653 |       0.909097\n",
      "                    would|time |       0.869823 |       0.831449 |       0.908197\n",
      "                      upon|one |       0.863019 |       0.820926 |       0.905113\n",
      "                     well|time |       0.856558 |       0.814965 |       0.898152\n",
      "                    time|first |       0.853771 |       0.811282 |       0.896260\n",
      "                      part|one |       0.852345 |       0.807422 |       0.897267\n",
      "                     may|first |       0.838751 |       0.792548 |       0.884954\n",
      "                      one|made |       0.838415 |       0.789315 |       0.887516\n",
      "                    would|well |       0.835533 |       0.788991 |       0.882075\n",
      "                     one|great |       0.834691 |       0.784708 |       0.884674\n",
      "                      upon|may |       0.832910 |       0.784913 |       0.880907\n",
      "                       two|one |       0.829947 |       0.778672 |       0.881222\n",
      "=====================================================================================================================\n",
      "Bottom 20 of Top 1000\n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "                     would|set |       0.591774 |       0.497859 |       0.685689\n",
      "                    place|life |       0.591728 |       0.508834 |       0.674623\n",
      "                   nature|form |       0.591611 |       0.507576 |       0.675646\n",
      "                    much|means |       0.591404 |       0.507328 |       0.675481\n",
      "                    two|either |       0.591355 |       0.504415 |       0.678295\n",
      "                    upon|among |       0.591347 |       0.500549 |       0.682144\n",
      "                   would|never |       0.591261 |       0.492239 |       0.690283\n",
      "                    great|case |       0.591220 |       0.506329 |       0.676110\n",
      "                      upon|set |       0.591183 |       0.499446 |       0.682920\n",
      "                     new|large |       0.590938 |       0.503911 |       0.677966\n",
      "                     world|man |       0.590808 |       0.507803 |       0.673814\n",
      "                   could|among |       0.590791 |       0.502288 |       0.679293\n",
      "                     would|day |       0.590648 |       0.494565 |       0.686732\n",
      "                       two|old |       0.590640 |       0.503326 |       0.677953\n",
      "                    world|like |       0.590243 |       0.507194 |       0.673292\n",
      "                 certain|first |       0.590046 |       0.497854 |       0.682239\n",
      "                 first|certain |       0.590046 |       0.497854 |       0.682239\n",
      "                     point|one |       0.590017 |       0.485887 |       0.694147\n",
      "                   would|right |       0.590007 |       0.496265 |       0.683749\n",
      "                    well|small |       0.589933 |       0.494681 |       0.685185\n"
     ]
    }
   ],
   "source": [
    "#Print the top and bottom results of the top 1000\n",
    "sortedSims = open('top_1k_sim_results').readlines()\n",
    "\n",
    "print \"\\nTop/Bottom 20 results - Similarity measures - sorted by average\"\n",
    "print \"(From the entire data set)\"\n",
    "print ''*117\n",
    "print \"{0:>30} |{1:>15} |{2:>15} |{3:>15}\".format(\n",
    "        \"pair\", \"average\", \"cosine\", \"jaccard\")\n",
    "\n",
    "print '='*117\n",
    "print \"Top 20\"\n",
    "print '-'*117\n",
    "\n",
    "for stripe in sortedSims[:20]:\n",
    "    stripe_print = stripe.split(\"\\t\")\n",
    "    print \"{0:>30} |{1:>15f} |{2:>15f} |{3:>15f}\".format(\n",
    "        stripe_print[0], float(stripe_print[1]), float(stripe_print[2]), float(stripe_print[3]))\n",
    "\n",
    "print '='*117\n",
    "print \"Bottom 20 of Top 1000\"\n",
    "print '-'*117\n",
    "    \n",
    "for stripe in sortedSims[-20:]:\n",
    "    stripe_print = stripe.split(\"\\t\")\n",
    "    print \"{0:>30} |{1:>15f} |{2:>15f} |{3:>15f}\".format(\n",
    "        stripe_print[0], float(stripe_print[1]), float(stripe_print[2]), float(stripe_print[3]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# END STUDENT CODE 5.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Top/Bottom 20 results - Similarity measures - sorted by cosine\n",
    "(From the entire data set)\n",
    "\n",
    "                          pair |         cosine |        jaccard |        overlap |           dice |        average\n",
    "---------------------------------------------------------------------------------------------------------------------\n",
    "                   cons - pros |       0.894427 |       0.800000 |       1.000000 |       0.888889 |       0.895829\n",
    "            forties - twenties |       0.816497 |       0.666667 |       1.000000 |       0.800000 |       0.820791\n",
    "                    own - time |       0.809510 |       0.670563 |       0.921168 |       0.802799 |       0.801010\n",
    "                 little - time |       0.784197 |       0.630621 |       0.926101 |       0.773473 |       0.778598\n",
    "                  found - time |       0.783434 |       0.636364 |       0.883788 |       0.777778 |       0.770341\n",
    "                 nova - scotia |       0.774597 |       0.600000 |       1.000000 |       0.750000 |       0.781149\n",
    "                   hong - kong |       0.769800 |       0.615385 |       0.888889 |       0.761905 |       0.758995\n",
    "                   life - time |       0.769666 |       0.608789 |       0.925081 |       0.756829 |       0.765091\n",
    "                  time - world |       0.755476 |       0.585049 |       0.937500 |       0.738209 |       0.754058\n",
    "                  means - time |       0.752181 |       0.587117 |       0.902597 |       0.739854 |       0.745437\n",
    "                   form - time |       0.749943 |       0.588418 |       0.876733 |       0.740885 |       0.738995\n",
    "       infarction - myocardial |       0.748331 |       0.560000 |       1.000000 |       0.717949 |       0.756570\n",
    "                 people - time |       0.745788 |       0.573577 |       0.923875 |       0.729010 |       0.743063\n",
    "                 angeles - los |       0.745499 |       0.586207 |       0.850000 |       0.739130 |       0.730209\n",
    "                  little - own |       0.739343 |       0.585834 |       0.767296 |       0.738834 |       0.707827\n",
    "                    life - own |       0.737053 |       0.582217 |       0.778502 |       0.735951 |       0.708430\n",
    "          anterior - posterior |       0.733388 |       0.576471 |       0.790323 |       0.731343 |       0.707881\n",
    "                  power - time |       0.719611 |       0.533623 |       0.933586 |       0.695898 |       0.720680\n",
    "              dearly - install |       0.707107 |       0.500000 |       1.000000 |       0.666667 |       0.718443\n",
    "                   found - own |       0.704802 |       0.544134 |       0.710949 |       0.704776 |       0.666165\n",
    "\n",
    "           arrival - essential |       0.008258 |       0.004098 |       0.009615 |       0.008163 |       0.007534\n",
    "         governments - surface |       0.008251 |       0.003534 |       0.014706 |       0.007042 |       0.008383\n",
    "                king - lesions |       0.008178 |       0.003106 |       0.017857 |       0.006192 |       0.008833\n",
    "              clinical - stood |       0.008178 |       0.003831 |       0.011905 |       0.007634 |       0.007887\n",
    "               till - validity |       0.008172 |       0.003367 |       0.015625 |       0.006711 |       0.008469\n",
    "            evidence - started |       0.008159 |       0.003802 |       0.012048 |       0.007576 |       0.007896\n",
    "               forces - record |       0.008152 |       0.003876 |       0.011364 |       0.007722 |       0.007778\n",
    "               primary - stone |       0.008146 |       0.004065 |       0.009091 |       0.008097 |       0.007350\n",
    "             beneath - federal |       0.008134 |       0.004082 |       0.008403 |       0.008130 |       0.007187\n",
    "                factors - rose |       0.008113 |       0.004032 |       0.009346 |       0.008032 |       0.007381\n",
    "           evening - functions |       0.008069 |       0.004049 |       0.008333 |       0.008065 |       0.007129\n",
    "                   bone - told |       0.008061 |       0.003704 |       0.012346 |       0.007380 |       0.007873\n",
    "             building - occurs |       0.008002 |       0.003891 |       0.010309 |       0.007752 |       0.007489\n",
    "                 company - fig |       0.007913 |       0.003257 |       0.015152 |       0.006494 |       0.008204\n",
    "               chronic - north |       0.007803 |       0.003268 |       0.014493 |       0.006515 |       0.008020\n",
    "             evaluation - king |       0.007650 |       0.003030 |       0.015625 |       0.006042 |       0.008087\n",
    "             resulting - stood |       0.007650 |       0.003663 |       0.010417 |       0.007299 |       0.007257\n",
    "                 agent - round |       0.007515 |       0.003289 |       0.012821 |       0.006557 |       0.007546\n",
    "         afterwards - analysis |       0.007387 |       0.003521 |       0.010204 |       0.007018 |       0.007032\n",
    "            posterior - spirit |       0.007156 |       0.002660 |       0.016129 |       0.005305 |       0.007812"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.6  <a name=\"5.6\"></a> Evaluation of synonyms that your discovered\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "\n",
    "In this part of the assignment you will evaluate the success of you synonym detector (developed in response to HW5.4).\n",
    "Take the top 1,000 closest/most similar/correlative pairs of words as determined by your measure in HW5.4, and use the synonyms function in the accompanying python code:\n",
    "\n",
    "nltk_synonyms.py\n",
    "\n",
    "Note: This will require installing the python nltk package:\n",
    "\n",
    "http://www.nltk.org/install.html\n",
    "\n",
    "and downloading its data with nltk.download().\n",
    "\n",
    "For each (word1,word2) pair, check to see if word1 is in the list, \n",
    "synonyms(word2), and vice-versa. If one of the two is a synonym of the other, \n",
    "then consider this pair a 'hit', and then report the precision, recall, and F1 measure  of \n",
    "your detector across your 1,000 best guesses. Report the macro averages of these measures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Calculate performance measures:\n",
    "$$Precision (P) = \\frac{TP}{TP + FP} $$  \n",
    "$$Recall (R) = \\frac{TP}{TP + FN} $$  \n",
    "$$F1 = \\frac{2 * ( precision * recall )}{precision + recall}$$\n",
    "\n",
    "\n",
    "We calculate Precision by counting the number of hits and dividing by the number of occurances in our top1000 (opportunities)   \n",
    "We calculate Recall by counting the number of hits, and dividing by the number of synonyms in wordnet (syns)\n",
    "\n",
    "\n",
    "Other diagnostic measures not implemented here:  https://en.wikipedia.org/wiki/F1_score#Diagnostic_Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/nwchen24/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Download nltk wordnet\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of Hits: 12 out of top 1000\n",
      "Number of words without synonyms: 11\n",
      "\n",
      "Precision\t0.0061494750275\n",
      "Recall\t\t0.0446299451837\n",
      "F1\t\t0.00781238955623\n",
      "\n",
      "Words without synonyms:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "among\n",
      "hong\n",
      "would\n",
      "shall\n",
      "could\n",
      "upon\n",
      "per\n",
      "kong\n",
      "angeles\n",
      "los\n",
      "without\n"
     ]
    }
   ],
   "source": [
    "''' Performance measures '''\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import sys\n",
    "#print all the synset element of an element\n",
    "def synonyms(string):\n",
    "    syndict = {}\n",
    "    for i,j in enumerate(wn.synsets(string)):\n",
    "        syns = j.lemma_names()\n",
    "        for syn in syns:\n",
    "            syndict.setdefault(syn,1)\n",
    "    return syndict.keys()\n",
    "hits = []\n",
    "\n",
    "TP = 0\n",
    "FP = 0\n",
    "\n",
    "TOTAL = 0\n",
    "flag = False # so we don't double count, but at the same time don't miss hits\n",
    "\n",
    "top1000sims = []\n",
    "with open(\"top_1k_sim_results\",\"r\") as f:\n",
    "    for line in f.readlines():\n",
    "\n",
    "        line = line.strip()\n",
    "        words = line.split(\"\\t\")[0]\n",
    "        top1000sims.append(words)\n",
    "    \n",
    "\n",
    "measures = {}\n",
    "not_in_wordnet = []\n",
    "\n",
    "for line in top1000sims:\n",
    "    TOTAL += 1\n",
    "\n",
    "    words = line.split(\"|\")\n",
    "    \n",
    "    for word in words:\n",
    "        if word not in measures:\n",
    "            measures[word] = {\"syns\":0,\"opps\": 0,\"hits\":0}\n",
    "        measures[word][\"opps\"] += 1 \n",
    "    \n",
    "    syns0 = synonyms(words[0])\n",
    "    measures[words[1]][\"syns\"] = len(syns0)\n",
    "    if len(syns0) == 0:\n",
    "        not_in_wordnet.append(words[0])\n",
    "        \n",
    "    if words[1] in syns0:\n",
    "        TP += 1\n",
    "        hits.append(line)\n",
    "        flag = True\n",
    "        measures[words[1]][\"hits\"] += 1\n",
    "        \n",
    "        \n",
    "        \n",
    "    syns1 = synonyms(words[1]) \n",
    "    measures[words[0]][\"syns\"] = len(syns1)\n",
    "    if len(syns1) == 0:\n",
    "        not_in_wordnet.append(words[1])\n",
    "\n",
    "    if words[0] in syns1:\n",
    "        if flag == False:\n",
    "            TP += 1\n",
    "            hits.append(line)\n",
    "            measures[words[0]][\"hits\"] += 1\n",
    "            \n",
    "    flag = False    \n",
    "\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "\n",
    "for key in measures:\n",
    "    p,r,f = 0,0,0\n",
    "    if measures[key][\"hits\"] > 0 and measures[key][\"syns\"] > 0:\n",
    "        p = measures[key][\"hits\"]/measures[key][\"opps\"]\n",
    "        r = measures[key][\"hits\"]/measures[key][\"syns\"]\n",
    "        f = 2 * (p*r)/(p+r)\n",
    "    \n",
    "    # For calculating measures, only take into account words that have synonyms in wordnet\n",
    "    if measures[key][\"syns\"] > 0:\n",
    "        precision.append(p)\n",
    "        recall.append(r)\n",
    "        f1.append(f)\n",
    "\n",
    "#Get unique list of words not in wordnet\n",
    "not_in_wordnet_set = set(not_in_wordnet)   \n",
    "\n",
    "# Take the mean of each measure    \n",
    "print \"\"*110    \n",
    "print \"Number of Hits:\",TP, \"out of top\",TOTAL\n",
    "print \"Number of words without synonyms:\",len(not_in_wordnet_set)\n",
    "print \"\"*110 \n",
    "print \"Precision\\t\", np.mean(precision)\n",
    "print \"Recall\\t\\t\", np.mean(recall)\n",
    "print \"F1\\t\\t\", np.mean(f1)\n",
    "print \"\"*110  \n",
    "\n",
    "print \"Words without synonyms:\"\n",
    "print \"-\"*100\n",
    "\n",
    "\n",
    "for word in not_in_wordnet_set:\n",
    "    print word\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "Number of Hits: 31 out of top 1000\n",
    "Number of words without synonyms: 67\n",
    "\n",
    "Precision\t0.0280214404967\n",
    "Recall\t\t0.0178598869579\n",
    "F1\t\t0.013965517619\n",
    "\n",
    "Words without synonyms:\n",
    "----------------------------------------------------------------------------------------------------\n",
    "[] scotia\n",
    "[] hong\n",
    "[] kong\n",
    "[] angeles\n",
    "[] los\n",
    "[] nor\n",
    "[] themselves\n",
    "[] \n",
    "......."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.7  <a name=\"5.7\"></a> OPTIONAL: using different vocabulary subsets\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "\n",
    "Repeat HW5 using vocabulary words ranked from 8001,-10,000;  7001,-10,000; 6001,-10,000; 5001,-10,000; 3001,-10,000; and 1001,-10,000;\n",
    "Dont forget to report you Cluster configuration.\n",
    "\n",
    "Generate the following graphs:\n",
    "-- vocabulary size (X-Axis) versus CPU time for indexing\n",
    "-- vocabulary size (X-Axis) versus number of pairs processed\n",
    "-- vocabulary size (X-Axis) versus F1 measure, Precision, Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.8  <a name=\"5.8\"></a> OPTIONAL: filter stopwords\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "There is also a corpus of stopwords, that is, high-frequency words like \"the\", \"to\" and \"also\" that we sometimes want to filter out of a document before further processing. Stopwords usually have little lexical content, and their presence in a text fails to distinguish it from other texts. Python's nltk comes with a prebuilt list of stopwords (see below). Using this stopword list filter out these tokens from your analysis and rerun the experiments in 5.5 and disucuss the results of using a stopword list and without using a stopword list.\n",
    "\n",
    "> from nltk.corpus import stopwords\n",
    ">> stopwords.words('english')\n",
    "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n",
    "'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
    "'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
    "'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
    "'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
    "'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
    "'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n",
    "'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
    "'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here',\n",
    "'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
    "'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so',\n",
    "'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.9 <a name=\"5.9\"></a> OPTIONAL \n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "There are many good ways to build our synonym detectors, so for this optional homework, \n",
    "measure co-occurrence by (left/right/all) consecutive words only, \n",
    "or make stripes according to word co-occurrences with the accompanying \n",
    "2-, 3-, or 4-grams (note here that your output will no longer \n",
    "be interpretable as a network) inside of the 5-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.10 <a name=\"5.10\"></a> OPTIONAL \n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "Once again, benchmark your top 10,000 associations (as in 5.5), this time for your\n",
    "results from 5.6. Has your detector improved?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "511px",
    "width": "251px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
